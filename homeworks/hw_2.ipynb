{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d465100e",
   "metadata": {},
   "source": [
    "# Analisi e Implementazione di Metodi di Ottimizzazione\n",
    "\n",
    "Creiamo i tre metodi di ottimizzazione per trovare i minimi di diverse funzioni:\n",
    "1.  **Metodo del Gradiente (Gradient Descent - GD)**\n",
    "2.  **Metodo di Newton**\n",
    "3.  **Metodo del Gradiente Stocastico (SGD)**\n",
    "\n",
    "Viene utilizzata una strategia di **Backtracking Line Search** per la determinazione del passo di apprendimento (alpha)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ebaf8",
   "metadata": {},
   "source": [
    "## 1 Import delle librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a3ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "from mpl_toolkits.mplot3d import Axes3D # Per grafici 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d34ad9f",
   "metadata": {},
   "source": [
    "## 2 Implementazione degli Algoritmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caaa8bd",
   "metadata": {},
   "source": [
    "## 2.1 Funzione Backtracking (Line Search)\n",
    "\n",
    "Questa funzione implementa la ricerca del passo $\\alpha$ tramite backtracking, basandosi sulla condizione di Armijo per garantire che il risultato converge alla soluzione.\n",
    "\n",
    "**Parametri**:\n",
    "- $f$: La funzione obiettivo\n",
    "- $df$: Il gradiente della funzione obiettivo\n",
    "- $X_k$: Il punto corrente (vettore)\n",
    "- $p_k$: La direzione di discesa (vettore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83aebdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtracking per alpha\n",
    "def backtracking(f, df, X_k, p_k):\n",
    "    alpha = 1\n",
    "    rho = 1/2 # Fattore di riduzione per alpha\n",
    "    c1 = 0.25\n",
    "\n",
    "    # Calcola il gradiente del punto passato\n",
    "    grad_k = df(X_k)\n",
    "\n",
    "    # Calcola il prodotto scalare\n",
    "    grad_dot_p = np.dot(grad_k, p_k)\n",
    "    \n",
    "    # Condizione Di Armijo: Il loop \"while\" continua FINCHÉ la condizione è VIOLATA (segno >)\n",
    "    while f(X_k + alpha * p_k) > f(X_k) + c1 * alpha * grad_dot_p:\n",
    "        alpha = rho * alpha\n",
    "\n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad29c54",
   "metadata": {},
   "source": [
    "## 2.2 Metodo del gradiente\n",
    "\n",
    "La direzione di discesa $p_k = -\\nabla f(x)$, ovvero l'antigradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38db714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo del gradiente\n",
    "def GD (f, df, X_old, maxit, tol_f, tol_x):\n",
    "    \n",
    "    count=0\n",
    "    dim = len(X_old) # Calcolo la dimensione di X_old\n",
    "\n",
    "    # Dichiare una matrice di 0 di dimensione maxit + 1 per dim\n",
    "    fun_history = np.zeros((maxit + 1, dim))\n",
    "    fun_history[0,] = X_old\n",
    "\n",
    "    exit_flag = 'maxit' # Flag di uscita di default\n",
    "\n",
    "    current_grad_norm=np.linalg.norm(df(X_old))\n",
    "\n",
    "    # Controllo se ho superato le iterazioni o se ho raggiunto la tolleranza desiderata\n",
    "    while count < maxit and current_grad_norm > tol_f:\n",
    "        # Direzione di discesa\n",
    "        p_k=-df(X_old)\n",
    "        \n",
    "        # Backtracking di alpha\n",
    "        alpha=backtracking(f, df, X_old, p_k)\n",
    "\n",
    "        # Calcolo di X_k+1\n",
    "        X_new = X_old + alpha * p_k\n",
    "\n",
    "        # Controllo se sto facendo progressi (tolleranza x)\n",
    "        if np.linalg.norm(X_new-X_old) < tol_x:\n",
    "            exit_flag = 'tol_x'\n",
    "            break\n",
    "\n",
    "        # Ricalcolo la norma del gradiente\n",
    "        current_grad_norm=np.linalg.norm(df(X_new))\n",
    "\n",
    "        # Aggiorno per l'iterazione successiva\n",
    "        X_old = X_new\n",
    "        count+=1\n",
    "\n",
    "        # Aggiungo il valore della funzione in x_k nella matrice dei risultati \n",
    "        fun_history[count] = X_new\n",
    "\n",
    "    # Controllo se l'uscita è dovuta a tol_f (norma del gradiente)\n",
    "    if exit_flag == 'maxit' and current_grad_norm <= tol_f:\n",
    "        exit_flag = 'tol_f'\n",
    "\n",
    "    # Rimuovo le righe non utilizzate dalla cronologia\n",
    "    if count<maxit:\n",
    "        fun_history = fun_history[:count+1]\n",
    "\n",
    "    return X_old, count, fun_history, exit_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af6510",
   "metadata": {},
   "source": [
    "## 2.2 Metodo Newton\n",
    "\n",
    "La direzione di discesa è $p_k = - \\frac{\\nabla f_k}{H_k} $\n",
    "\n",
    "Nota: questo metodo è molto lento perchè ad ogni iterazione si deve moltiplicare il gradiente e l'hessiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75834970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo di Newton\n",
    "def Newton (f, df, hess_f, X_old, maxit, tol_f, tol_x):\n",
    "    \n",
    "    count=0\n",
    "    dim = len(X_old) \n",
    "    \n",
    "    # Cronologia\n",
    "    fun_history = np.zeros((maxit + 1, dim))\n",
    "    fun_history[0,]=X_old\n",
    "\n",
    "    exit_flag = 'maxit'\n",
    "\n",
    "    current_grad_norm=np.linalg.norm(df(X_old))\n",
    "\n",
    "    while count < maxit and current_grad_norm > tol_f:\n",
    "\n",
    "        # Calcolo l'Hessiana\n",
    "        H_k = hess_f(X_old)\n",
    "\n",
    "        # Calcolo il gradiente\n",
    "        grad_k = df(X_old)\n",
    "\n",
    "        # Direzione di discesa\n",
    "        p_k=np.linalg.solve(H_k, -grad_k)\n",
    "\n",
    "        # Backtracking di alpha\n",
    "        alpha=backtracking(f, df, X_old, p_k)\n",
    "\n",
    "        # Calcolo di X_k+1\n",
    "        X_new = X_old + alpha * p_k\n",
    "\n",
    "        # Controllo se sto facendo progressi (tolleranza x)\n",
    "        if np.linalg.norm(X_new-X_old) < tol_x:\n",
    "            exit_flag = 'tol_x'\n",
    "            break\n",
    "\n",
    "        # Ricalcolo la norma del gradiente\n",
    "        current_grad_norm=np.linalg.norm(df(X_new))\n",
    "\n",
    "        # Aggiorno per l'iterazione successiva\n",
    "        X_old = X_new\n",
    "        count+=1\n",
    "        fun_history[count] = X_new\n",
    "\n",
    "    \n",
    "    if exit_flag == 'maxit' and current_grad_norm <= tol_f:\n",
    "        exit_flag = 'tol_f'\n",
    "\n",
    "    if count<maxit:\n",
    "        fun_history = fun_history[:count+1]\n",
    "\n",
    "    return X_old, count, fun_history, exit_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e85ae8",
   "metadata": {},
   "source": [
    "## 2.4 Metodo gradiente stocastico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD (df_completo, df_stocastico, X_old, n_samples, max_steps, tol_f, tol_x, k, alpha):\n",
    "\n",
    "    dim = len(X_old)\n",
    "\n",
    "    S_k = np.arange(n_samples)\n",
    "\n",
    "    step_count=0\n",
    "    epoch_count=0\n",
    "\n",
    "    fun_history = np.zeros((max_steps + 1, dim))\n",
    "    fun_history[0,]=X_old\n",
    "\n",
    "    exit_flag = 'maxit'\n",
    "\n",
    "    current_grad_norm=np.linalg.norm(df_completo(X_old))\n",
    "\n",
    "    while step_count < max_steps and current_grad_norm > tol_f :\n",
    "\n",
    "        np.random.shuffle(S_k)\n",
    "\n",
    "        epoch_count += 1\n",
    "v\n",
    "        for i in range (0, n_samples, k):\n",
    "\n",
    "            indices = S_k[i:i+k]\n",
    "            grad_stoc = df_stocastico(X_old, indices)\n",
    "            p_k = -grad_stoc\n",
    "\n",
    "            X_new = X_old + alpha * p_k\n",
    "\n",
    "            if np.linalg.norm(X_new - X_old) < tol_x:\n",
    "                exit_flag = 'tol_x'\n",
    "                break\n",
    "\n",
    "            X_old = X_new\n",
    "            step_count += 1\n",
    "            fun_history[step_count] = X_new\n",
    "\n",
    "            if step_count >= max_steps:\n",
    "                break\n",
    "\n",
    "        current_grad_norm=np.linalg.norm(df_completo(X_new))\n",
    "\n",
    "        if exit_flag == 'tol_x':\n",
    "            break\n",
    "\n",
    "    if exit_flag == 'maxit' and current_grad_norm <= tol_f:\n",
    "        exit_flag = 'tol_f'\n",
    "\n",
    "    if step_count<max_steps: # troncamento\n",
    "        fun_history = fun_history[:step_count+1]\n",
    "\n",
    "    return X_old, step_count, fun_history, exit_flag, epoch_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a46d2a",
   "metadata": {},
   "source": [
    "## 2.5 Funzione per la generazione dei grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc521666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_generator (x0, y0, f,path_history_gd, path_history_newton, tmin, titolo):\n",
    "    X_mesh, Y_mesh = np.meshgrid(x0, y0)\n",
    "    Z = f([X_mesh, Y_mesh])\n",
    "\n",
    "    # Memorizzo il percorso delle x e y\n",
    "    path_x_gd = path_history_gd[:, 0]\n",
    "    path_y_gd = path_history_gd[:, 1]\n",
    "    path_x_new = path_history_newton[:, 0]\n",
    "    path_y_new = path_history_newton[:, 1]\n",
    "\n",
    "    # Definisco la figura\n",
    "    fig = plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # <-- GENERAZIONE GRAFICO 3D -->\n",
    "\n",
    "    # Aggiungo subplot 3D\n",
    "    axs1 = fig.add_subplot(1,2,1,projection='3d')\n",
    "\n",
    "    # Disegno la superficie\n",
    "    axs1.plot_surface(X_mesh, Y_mesh, Z, cmap='viridis', rstride=1, cstride=1, linewidth=0, antialiased=True, alpha=0.7)\n",
    "\n",
    "    # Aggiunge il percorso anche nel grafico 3D gradiente\n",
    "    z_path_gd = f([path_x_gd, path_y_gd])\n",
    "    axs1.plot(path_x_gd, path_y_gd, z_path_gd, 'r-o', label='Percorso del Gradiente')\n",
    "\n",
    "    # Aggiunge il percorso anche nel grafico 3D newton\n",
    "    z_path_newton = f([path_x_new, path_y_new])\n",
    "    axs1.plot(path_x_new, path_y_new, z_path_newton, 'b-o', label='Percorso di Newton')\n",
    "\n",
    "    # Evidenzia il punto iniziale e finale gradiente\n",
    "    axs1.plot(path_x_gd[0], path_y_gd[0], 'go', label=f'Start: ({path_x_gd[0]}, {path_y_gd[0]})') # punto iniziale\n",
    "    axs1.plot(path_x_gd[-1], path_y_gd[-1], 'rx', label=f'End: ({path_x_gd[-1]:.2f}, {path_y_gd[-1]:.2f})', markersize=16) # punto finale\n",
    "\n",
    "    # Evidenzia il punto iniziale e finale newton\n",
    "    axs1.plot(path_x_new[0], path_y_new[0], 'go', label=f'Start: ({path_x_new[0]}, {path_y_new[0]})') # punto iniziale\n",
    "    axs1.plot(path_x_new[-1], path_y_new[-1], 'rx', label=f'End: ({path_x_new[-1]:.2f}, {path_y_new[-1]:.2f})', markersize=16) # punto finale\n",
    "\n",
    "    axs1.plot(tmin[0], tmin[1], 'b*', markersize=15, label=f\"Minimo Teorico ({tmin[0]}, {tmin[1]})\") # Minimo teorico\n",
    "\n",
    "    # Aggiungo label e titolo\n",
    "    axs1.set_title(f\"{titolo}\")\n",
    "    axs1.set_xlabel(\"x\")\n",
    "    axs1.set_ylabel(\"y\")\n",
    "    axs1.set_zlabel(\"z\")\n",
    "    axs1.legend()\n",
    "    axs1.view_init(elev=30, azim=135) # Imposta un angolo di visuale\n",
    "\n",
    "    # <--- GENERAZIONE GRAFICO 2D --->\n",
    "\n",
    "    # Definisco il grafico 2D\n",
    "    axs0 = fig.add_subplot(1,2,2)\n",
    "\n",
    "    # Disegna le curve di livello\n",
    "    c1 = axs0.contour(X_mesh, Y_mesh, Z, levels=50, cmap='viridis', alpha=0.7)\n",
    "    fig.colorbar(c1, label='Valore di $f(x,y)$')\n",
    "\n",
    "    # Disegna il percorso\n",
    "    axs0.plot(path_x_gd, path_y_gd, 'r-o', label='Percorso del Gradiente')\n",
    "    axs0.plot(path_x_new, path_y_new, 'b-o', label='Percorso di Newton')\n",
    "\n",
    "    # Evidenzia il punto iniziale e finale gradiente\n",
    "    axs0.plot(path_x_gd[0], path_y_gd[0], 'go', label=f'Start: ({path_x_gd[0]}, {path_y_gd[0]})') # punto iniziale\n",
    "    axs0.plot(path_x_gd[-1], path_y_gd[-1], 'rx', label=f'End: ({path_x_gd[-1]:.2f}, {path_y_gd[-1]:.2f})', markersize=16) # punto finale\n",
    "    \n",
    "    # Evidenzia il punto iniziale e finale newton\n",
    "    axs0.plot(path_x_new[0], path_y_new[0], 'go', label=f'Start: ({path_x_new[0]}, {path_y_new[0]})') # punto iniziale\n",
    "    axs0.plot(path_x_new[-1], path_y_new[-1], 'bx', label=f'End: ({path_x_new[-1]:.2f}, {path_y_new[-1]:.2f})', markersize=16) # punto finale\n",
    "\n",
    "    axs0.plot(tmin[0], tmin[1], 'y*', markersize=15, label=f\"Minimo Teorico ({tmin[0]}, {tmin[1]})\") # Minimo teorico\n",
    "    # Etichette e titoli\n",
    "    axs0.set_title(f\"{titolo}\")\n",
    "    axs0.set_xlabel(\"x\")\n",
    "    axs0.set_ylabel(\"y\")\n",
    "    axs0.legend()\n",
    "    axs0.set_aspect('equal', adjustable='box') # \"Grafo è \"quadrato\"\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07468013",
   "metadata": {},
   "source": [
    "### 2.6 Funzione per gli output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "770d5f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solutions(f, df, h_f, x1, x2, maxit, tol_grad, tol_step):\n",
    "    print(\"--- Metodo del Gradiente (GD) ---\")\n",
    "    sol_gd, iter_gd, path_gd, exit_gd = GD(f, df, x1, maxit, tol_grad, tol_step)\n",
    "    print(f\"Soluzione: {sol_gd}\")\n",
    "    print(f\"Numero iterazioni: {iter_gd}\")\n",
    "    print(f\"Condizione di uscita: {exit_gd}\")\n",
    "\n",
    "    print(\"--- Metodo di Newton ---\")\n",
    "    sol_newton, iter_newton, path_newton, exit_newton = Newton(f, df, h_f, x2, maxit, tol_grad, tol_step)\n",
    "    print(f\"Soluzione: {sol_newton}\")\n",
    "    print(f\"Numero iterazioni: {iter_newton}\")\n",
    "    print(f\"Condizione di uscita: {exit_newton}\")\n",
    "\n",
    "    return path_gd, path_newton\n",
    "\n",
    "def get_SGD_solutions(f, sdf, x, n, maxit, tol_grad, tol_step, batch_k, learning_rate):\n",
    "    print(\"--- Metodo del Gradiente Stocastico (SGD) ---\")\n",
    "    sol_sgd, iter_sgd, path_sgd, exit_sgd, epoch = SGD(f, sdf, x, n, maxit, tol_grad, tol_step, batch_k, learning_rate)\n",
    "    print(f\"Soluzione: {sol_sgd}\")\n",
    "    print(f\"Numero iterazioni: {iter_sgd}\")\n",
    "    print(f\"Condizione di uscita: {exit_sgd}\")\n",
    "    print(f\"Epoche fatte: {epoch}\")\n",
    "\n",
    "    return path_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e83c4066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_graph(f, df, gd, newton, titolo, sgd=None): \n",
    "    \n",
    "    errori_gd = [f(x_k) for x_k in gd]\n",
    "    errori_newton = [f(x_k) for x_k in newton]\n",
    "\n",
    "    norma_grad_gd = [np.linalg.norm(df(x_k)) for x_k in gd]\n",
    "    norma_grad_newton = [np.linalg.norm(df(x_k)) for x_k in newton]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(16, 10)) \n",
    "\n",
    "    ax = axs[0, 0]\n",
    "    ax.plot(errori_gd, label=\"Gradient Descent (GD)\", marker='.', linestyle='-')\n",
    "    ax.plot(errori_newton, label=\"Newton Smorzato\", marker='.', linestyle='--')\n",
    "    ax.set_title(\"Valore Funzione (GD vs Newton)\")\n",
    "    ax.set_ylabel(\"Valore Funzione $f(x_k)$ (log scale)\")\n",
    "    ax.set_xlabel(\"Iterazione (k)\")\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(which=\"both\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax = axs[1, 0]\n",
    "    ax.plot(norma_grad_gd, label=\"Gradient Descent (GD)\", marker='.', linestyle='-')\n",
    "    ax.plot(norma_grad_newton, label=\"Newton Smorzato\", marker='.', linestyle='--')\n",
    "    ax.set_title(\"Norma Gradiente (GD vs Newton)\")\n",
    "    ax.set_ylabel(r\"Norma Gradiente $\\|\\nabla f(x_k)\\|$ (log scale)\")\n",
    "    ax.set_xlabel(\"Iterazione (k)\")\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "\n",
    "    if sgd is not None:\n",
    "        errori_sgd = [f(x_k) for x_k in sgd]\n",
    "        norma_grad_sgd = [np.linalg.norm(df(x_k)) for x_k in sgd]\n",
    "\n",
    "        ax = axs[0, 1]\n",
    "        ax.plot(errori_sgd, label=\"Stochastic GD\", color='green', alpha=0.8)\n",
    "        ax.set_title(\"Valore Funzione (SGD)\")\n",
    "        ax.set_ylabel(\"Valore Funzione $f(x_k)$ (log scale)\")\n",
    "        ax.set_xlabel(\"Iterazione (k)\")\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend()\n",
    "        ax.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "        ax = axs[1, 1]\n",
    "        ax.plot(norma_grad_sgd, label=\"Stochastic GD\", color='green', alpha=0.8)\n",
    "        ax.set_title(\"Norma Gradiente (SGD)\")\n",
    "        ax.set_ylabel(r\"Norma Gradiente $\\|\\nabla f(x_k)\\|$ (log scale)\")\n",
    "        ax.set_xlabel(\"Iterazione (k)\")\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend()\n",
    "        ax.grid(which=\"both\", linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    else:\n",
    "        axs[0, 1].axis('off')\n",
    "        axs[1, 1].axis('off')\n",
    "\n",
    "    fig.suptitle(f\"Analisi Convergenza per {titolo}\", fontsize=18)\n",
    "    \n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff313e",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3 Funzione Quadratica: $f(x, y) = (x-5)^2+(y-2)^2$\n",
    "\n",
    "Cerchiamo i punti critici della funzione.\n",
    "\n",
    "Calcoliamo la derivata rispetto a $x$ e a $y$.\n",
    "\n",
    "- $\\frac{∂x}{∂f} = ​2(x-5)$\n",
    "- $\\frac{∂y}{∂f} =​ 2(y-2)$\n",
    "\n",
    "Ponendo $x=5$ e $y=2$, troviamo un punto critico.\n",
    "Possiamo notare gli elementi della funzione sono elevati al quadrato, quindi sappiamo che il punto critico è un minimo globale.\n",
    "\n",
    "**Nota:** Nei metodi utilizzati, converge in una sola iterazione."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde6573f",
   "metadata": {},
   "source": [
    "### 3.1 Implementazione Dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75d54457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "    return (x-5)**2+(y-2)**2\n",
    "\n",
    "def df1(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    df_dx = 2*(x-5) \n",
    "    df_dy = 2*(y-2)\n",
    "    \n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def hess_f1(X):\n",
    "    HESS=np.zeros((2,2))\n",
    "\n",
    "    HESS[0][0] = 2\n",
    "    HESS[0][1] = 0\n",
    "    HESS[1][0] = 0\n",
    "    HESS[1][1] = 2\n",
    "    return HESS    \n",
    "\n",
    "x_range1=np.linspace(-2,12,200)\n",
    "y_range1=np.linspace(-2,8,200)\n",
    "\n",
    "maxit = 300\n",
    "tolleranza_f = 1.e-6\n",
    "tolleranza_x = 1.e-6\n",
    "punto1_gd=np.array([0.0,0.0])\n",
    "punto1_new=np.array([1.0,1.0])\n",
    "tmin1 = [5.0, 2.0]\n",
    "titolo1 = r\"$f(x, y) = (x-5)^2+(y-2)^2$\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef17cefe",
   "metadata": {},
   "source": [
    "### 3.2 Soluzioni dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea128d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [5. 2.]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [5. 2.]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n"
     ]
    }
   ],
   "source": [
    "path_gd, path_new = get_solutions(f1, df1, hess_f1, punto1_gd, punto1_new, maxit, tolleranza_f, tolleranza_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2096d54",
   "metadata": {},
   "source": [
    "### 3.2 Generazione Grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2f8ce90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid command name \"126576762792192process_stream_events\"\n",
      "    while executing\n",
      "\"126576762792192process_stream_events\"\n",
      "    (\"after\" script)\n",
      "/home/dule/anaconda3/envs/CN25/lib/python3.13/tkinter/__init__.py:862: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  func(*args)\n"
     ]
    }
   ],
   "source": [
    "# Generazione grafico\n",
    "graph_generator(x_range1, y_range1, f1, path_gd, path_new, tmin1, titolo1)\n",
    "error_graph(f1, df1, path_gd, path_new, titolo1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687edc4a",
   "metadata": {},
   "source": [
    "--- \n",
    "## 4. Funzione di Rosenbrock: $f(x, y) = (1-x)^2+100(y-x^2)^2$\n",
    "\n",
    "Cerchiamo i punti critici della funzione.\n",
    "\n",
    "Calcoliamo la derivata rispetto a $x$ e a $y$.\n",
    "\n",
    "$\\frac{df}{dx} = -2(1-x) + 200(y-x^2)(-2x) = 2(x-1) -400x(y-x^2) $\n",
    "\n",
    "$\\frac{df}{dy} =​ 200(y-x^2)$\n",
    "\n",
    "Poniamo $y=x^2$, troviamo che $ 2(x-1) -400x(x^2-x^2)=0 \\to 2x-2=0 \\to x=1 $  \n",
    "\n",
    "Quindi l'unico punto critico è su $(1,1)$\n",
    "\n",
    "$H= \\begin{bmatrix} \n",
    "\\frac{d^2f}{dx^2} & \\frac{d^2f}{dxdy} \\\\\n",
    "\\frac{d^2f}{dydx} & \\frac{d^2f}{dy^2}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix} \n",
    " 2-400y+1200x^2& -400x \\\\\n",
    "-400x & 200\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Studiamo $\\det(H)$.\n",
    "\n",
    "$P(1,1) = 802\\cdot 200 - 160000 = 160400 - 160000 = 400 > 0$, quindi è un punto di minimo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071591da",
   "metadata": {},
   "source": [
    "### 4.1 Implementazione dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d99e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    return (1-x)**2+100*(y-x**2)**2\n",
    "\n",
    "def df2(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    df_dx = -2*(1-x) - 400*(y-x**2)*x\n",
    "    df_dy = 200*(y-x**2)\n",
    "    \n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def hess_f2(X):\n",
    "    HESS=np.zeros((2,2)) # Crea una matrice 2x2\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    HESS[0][0] = 2 - 400*y + 1200*x**2 \n",
    "    HESS[0][1] = -400*x \n",
    "    HESS[1][0] = HESS[0][1] # Per teorema di Sxhwarz\n",
    "    HESS[1][1] = 200 \n",
    "    return HESS\n",
    "    \n",
    "\n",
    "x_range2=np.linspace(-2,2,200)\n",
    "y_range2=np.linspace(-1,3,200)\n",
    "\n",
    "maxit = 5000\n",
    "tolleranza_f = 1.e-6\n",
    "tolleranza_x = 1.e-6\n",
    "punto2_gd=np.array([0.0,0.0])\n",
    "punto2_new=np.array([0.5,0.5])\n",
    "tmin2 = [1.0,1.0]\n",
    "titolo2 = r\"$f(x, y) = (1-x)^2+100(y-x^2)^2$\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3096080e",
   "metadata": {},
   "source": [
    "### 4.2 Soluzioni dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d2f42ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [0.9998038  0.99960895]\n",
      "Numero iterazioni: 2200\n",
      "Condizione di uscita: tol_x\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [0.99999971 0.99999935]\n",
      "Numero iterazioni: 10\n",
      "Condizione di uscita: tol_x\n"
     ]
    }
   ],
   "source": [
    "path_gd_r, path_newton_r = get_solutions(f2, df2, hess_f2, punto2_gd, punto2_new, maxit, tolleranza_f, tolleranza_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5478e08",
   "metadata": {},
   "source": [
    "### 4.3 Generazione Grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c04dabfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid command name \"126576721289472process_stream_events\"\n",
      "    while executing\n",
      "\"126576721289472process_stream_events\"\n",
      "    (\"after\" script)\n"
     ]
    }
   ],
   "source": [
    "# Grafico del metodo GD\n",
    "graph_generator(x_range2, y_range2, f2, path_gd_r, path_newton_r, tmin2, titolo2)\n",
    "error_graph(f2, df2, path_gd_r, path_newton_r, titolo2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbbfc40",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 Funzione multimodale: $f(x,y) = x^4-x^2+y^2$\n",
    "\n",
    "Cerchiamo i punti critici della funzione.\n",
    "\n",
    "Calcoliamo la derivata rispetto a $x$ e a $y$.\n",
    "\n",
    "- $\\frac{df}{dx} = ​4x^3-2x = 2x(2x^2-1) $\n",
    "- $\\frac{df}{dy} =​ 2y$\n",
    "\n",
    "Notiamo che ponendo $x=0$ e $y= 0$ troviamo il nostro primo punto critico.\n",
    "\n",
    "Ponendo $y=0$, troviamo i casi in cui $2x^2-1=0$, ovvero $x= \\pm \\sqrt{\\frac12}$, ovvero $\\simeq 0.70717$\n",
    "\n",
    "Calcoliamo se questi punti critici sono punti minimi, massimi o di sella.\n",
    "\n",
    "$H= \\begin{bmatrix} \n",
    "\\frac{d^2f}{dx^2} & \\frac{d^2f}{dxdy} \\\\\n",
    "\\frac{d^2f}{dydx} & \\frac{d^2f}{dy^2}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix} \n",
    "12x^2-2 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Studiamo $ \\det(H)$.\n",
    "\n",
    "$P(0,0) = -4 < 0$, quindi è un punto di sella\n",
    "\n",
    "$P(\\sqrt{\\frac12},0) = 2(6-2) = 8 > 0$, quindi è un punto di minimo\n",
    "\n",
    "$P(-\\sqrt{\\frac12},0) = 2(6-2) = 8 >0$, quindi è un punto di minimo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46a523a",
   "metadata": {},
   "source": [
    "### 5.1 Implementazione dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37944ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    return x**4-x**2+y**2\n",
    "\n",
    "def df3(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    df_dx = 4*x**3-2*x\n",
    "    df_dy = 2*y\n",
    "    \n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def hess_f3(X):\n",
    "    HESS=np.zeros((2,2))\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    HESS[0][0] = 12*x**2-2 # Derivata seconda rispetto a x\n",
    "    HESS[0][1] = 0 # Derivata mista\n",
    "    HESS[1][0] = HESS[0][1] # Per teorema di Sxhwarz, la derivata mista è uguale\n",
    "    HESS[1][1] = 2 # Derivata seconda rispetto a y\n",
    "    return HESS\n",
    "    \n",
    "\n",
    "x_range3=np.linspace(-1.5,1.5,200)\n",
    "y_range3=np.linspace(-1.5,1.5,200)\n",
    "\n",
    "maxit = 5000\n",
    "tolleranza_f = 1.e-6\n",
    "tolleranza_x = 1.e-6\n",
    "punto3_neg=np.array([-1.0,-1.0]) \n",
    "punto3_pos=np.array([1.0,1.0]) \n",
    "punto3_sad=np.array([0.0,5.0]) \n",
    "tmin3 = [0.0, 0.0]\n",
    "tmin3_alt = [-np.sqrt(1/2), 0.0]\n",
    "titolo3 = r\"$f(x,y) = x^4-x^2+y^2$\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bccf36",
   "metadata": {},
   "source": [
    "### 5.2 Soluzioni dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc864a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [0. 0.]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [-0.70710712  0.        ]\n",
      "Numero iterazioni: 4\n",
      "Condizione di uscita: tol_x\n"
     ]
    }
   ],
   "source": [
    "path_gd_m, path_new_m = get_solutions(f3, df3, hess_f3, punto3_pos, punto3_neg, maxit, tolleranza_f, tolleranza_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438c2880",
   "metadata": {},
   "source": [
    "### 5.3 Generazione Grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "663a79f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradiente\n",
    "graph_generator(x_range3, y_range3, f3, path_gd_m, path_new_m, tmin3, titolo3)\n",
    "error_graph(f3, df3, path_gd_m, path_new_m, titolo3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6196fa8",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Minimi Quadrati Lineari: $f(x) = \\frac12||Ax-b||_2^2$ con $A \\in \\R^{n \\times n}$\n",
    "\n",
    "Definiamo la $i$-esima funzione componente come: $f_i(x) = \\frac{1}{2} ( A_ix-b_i )^2$\n",
    "\n",
    "Per la linearità dell'operatore gradiente, il gradiente della somma è la somma dei gradienti:\n",
    "$\\nabla f(x) = \\nabla \\left( \\sum_{i=1}^m f_i(x) \\right) = \\sum_{i=1}^m \\nabla f_i(x)$\n",
    "\n",
    "Applichiamo la regola della catena:\n",
    "$\\nabla f_i(x) = \\frac{1}{2} \\cdot 2 \\cdot (A_i x - b_i) \\cdot \\nabla(A_i x - b_i)$\n",
    "\n",
    "Ora, calcoliamo $\\nabla(A_i x - b_i)$. La funzione $g(x) = A_i x - b_i$ è:\n",
    "$g(x) = (a_{i1}x_1 + a_{i2}x_2 + \\dots + a_{in}x_n) - b_i$\n",
    "\n",
    "Calcoliamo le derivate parziali rispetto a ogni $x_j$:\n",
    "$\\frac{\\partial g}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} (a_{i1}x_1 + \\dots + a_{ij}x_j + \\dots + a_{in}x_n - b_i) = a_{ij}$\n",
    "\n",
    "Assemblando queste derivate parziali in un vettore gradiente (colonna):\n",
    "$$ \\nabla(A_i x - b_i) = \n",
    "   \\begin{bmatrix} \n",
    "   \\frac{\\partial g}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial g}{\\partial x_n} \n",
    "   \\end{bmatrix} = \n",
    "   \\begin{bmatrix} \n",
    "   a_{i1} \\\\ \\vdots \\\\ a_{in} \n",
    "   \\end{bmatrix} = A_i^T $$\n",
    "(Questo è il trasposto della *riga* $A_i$).\n",
    "\n",
    "Sostituendo questo risultato, otteniamo il gradiente della $i$-esima componente:\n",
    "$\\nabla f_i(x) = (A_i x - b_i) A_i^T$\n",
    "Questo è un vettore $n \\times 1$ (uno scalare moltiplicato per un vettore colonna).\n",
    "\n",
    "**Ora sommiamo i gradienti di tutte le $m$ componenti:**\n",
    "\n",
    "$\\nabla f(x) = \\sum_{i=1}^m \\nabla f_i(x) = \\sum_{i=1}^m (A_i x - b_i) A_i^T$\n",
    "\n",
    "Definiamo il vettore \"errore\" $e = Ax - b$, dove $e_i = (A_i x - b_i)$ è la $i$-esima componente (scalare). La somma diventa:\n",
    "$\\nabla f(x) = \\sum_{i=1}^m e_i A_i^T$\n",
    "\n",
    "Analizziamo la matrice $A^T \\in \\R^{n \\times m}$. Le **colonne** di $A^T$ sono i vettori $A_i^T$:\n",
    "$$ A^T = \n",
    "   \\begin{bmatrix} \n",
    "   | & | & & | \\\\ \n",
    "   A_1^T & A_2^T & \\dots & A_m^T \\\\ \n",
    "   | & | & & | \n",
    "   \\end{bmatrix} $$\n",
    "\n",
    "La moltiplicazione matrice-vettore $A^T e$ è definita come la combinazione lineare delle colonne di $A^T$ con i pesi (scalari) $e_i$:\n",
    "$$ A^T e = \n",
    "   \\begin{bmatrix} \n",
    "   | & \\dots & | \\\\ \n",
    "   A_1^T & \\dots & A_m^T \\\\ \n",
    "   | & \\dots & | \n",
    "   \\end{bmatrix} \n",
    "   \\begin{bmatrix} \n",
    "   e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_m \n",
    "   \\end{bmatrix} \n",
    "   = e_1 A_1^T + e_2 A_2^T + \\dots + e_m A_m^T = \\sum_{i=1}^m e_i A_i^T $$\n",
    "\n",
    "Quindi $\\nabla f(x) = A^T (Ax - b)$\n",
    "\n",
    "I punti minimi sono $A^T(Ax-b)=0$\n",
    "\n",
    "Prendendo il gradiente ottenuto, notiamo che l'Hessiana è semplicemente $A^TA$, ovvero una costante.\n",
    "\n",
    "Questo significa che la curvatura del grafico è uguale ad ogni punto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da0bd1f",
   "metadata": {},
   "source": [
    "**Nota: questa è una loss function:**\n",
    "\n",
    "- $A$ sono i dati, ovvero il dataset in input\n",
    "\n",
    "- $x$ sono i parametri del modello, ovvero i $ \\theta$\n",
    "\n",
    "- $b$ sono i valori reali\n",
    "\n",
    "- $Ax-b$ sono le previsioni del modello - i valori reali: sono la distanza tra i valori predetti e quelli reali\n",
    "\n",
    "Lo eleviamo al quadrato per avere una distanza positiva e al quadrato per dare importanza ai valori distanti a quelli predetti.\n",
    "\n",
    "Quindi, questa funzione serve per quantificare quanto è \"sbagliato\" il nostro modello."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be580c84",
   "metadata": {},
   "source": [
    "## 6.1 Implementazione dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19d1e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f4(x, A, b):\n",
    "    e = A @ x -b\n",
    "    norm_sq = np.dot(e, e)\n",
    "    return 1/2 * norm_sq\n",
    "\n",
    "def df4(X, A, b):\n",
    "    return A.T @ (A @ X - b)\n",
    "\n",
    "def hess_f4(A):\n",
    "    return A.T @ A\n",
    "\n",
    "def sdf4(x, indices, A, b):\n",
    "    A_batch= A[indices, :]\n",
    "    b_batch= b[indices]\n",
    "    \n",
    "    return A_batch.T @ (A_batch @ x - b_batch)\n",
    "\n",
    "n = 50 \n",
    "A = np.random.rand(n,n) \n",
    "b = A @ np.ones(n) \n",
    "X_start_1 = np.zeros(n) \n",
    "X_start_2 = np.random.randn(n)\n",
    "\n",
    "title_f4 = r\"$f(x) = \\frac{1}{2}\\|Ax-b\\|_2^2 \\text{ con } A \\in \\mathbb{R}^{n \\times n}$\"\n",
    "maxit = 2000\n",
    "maxit_SGD = 20000\n",
    "tol_grad = 1e-3\n",
    "tol_step = 1e-6\n",
    "batch_k = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "f_min_sq = lambda x: f4(x, A, b) # Equivalente a scrivere def f_min_sq(x): return f4(x, A, b)\n",
    "df_min_sq = lambda x: df4(x, A, b) \n",
    "h_min_sq = lambda x: hess_f4(A)\n",
    "sdf_min_sq = lambda x, idx: sdf4(x, idx, A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97306e0",
   "metadata": {},
   "source": [
    "## 6.2 Soluzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc80fd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [0.99419172 1.00042572 0.9974993  0.99061471 0.99104586 1.00294025\n",
      " 0.99229851 0.99703716 0.99396093 0.99885337 0.99947053 1.00798297\n",
      " 1.00144444 1.00312878 1.00816649 0.99808564 1.00004362 0.99075617\n",
      " 0.99700698 1.01398527 0.99712775 1.00936569 0.99621996 0.99951262\n",
      " 0.99315607 1.00970292 0.98402    0.99554865 0.99867069 0.99869503\n",
      " 0.99956544 1.00706672 0.99849981 1.01045055 0.98843922 0.99239436\n",
      " 0.99303397 0.99752265 0.99994549 1.01167496 1.0229098  0.99288991\n",
      " 1.00074202 1.00391563 0.99942373 1.00530257 1.00200854 1.00663985\n",
      " 1.00052121 1.0000389 ]\n",
      "Numero iterazioni: 2000\n",
      "Condizione di uscita: maxit\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1.]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo del Gradiente Stocastico (SGD) ---\n",
      "Soluzione: [0.9745339  0.98652882 0.98185909 0.96271941 0.9860728  0.98563402\n",
      " 0.99692708 0.9918792  0.9933611  1.0229672  1.0029946  1.02896873\n",
      " 1.02984649 1.01086075 1.03419443 0.99600486 0.97648427 0.97666552\n",
      " 1.02182881 1.04627236 1.00802496 1.01634478 0.98367854 0.9888113\n",
      " 0.99581522 1.02499166 0.98387229 1.00709263 0.97819974 0.99208491\n",
      " 0.99644993 1.02573588 1.01486411 1.02119453 0.96574072 0.98285314\n",
      " 0.98476585 1.0002696  0.97128739 1.02315382 1.02925411 0.99151759\n",
      " 0.98370185 0.98701971 1.00302545 1.00572178 0.99072575 1.0162996\n",
      " 1.00348472 0.98158007]\n",
      "Numero iterazioni: 20000\n",
      "Condizione di uscita: maxit\n",
      "Epoche fatte: 2000\n"
     ]
    }
   ],
   "source": [
    "path_gd_f4, path_newton_f4 = get_solutions(f_min_sq, df_min_sq, h_min_sq, X_start_1, X_start_2, maxit, tol_grad, tol_step)\n",
    "\n",
    "path_sgd_f4 = get_SGD_solutions(f_min_sq, sdf_min_sq, X_start_1, n, maxit_SGD, tol_grad, tol_step, batch_k, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e86802",
   "metadata": {},
   "source": [
    "## 6.3 Generazione Grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5005e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_graph(f_min_sq, df_min_sq, path_gd_f4, path_newton_f4, title_f4, path_sgd_f4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b9f2c0",
   "metadata": {},
   "source": [
    "Notiamo che newton è velocissimo. Questo perchè in funzioni quadrate, salta direttamente in quel punto.\n",
    "Il grafico del gradiente è rumoroso, perchè nella iterazione k fa un passo fuori rotta, il grafico sale, per poi essere ritornato nella iterazione k+1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a619d",
   "metadata": {},
   "source": [
    "---\n",
    "# 7 Minimi quadrati con regolarizzazione: $f(x) = \\frac{1}{2}||Ax-b||^2_2 + \\lambda ||x||^2_2$, con $\\lambda \\in [0,1]$\n",
    "\n",
    "Questa funzione viene chiamata anche come Regressione di Ridge.\n",
    "\n",
    "Ora dobbiamo trovare il termine noto $\\frac{1}{2}||Ax-b||^2_2$ e il termine di regolarizzazione $\\lambda ||x||^2_2$\n",
    "\n",
    "La medesima, vuole trovare una x con componenti (pesi) il più possibile vicini a zero. Penalizza soluzioni con valori molto grandi.\n",
    "\n",
    "Separiamo il problema come $\\nabla f(x) = \\nabla f_{LS}(x) + \\nabla f_{Reg}(x)$\n",
    "\n",
    "Troviamo il gradiente:\n",
    "\n",
    "- Come già fatto nella funzione precedente, $\\nabla f_{LS}(x) = A^T (Ax - b)$\n",
    "\n",
    "- $\\nabla f_{Reg} = 2 \\lambda x $\n",
    "\n",
    "Quindi la soluzione è $\\nabla f(x) = A^T (Ax - b) + 2\\lambda x$\n",
    "\n",
    "Per l'Hessiana, scomponiamo la funzione $H=f_A+f_\\lambda$. \n",
    "\n",
    "$f_A$ l'abbiamo già definita nel caso di funzione precedente $A^TA$\n",
    "\n",
    "$f_\\lambda$, invece, ha le derivate per $H_{ki}$ con $k != i$ sono uguali a $0$ perchè sono costanti.\n",
    "\n",
    "Mentre per $k=i$, ovvero la matrice diagonale, non vengono considerate costanti e la loro derivata è $2\\lambda$, \n",
    "\n",
    "Quindi $A^T A + 2\\lambda I$, dove I è la matrice identità."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d321c30",
   "metadata": {},
   "source": [
    "## 7.1 Definisco le funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f3cbb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisco la funzione\n",
    "def f5(x, A, b, lambda_val):\n",
    "    e = A @ x -b\n",
    "    errore_ls = 1/2 * np.dot(e, e)\n",
    "    errore_reg = lambda_val*np.dot(x,x)\n",
    "    return errore_ls + errore_reg\n",
    "\n",
    "# Questa calcola il GRADIENTE COMPLETO (Batch)\n",
    "def df5(x, A, b, lambda_val):\n",
    "    grad_ls = A.T @ (A @ x - b)\n",
    "    grad_reg = 2*lambda_val*x\n",
    "    return grad_ls +  grad_reg\n",
    "\n",
    "# Hessiana\n",
    "def hess_f5(A, lambda_val):\n",
    "    return A.T @ A + 2 * lambda_val * np.identity(n)\n",
    "\n",
    "# Questa calcola il GRADIENTE STOCASTICO (Mini-Batch)\n",
    "def sdf5(x, indices, A, b, lambda_val):\n",
    "    A_batch= A[indices, :]\n",
    "    b_batch= b[indices]\n",
    "\n",
    "    grad_ls_stoc = A_batch.T @ (A_batch @ x - b_batch)\n",
    "    grad_reg = 2 * lambda_val * x\n",
    "    \n",
    "    return grad_ls_stoc + grad_reg\n",
    "\n",
    "n = 50 # siamo in R^50\n",
    "A = np.random.rand(n,n) # A è una matrice n x n con valori casuali\n",
    "X_true = np.ones(n)\n",
    "b = A @ X_true + 0.1 * np.random.randn(n) # rendiamo b in modo che b= A(vettori 1)+rumore\n",
    "lambda_val = 0.1 \n",
    "\n",
    "titolo_f5 = r\"$f(x) = \\frac{1}{2}\\|Ax-b\\|_2^2 + \\lambda \\|x\\|_2^2 \\text{ con } \\lambda \\in [0,1]$\"\n",
    "X_start1 = np.zeros(n)      # Punto di partenza\n",
    "X_start2 = np.ones(n)      # Punto di partenza\n",
    "maxit = 2000\n",
    "maxit_SGD = 20000         # L'SGD richiede molti più passi del GD\n",
    "batch_k = 5                # Dimensione del mini-batch\n",
    "learning_rate = 0.001      # Alpha (deve essere molto piccolo)\n",
    "# NOTA: Una epoca è fatta ad ogni 10 iterazioni, perchè n/batch_k = 50 / 5 = 10\n",
    "\n",
    "f_min_sqr = lambda x: f5(x, A, b, lambda_val)\n",
    "df_min_sqr = lambda x: df5(x, A, b, lambda_val)\n",
    "h_min_sqr = lambda x: hess_f5(A, lambda_val)\n",
    "sdf_min_sqr = lambda x, idx: sdf5(x, idx, A, b, lambda_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f2b35",
   "metadata": {},
   "source": [
    "## 7.2 Soluzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd31a4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [1.03178293 1.12426501 0.94263555 1.04350899 1.09165286 0.95509401\n",
      " 0.96430785 1.088189   1.00338672 0.96073464 0.99438199 0.99348787\n",
      " 1.00479878 1.03447613 0.97677608 0.90443434 0.92783998 1.10702218\n",
      " 0.94534034 0.92347477 0.96576797 0.99570113 1.03017254 1.0074972\n",
      " 0.85000696 0.98484963 0.91894817 0.99002637 0.92668414 0.9413196\n",
      " 0.97705318 1.02049943 1.11931237 0.99540824 1.07973342 1.07210597\n",
      " 1.03142573 1.19262973 0.97922404 1.04596857 1.00692446 0.9674641\n",
      " 0.94050765 0.97254543 0.99283741 0.95362001 0.98352352 0.95403676\n",
      " 1.0221426  1.0313038 ]\n",
      "Numero iterazioni: 2000\n",
      "Condizione di uscita: maxit\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [1.03127222 1.12483284 0.94358165 1.04178578 1.0913257  0.9540803\n",
      " 0.96395839 1.08920827 1.00219361 0.96135998 0.99501152 0.99456105\n",
      " 1.00567037 1.03566028 0.97799956 0.90247945 0.92831853 1.10820343\n",
      " 0.94667675 0.92286834 0.96707034 0.99418684 1.03120165 1.00799034\n",
      " 0.84781504 0.98526338 0.91936319 0.98778605 0.92534477 0.93988788\n",
      " 0.97810057 1.02065489 1.12052301 0.99240516 1.079451   1.07174775\n",
      " 1.03160541 1.192273   0.97923091 1.04744418 1.00775802 0.96770234\n",
      " 0.93997624 0.97362787 0.99298304 0.95336057 0.98394012 0.95360941\n",
      " 1.02194273 1.03182485]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo del Gradiente Stocastico (SGD) ---\n",
      "Soluzione: [1.01407608 1.0851021  0.89890528 1.0233423  1.03169155 0.98740067\n",
      " 0.96798163 1.03509158 1.03966547 0.96866627 1.01382862 1.01684536\n",
      " 1.02119449 0.99222754 0.9838316  0.94719458 0.97376811 1.0173134\n",
      " 1.03857388 1.01798898 0.98585136 1.00420129 1.03405586 0.98578274\n",
      " 0.89989071 0.93619077 0.94940329 1.02636106 0.92188484 0.98548873\n",
      " 0.92435296 1.02840839 1.02654734 1.0242529  1.07963303 1.02004747\n",
      " 1.00479515 1.13180877 0.99161761 0.93240119 0.98693161 0.98078044\n",
      " 0.9458796  0.95707796 0.97907643 0.98241335 0.87669568 1.04091642\n",
      " 0.99841023 1.02054714]\n",
      "Numero iterazioni: 20000\n",
      "Condizione di uscita: maxit\n",
      "Epoche fatte: 2000\n"
     ]
    }
   ],
   "source": [
    "path_gd_f5, path_newton_f5 = get_solutions(f_min_sqr, df_min_sqr, h_min_sqr, X_start1, X_start2, maxit, tol_grad, tol_step)\n",
    "\n",
    "path_sgd_f5 = get_SGD_solutions(f_min_sqr, sdf_min_sqr, X_start1, n, maxit_SGD, tol_grad, tol_step, batch_k, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db42ed6",
   "metadata": {},
   "source": [
    "## 7.3 Generazione Grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5039254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_graph(f_min_sqr, df_min_sqr, path_gd_f5, path_newton_f5, titolo_f5, path_sgd_f5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379d5c3",
   "metadata": {},
   "source": [
    "---\n",
    "# 8 $f(x) = \\sum \\limits_{i=1}^n (x_i-i)^2-\\sum \\limits_{i=1}^n \\ln(x_i)$, con $x_i>0$\n",
    "\n",
    "$\\nabla f(x) = 2 (x-i) - \\frac{1}{x}$\n",
    "\n",
    "Notiamo che per $ i!= k$,$H_{ik}$ non ha elementi perchè la derivata rispetto a x_k ha solo costanti.\n",
    "\n",
    "I punti minimi sono: $2 (x-i) - \\frac{1}{x} = 0$ \n",
    "\n",
    "Ovvero $2x^2-2ix-1 = 0 \\to \\frac{2i\\pm\\sqrt{4i^2+8}}{4}$\n",
    "\n",
    "Invece, per $i = k$, $H_{ik} = 2+(\\frac{1}{x^2})$\n",
    "\n",
    "Quindi l'Hessiana ha solo elementi nella diagonale\n",
    "\n",
    "**Nota**: qua il metodo stocastico è inutile, dato che il gradiente non ha matrici e il metodo del gradiente ha già un costo $O(n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "331a89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f6 (x, idx):\n",
    "    if np.any(x <= 0):\n",
    "        return np.inf # ritorna infinito se x non è nel dominio (ln(x) con x<=0)\n",
    "\n",
    "    sum1 = np.sum((x-idx)**2)\n",
    "    sum2 = np.sum(np.log(x))\n",
    "    return sum1 - sum2\n",
    "    \n",
    "def df6 (x, idx):\n",
    "    return 2 * (x - idx) - 1/x\n",
    "\n",
    "def hess_f6 (x):\n",
    "    HESS = 2+(1/(x**2))\n",
    "    return np.diag(HESS)\n",
    "\n",
    "def sdf6(x, mini_batch, n, idx):\n",
    "\n",
    "    x_batch = x[mini_batch]\n",
    "    idx_batch = idx[mini_batch]\n",
    "    grad_stoc = np.zeros(n)\n",
    "    \n",
    "    # Calcola il gradiente solo per quei componenti\n",
    "    grad_batch_components = 2 * (x_batch - idx_batch) - (1 / x_batch)\n",
    "    \n",
    "    # Inserisci i componenti calcolati nel vettore zero\n",
    "    grad_stoc[mini_batch] = grad_batch_components\n",
    "    \n",
    "    return grad_stoc\n",
    "\n",
    "n = 50\n",
    "title_f6 = r\"$f(x) = \\sum_{i=1}^n (x_i-i)^2 - \\sum_{i=1}^n \\ln(x_i)$\"\n",
    "idx = np.arange(1, n+1)\n",
    "X_start_1 = np.ones(n)\n",
    "X_start_2 = np.random.rand(n)\n",
    "\n",
    "f_lambda = lambda x: f6(x, idx)\n",
    "df_lambda = lambda x: df6(x, idx)\n",
    "h_lambda = lambda x: hess_f6(x)\n",
    "sdf_lambda = lambda x, k: sdf6(x, k, n, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e359e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [ 1.36585366  2.22474227  3.1583123   4.12132034  5.09807621  6.082207\n",
      "  7.07071421  8.0620192   9.05521679 10.04975247 11.04526825 12.04152299\n",
      " 13.03834842 14.03562364 15.03325959 16.0311892  17.02936105 18.02773504\n",
      " 19.02627944 20.02496883 21.02378259 22.02270384 23.02171862 24.02081528\n",
      " 25.01998403 26.01921657 27.01850583 28.01784577 29.01723114 30.01665742\n",
      " 31.01612065 32.01561738 33.01514456 34.01469953 35.01427989 36.01388353\n",
      " 37.01350858 38.01315334 39.0128163  40.0124961  41.0121915  42.01190139\n",
      " 43.01162476 44.0113607  45.01110837 46.010867   47.01063589 48.01041441\n",
      " 49.01020196 50.009998  ]\n",
      "Numero iterazioni: 6\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [ 1.3660254   2.22474487  3.1583124   4.12132034  5.09807621  6.082207\n",
      "  7.07071421  8.0620192   9.05521679 10.04975247 11.04526825 12.04152299\n",
      " 13.03834842 14.03562364 15.03325959 16.0311892  17.02936105 18.02773504\n",
      " 19.02627944 20.02494806 21.02378259 22.02270384 23.02171862 24.02081528\n",
      " 25.01998403 26.01921657 27.01850583 28.01784577 29.01723114 30.01665742\n",
      " 31.01612065 32.01561738 33.01514456 34.01469953 35.01427989 36.01388353\n",
      " 37.01350858 38.01315334 39.0128163  40.0124961  41.0121915  42.01190139\n",
      " 43.01162476 44.0113607  45.01110837 46.010867   47.01063589 48.01041441\n",
      " 49.01020196 50.009998  ]\n",
      "Numero iterazioni: 6\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo del Gradiente Stocastico (SGD) ---\n",
      "Soluzione: [ 1.36388497  2.21084459  3.12775102  4.07265059  5.03089445  5.99641663\n",
      "  6.96631387  7.9390364   8.91368575  9.88970649 10.86673737 11.84453349\n",
      " 12.82292304 13.80178201 14.78101864 15.76056362 16.74036359 17.72037685\n",
      " 18.70057031 19.68091737 20.66139645 21.64198982 22.62268278 23.60346306\n",
      " 24.58432033 25.5652458  26.54623201 27.52727253 28.5083618  29.48949501\n",
      " 30.47066795 31.45187694 32.43311874 33.41439048 34.39568963 35.37701392\n",
      " 36.35836134 37.33973008 38.32111851 39.30252518 40.28394876 41.26538806\n",
      " 42.24684198 43.22830954 44.20978985 45.19128207 46.17278545 47.1542993\n",
      " 48.13582299 49.11735593]\n",
      "Numero iterazioni: 20000\n",
      "Condizione di uscita: maxit\n",
      "Epoche fatte: 2000\n"
     ]
    }
   ],
   "source": [
    "path_gd_f6, path_newton_f6 = get_solutions(f_lambda, df_lambda, h_lambda, X_start_1, X_start_2, maxit, tol_grad, tol_step)\n",
    "\n",
    "path_sgd_f6 = get_SGD_solutions(f_lambda, sdf_lambda, X_start_1, n, maxit_SGD, tol_grad, tol_step, batch_k, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ef8e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_graph(f_lambda, df_lambda, path_gd_f6, path_newton_f6, title_f6, path_sgd_f6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89828313",
   "metadata": {},
   "source": [
    "---\n",
    "# 9 $f(x) = \\sum \\limits_{i=1}^n (x_i-b_i)^2+c\\sum \\limits_{i=1}^n \\ln(x_i)$\n",
    "\n",
    "$\\nabla f(x) = 2(x_i-b_i) + c\\frac{1}{x_i}$\n",
    "\n",
    "I punti minimi sono $2(x_i-b_i) + c\\frac{1}{x_i} = 0 \\to 2x_i^2-2x_ib_i+c = 0 \\to \\frac{2b_i\\pm\\sqrt{4b_i^2-8c}}{4}$\n",
    "\n",
    "**Nota**: dato che abbiamo $\\ln(x_i)$, quando la funzione ha $x\\simeq 0$, si trova $-\\infty$. Quindi non esiste un minimo locale.\n",
    "\n",
    "Scomponiamo la funzione in $f(x) = f_{xb} + f_{ln}$\n",
    "\n",
    "Per l'Hessiana, se deriviamo rispetto a $x_k$:\n",
    "\n",
    "$f_{xb} = 2$, ovvero una $2\\cdot I$, la matrice identità\n",
    "\n",
    "$f_{ln} = -c\\frac{1}{x^2}$, sempre nella matrice diagonale.\n",
    "\n",
    "Quindi $H_{ik} = 2-c\\frac{1}{x^2}$\n",
    "\n",
    "**Note**: questa Hessiana non è sempre definita positiva, quindi è più difficile minimizzare.\n",
    "\n",
    "- Il metodo di Newton convergerà verso il risultato, il metodo del gradiente non funziona e quello del SGD converge molto lentamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f0fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f7(x,c,b):\n",
    "\n",
    "    sum1 = np.sum((x-b)**2)\n",
    "    sum2 = c*np.sum(np.log(x))\n",
    "\n",
    "    return sum1+sum2\n",
    "\n",
    "def df7(x,c,b):\n",
    "    return 2*(x-b)+c*(1/x)\n",
    "\n",
    "def hess_f7(x,c):\n",
    "    HESS =2-c*(1/(x**2))\n",
    "    return np.diag(HESS)\n",
    "\n",
    "def sdf7(x, c, b, n, mini_batch):\n",
    "    x_batch = x[mini_batch]\n",
    "    b_batch = b[mini_batch]\n",
    "    grad_stoc = np.zeros(n)\n",
    "    \n",
    "    # Calcola il gradiente solo per quei componenti\n",
    "    grad_batch_components = 2 * (x_batch - b_batch) + c*(1 / x_batch)\n",
    "    \n",
    "    # Inserisci i componenti calcolati nel vettore zero\n",
    "    grad_stoc[mini_batch] = grad_batch_components\n",
    "    \n",
    "    return grad_stoc\n",
    "\n",
    "title_f7 = r\"$f(x) = \\sum_{i=1}^n (x_i-b_i)^2+c\\sum_{i=1}^n \\ln(x_i)$\"\n",
    "b = np.arange(1, n+1)\n",
    "X_start = np.ones(n) # Partiamo da tutte x=1\n",
    "c= -2.0 # DEVE essere negativo\n",
    "\n",
    "f_c = lambda x: f7(x, c, b)\n",
    "df_c = lambda x: df7(x, c, b)\n",
    "h_c = lambda x: hess_f7(x, c)\n",
    "sdf_c = lambda x, k: sdf7(x, c, b, n, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_gd, path_newton = get_solutions(f_c, df_c, h_c, X_start, maxit, tol_grad, tol_step)\n",
    "\n",
    "path_sgd = get_SGD_solutions(df_c, sdf_c, X_start, n, maxit_SGD, tol_grad, tol_step, batch_k, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_graph(f_c, path_gd, path_newton, path_sgd, title_f7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CN25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
