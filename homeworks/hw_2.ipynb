{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d465100e",
   "metadata": {},
   "source": [
    "# Analisi e Implementazione di Metodi di Ottimizzazione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ebaf8",
   "metadata": {},
   "source": [
    "## 1 Import delle librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a3ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "from mpl_toolkits.mplot3d import Axes3D # Per grafici 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d34ad9f",
   "metadata": {},
   "source": [
    "# 2 Implementazione degli Algoritmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caaa8bd",
   "metadata": {},
   "source": [
    "## 2.1 Funzione Backtracking (Line Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83aebdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(f, df, X_k, p_k):\n",
    "    alpha = 1\n",
    "    rho = 1/2\n",
    "    c1 = 0.25\n",
    "    grad_k = df(X_k)\n",
    "    grad_dot_p = np.dot(grad_k, p_k)    \n",
    "    # Condizione Di Armijo\n",
    "    while f(X_k + alpha * p_k) > f(X_k) + c1 * alpha * grad_dot_p:\n",
    "        alpha = rho * alpha\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad29c54",
   "metadata": {},
   "source": [
    "## 2.2 Metodo del gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38db714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD (f, df, X_old, maxit, tol_f, tol_x):\n",
    "    dim = len(X_old)\n",
    "    fun_history = np.zeros((maxit + 1, dim))\n",
    "    fun_history[0,] = X_old\n",
    "    exit_flag = 'maxit'\n",
    "\n",
    "    current_grad_norm=np.linalg.norm(df(X_old))\n",
    "\n",
    "    for count in range (maxit):\n",
    "        if current_grad_norm < tol_f:\n",
    "            break\n",
    "        p_k=-df(X_old)\n",
    "        alpha=backtracking(f, df, X_old, p_k)\n",
    "        X_new = X_old + alpha * p_k\n",
    "        if np.linalg.norm(X_new-X_old) < tol_x:\n",
    "            exit_flag = 'tol_x'\n",
    "            break\n",
    "        current_grad_norm=np.linalg.norm(df(X_new))\n",
    "        X_old = X_new\n",
    "        count+=1\n",
    "        fun_history[count] = X_new\n",
    "\n",
    "    if exit_flag == 'maxit' and current_grad_norm <= tol_f:\n",
    "        exit_flag = 'tol_f'\n",
    "\n",
    "    if count<maxit:\n",
    "        fun_history = fun_history[:count+1]\n",
    "\n",
    "    return X_old, count, fun_history, exit_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af6510",
   "metadata": {},
   "source": [
    "## 2.2 Metodo Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75834970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton (f, df, hess_f, X_old, maxit, tol_f, tol_x):\n",
    "    dim = len(X_old) \n",
    "    fun_history = np.zeros((maxit + 1, dim))\n",
    "    fun_history[0,]=X_old\n",
    "    exit_flag = 'maxit'\n",
    "\n",
    "    current_grad_norm=np.linalg.norm(df(X_old))\n",
    "\n",
    "    for count in range(maxit):\n",
    "\n",
    "        if current_grad_norm < tol_f:\n",
    "            break\n",
    "\n",
    "        Hess_k = hess_f(X_old)\n",
    "\n",
    "        grad_k = df(X_old)\n",
    "\n",
    "        p_k=np.linalg.solve(Hess_k, -grad_k)\n",
    "\n",
    "        alpha=backtracking(f, df, X_old, p_k)\n",
    "\n",
    "        X_new = X_old + alpha * p_k\n",
    "\n",
    "        if np.linalg.norm(X_new-X_old) < tol_x:\n",
    "            exit_flag = 'tol_x'\n",
    "            break\n",
    "\n",
    "        current_grad_norm=np.linalg.norm(df(X_new))\n",
    "\n",
    "        X_old = X_new\n",
    "        count+=1\n",
    "        fun_history[count] = X_new\n",
    "\n",
    "    \n",
    "    if exit_flag == 'maxit' and current_grad_norm <= tol_f:\n",
    "        exit_flag = 'tol_f'\n",
    "\n",
    "    if count<maxit: # troncamento\n",
    "        fun_history = fun_history[:count+1]\n",
    "\n",
    "    return X_old, count, fun_history, exit_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e85ae8",
   "metadata": {},
   "source": [
    "## 2.4 Metodo gradiente stocastico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "764e6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD (df_completo, df_stocastico, X_old, n_samples, max_steps, tol_f, tol_x, k, alpha):\n",
    "\n",
    "    dim = len(X_old)\n",
    "    S_k = np.arange(n_samples)\n",
    "\n",
    "    fun_history = np.zeros((max_steps + 1, dim))\n",
    "    fun_history[0,]=X_old\n",
    "\n",
    "    exit_flag = 'maxit'\n",
    "    epoch_count=0\n",
    "    total_steps=0\n",
    "\n",
    "    decay_rate = 0.001\n",
    "\n",
    "    while total_steps < max_steps:\n",
    "\n",
    "        epoch_count += 1\n",
    "        np.random.shuffle(S_k)\n",
    "\n",
    "        # Ciclo dei mini batch\n",
    "        for i in range (0, n_samples, k):\n",
    "\n",
    "            if total_steps >= max_steps:\n",
    "                break\n",
    "\n",
    "            indices = S_k[i:i+k] # minibatch\n",
    "            grad_stoc = df_stocastico(X_old, indices)\n",
    "            p_k = -grad_stoc\n",
    "\n",
    "            current_alpha = alpha / (1 + decay_rate * total_steps) # Opzionale, man mano che si va avanti si stabilizza\n",
    "\n",
    "            X_new = X_old + current_alpha * p_k # bisogna fare backtracking?\n",
    "\n",
    "            # X_new = np.maximum(X_new, 1e-6)\n",
    "\n",
    "            total_steps += 1\n",
    "            fun_history[total_steps] = X_new\n",
    "\n",
    "            if np.linalg.norm(X_new - X_old) < tol_x:\n",
    "                exit_flag = 'tol_x'\n",
    "                return X_new, total_steps, fun_history[:total_steps+1], exit_flag, epoch_count\n",
    "\n",
    "            X_old = X_new\n",
    "        # Fine  epoca\n",
    "        \n",
    "        full_grad_norm=np.linalg.norm(df_completo(X_new))\n",
    "\n",
    "        if full_grad_norm < tol_f:\n",
    "            exit_flag = 'tol_f'\n",
    "            return X_new, total_steps, fun_history[:total_steps+1], exit_flag, epoch_count\n",
    "\n",
    "    if total_steps<max_steps: # troncamento\n",
    "        fun_history = fun_history[:total_steps+1]\n",
    "\n",
    "    return X_old, total_steps, fun_history, exit_flag, epoch_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a46d2a",
   "metadata": {},
   "source": [
    "## 2.5 Funzione per la generazione dei grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc521666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_generator (x0, y0, f, path_history_gd, path_history_newton, tmin, titolo):\n",
    "    X_mesh, Y_mesh = np.meshgrid(x0, y0)\n",
    "    Z = f([X_mesh, Y_mesh])\n",
    "\n",
    "    # Memorizzo il percorso delle x e y\n",
    "    path_x_gd = path_history_gd[:, 0]\n",
    "    path_y_gd = path_history_gd[:, 1]\n",
    "    path_x_new = path_history_newton[:, 0]\n",
    "    path_y_new = path_history_newton[:, 1]\n",
    "\n",
    "    # Definisco la figura\n",
    "    fig = plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # <-- GENERAZIONE GRAFICO 3D -->\n",
    "\n",
    "    # Aggiungo subplot 3D\n",
    "    axs1 = fig.add_subplot(1,2,1,projection='3d')\n",
    "\n",
    "    # Disegno la superficie\n",
    "    axs1.plot_surface(X_mesh, Y_mesh, Z, cmap='viridis', rstride=1, cstride=1, linewidth=0, antialiased=True, alpha=0.7)\n",
    "\n",
    "    # Aggiunge il percorso anche nel grafico 3D gradiente\n",
    "    z_path_gd = f([path_x_gd, path_y_gd])\n",
    "    axs1.plot(path_x_gd, path_y_gd, z_path_gd, 'r-o', label='Percorso del Gradiente')\n",
    "\n",
    "    # Aggiunge il percorso anche nel grafico 3D newton\n",
    "    z_path_newton = f([path_x_new, path_y_new])\n",
    "    axs1.plot(path_x_new, path_y_new, z_path_newton, 'b-o', label='Percorso di Newton')\n",
    "\n",
    "    # Evidenzia il punto iniziale e finale gradiente\n",
    "    axs1.plot(path_x_gd[0], path_y_gd[0], 'go', label=f'Start Gradiente: ({path_x_gd[0]}, {path_y_gd[0]})') # punto iniziale\n",
    "    axs1.plot(path_x_gd[-1], path_y_gd[-1], 'rx', label=f'End: ({path_x_gd[-1]:.2f}, {path_y_gd[-1]:.2f})', markersize=16) # punto finale\n",
    "\n",
    "    # Evidenzia il punto iniziale e finale newton\n",
    "    axs1.plot(path_x_new[0], path_y_new[0], 'go', label=f'Start Newton: ({path_x_new[0]}, {path_y_new[0]})') # punto iniziale\n",
    "    axs1.plot(path_x_new[-1], path_y_new[-1], 'rx', label=f'End: ({path_x_new[-1]:.2f}, {path_y_new[-1]:.2f})', markersize=16) # punto finale\n",
    "\n",
    "    axs1.plot(tmin[0], tmin[1], 'b*', markersize=15, label=f\"Minimo Teorico ({tmin[0]}, {tmin[1]})\") # Minimo teorico\n",
    "\n",
    "    # Aggiungo label e titolo\n",
    "    axs1.set_title(f\"{titolo}\")\n",
    "    axs1.set_xlabel(\"x\")\n",
    "    axs1.set_ylabel(\"y\")\n",
    "    axs1.set_zlabel(\"z\")\n",
    "    axs1.legend()\n",
    "    axs1.view_init(elev=30, azim=135) # Imposta un angolo di visuale\n",
    "\n",
    "    # <--- GENERAZIONE GRAFICO 2D --->\n",
    "\n",
    "    # Definisco il grafico 2D\n",
    "    axs0 = fig.add_subplot(1,2,2)\n",
    "\n",
    "    # Disegna le curve di livello\n",
    "    c1 = axs0.contour(X_mesh, Y_mesh, Z, levels=50, cmap='viridis', alpha=0.7)\n",
    "    fig.colorbar(c1, label='Valore di $f(x,y)$')\n",
    "\n",
    "    # Disegna il percorso\n",
    "    axs0.plot(path_x_gd, path_y_gd, 'r-o', label='Percorso del Gradiente')\n",
    "    axs0.plot(path_x_new, path_y_new, 'b-o', label='Percorso di Newton')\n",
    "\n",
    "    # Evidenzia il punto iniziale e finale gradiente\n",
    "    axs0.plot(path_x_gd[0], path_y_gd[0], 'go', label=f'Start: ({path_x_gd[0]}, {path_y_gd[0]})') # punto iniziale\n",
    "    axs0.plot(path_x_gd[-1], path_y_gd[-1], 'rx', label=f'End: ({path_x_gd[-1]:.2f}, {path_y_gd[-1]:.2f})', markersize=16) # punto finale\n",
    "    \n",
    "    # Evidenzia il punto iniziale e finale newton\n",
    "    axs0.plot(path_x_new[0], path_y_new[0], 'go', label=f'Start: ({path_x_new[0]}, {path_y_new[0]})') # punto iniziale\n",
    "    axs0.plot(path_x_new[-1], path_y_new[-1], 'bx', label=f'End: ({path_x_new[-1]:.2f}, {path_y_new[-1]:.2f})', markersize=16) # punto finale\n",
    "\n",
    "    axs0.plot(tmin[0], tmin[1], 'y*', markersize=15, label=f\"Minimo Teorico ({tmin[0]}, {tmin[1]})\") # Minimo teorico\n",
    "    # Etichette e titoli\n",
    "    axs0.set_title(f\"{titolo}\")\n",
    "    axs0.set_xlabel(\"x\")\n",
    "    axs0.set_ylabel(\"y\")\n",
    "    axs0.legend()\n",
    "    axs0.set_aspect('equal', adjustable='box') # \"Grafo è \"quadrato\"\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07468013",
   "metadata": {},
   "source": [
    "### 2.6 Funzione per gli output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "770d5f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solutions(f, df, h_f, x1, x2, maxit, tol_grad, tol_step):\n",
    "    print(\"--- Metodo del Gradiente (GD) ---\")\n",
    "    sol_gd, iter_gd, path_gd, exit_gd = GD(f, df, x1, maxit, tol_grad, tol_step)\n",
    "    print(f\"Soluzione: {sol_gd}\")\n",
    "    print(f\"Numero iterazioni: {iter_gd}\")\n",
    "    print(f\"Condizione di uscita: {exit_gd}\")\n",
    "\n",
    "    print(\"--- Metodo di Newton ---\")\n",
    "    sol_newton, iter_newton, path_newton, exit_newton = Newton(f, df, h_f, x2, maxit, tol_grad, tol_step)\n",
    "    print(f\"Soluzione: {sol_newton}\")\n",
    "    print(f\"Numero iterazioni: {iter_newton}\")\n",
    "    print(f\"Condizione di uscita: {exit_newton}\")\n",
    "\n",
    "    return path_gd, path_newton\n",
    "\n",
    "def get_SGD_solutions(f, sdf, x, n, maxit, tol_grad, tol_step, batch_k, learning_rate):\n",
    "    print(\"--- Metodo del Gradiente Stocastico (SGD) ---\")\n",
    "    sol_sgd, iter_sgd, path_sgd, exit_sgd, epoch = SGD(f, sdf, x, n, maxit, tol_grad, tol_step, batch_k, learning_rate)\n",
    "    print(f\"Soluzione: {sol_sgd}\")\n",
    "    print(f\"Numero iterazioni: {iter_sgd}\")\n",
    "    print(f\"Condizione di uscita: {exit_sgd}\")\n",
    "    print(f\"Epoche fatte: {epoch}\")\n",
    "\n",
    "    return path_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e83c4066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_graph(f, df, gd, newton, titolo, sgd=None): \n",
    "    \n",
    "    errori_gd = [f(x_k) for x_k in gd]\n",
    "    errori_newton = [f(x_k) for x_k in newton]\n",
    "\n",
    "    norma_grad_gd = [np.linalg.norm(df(x_k)) for x_k in gd]\n",
    "    norma_grad_newton = [np.linalg.norm(df(x_k)) for x_k in newton]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(16, 10)) \n",
    "\n",
    "    ax = axs[0, 0]\n",
    "    ax.plot(errori_gd, label=\"Gradient Descent (GD)\", marker='.', linestyle='-')\n",
    "    ax.plot(errori_newton, label=\"Newton Smorzato\", marker='.', linestyle='--')\n",
    "    ax.set_title(\"Valore Funzione (GD vs Newton)\")\n",
    "    ax.set_ylabel(\"Valore Funzione $f(x_k)$ (log scale)\")\n",
    "    ax.set_xlabel(\"Iterazione (k)\")\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(which=\"both\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax = axs[1, 0]\n",
    "    ax.plot(norma_grad_gd, label=\"Gradient Descent (GD)\", marker='.', linestyle='-')\n",
    "    ax.plot(norma_grad_newton, label=\"Newton Smorzato\", marker='.', linestyle='--')\n",
    "    ax.set_title(\"Norma Gradiente (GD vs Newton)\")\n",
    "    ax.set_ylabel(r\"Norma Gradiente $\\|\\nabla f(x_k)\\|$ (log scale)\")\n",
    "    ax.set_xlabel(\"Iterazione (k)\")\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "\n",
    "    if sgd is not None:\n",
    "        errori_sgd = [f(x_k) for x_k in sgd]\n",
    "        norma_grad_sgd = [np.linalg.norm(df(x_k)) for x_k in sgd]\n",
    "\n",
    "        ax = axs[0, 1]\n",
    "        ax.plot(errori_sgd, label=\"Stochastic GD\", color='green', alpha=0.8)\n",
    "        ax.set_title(\"Valore Funzione (SGD)\")\n",
    "        ax.set_ylabel(\"Valore Funzione $f(x_k)$ (log scale)\")\n",
    "        ax.set_xlabel(\"Iterazione (k)\")\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend()\n",
    "        ax.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "        ax = axs[1, 1]\n",
    "        ax.plot(norma_grad_sgd, label=\"Stochastic GD\", color='green', alpha=0.8)\n",
    "        ax.set_title(\"Norma Gradiente (SGD)\")\n",
    "        ax.set_ylabel(r\"Norma Gradiente $\\|\\nabla f(x_k)\\|$ (log scale)\")\n",
    "        ax.set_xlabel(\"Iterazione (k)\")\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend()\n",
    "        ax.grid(which=\"both\", linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    else:\n",
    "        axs[0, 1].axis('off')\n",
    "        axs[1, 1].axis('off')\n",
    "\n",
    "    fig.suptitle(f\"Analisi Convergenza per {titolo}\", fontsize=18)\n",
    "    \n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff313e",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3 Funzione Quadratica: $f(x, y) = (x-5)^2+(y-2)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75d54457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [5. 2.]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [5. 2.]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n"
     ]
    }
   ],
   "source": [
    "def f1(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "    return (x-5)**2+(y-2)**2\n",
    "\n",
    "def df1(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    df_dx = 2*(x-5) \n",
    "    df_dy = 2*(y-2)\n",
    "    \n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def hess_f1(X):\n",
    "    HESS=np.zeros((2,2))\n",
    "\n",
    "    HESS[0][0] = 2\n",
    "    HESS[0][1] = 0\n",
    "    HESS[1][0] = 0\n",
    "    HESS[1][1] = 2\n",
    "    return HESS    \n",
    "\n",
    "x_range1=np.linspace(-2,12,200)\n",
    "y_range1=np.linspace(-2,8,200)\n",
    "\n",
    "maxit = 300\n",
    "tolleranza_f = 1.e-6\n",
    "tolleranza_x = 1.e-6\n",
    "punto1_gd=np.array([0.0,0.0])\n",
    "punto1_new=np.array([1.0,1.0])\n",
    "tmin1 = [5.0, 2.0]\n",
    "titolo1 = r\"$f(x, y) = (x-5)^2+(y-2)^2$\"\n",
    "\n",
    "path_gd_f1, path_new_f1 = get_solutions(f1, df1, hess_f1, punto1_gd, punto1_new, maxit, tolleranza_f, tolleranza_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2f8ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_generator(x_range1, y_range1, f1, path_gd_f1, path_new_f1, tmin1, titolo1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687edc4a",
   "metadata": {},
   "source": [
    "--- \n",
    "## 4. Funzione di Rosenbrock: $f(x, y) = (1-x)^2+100(y-x^2)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d99e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    return (1-x)**2+100*(y-x**2)**2\n",
    "\n",
    "def df2(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    df_dx = -2*(1-x) - 400*(y-x**2)*x\n",
    "    df_dy = 200*(y-x**2)\n",
    "    \n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def hess_f2(X):\n",
    "    HESS=np.zeros((2,2)) # Crea una matrice 2x2\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    HESS[0][0] = 2 - 400*y + 1200*x**2 \n",
    "    HESS[0][1] = -400*x \n",
    "    HESS[1][0] = HESS[0][1] # Per teorema di Sxhwarz\n",
    "    HESS[1][1] = 200 \n",
    "    return HESS\n",
    "    \n",
    "\n",
    "x_range2=np.linspace(-2,2,200)\n",
    "y_range2=np.linspace(-1,3,200)\n",
    "\n",
    "maxit = 5000\n",
    "tolleranza_f = 1.e-6\n",
    "tolleranza_x = 1.e-6\n",
    "punto2_gd=np.array([0.0,0.0])\n",
    "punto2_new=np.array([0.5,0.5])\n",
    "tmin2 = [1.0,1.0]\n",
    "titolo2 = r\"$f(x, y) = (1-x)^2+100(y-x^2)^2$\"\n",
    "\n",
    "path_gd_r, path_newton_r = get_solutions(f2, df2, hess_f2, punto2_gd, punto2_new, maxit, tolleranza_f, tolleranza_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04dabfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_generator(x_range2, y_range2, f2, path_gd_r, path_newton_r, tmin2, titolo2)\n",
    "error_graph(f2, df2, path_gd_r, path_newton_r, titolo2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbbfc40",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 Funzione multimodale: $f(x,y) = x^4-x^2+y^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37944ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    return x**4-x**2+y**2\n",
    "\n",
    "def df3(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    df_dx = 4*x**3-2*x\n",
    "    df_dy = 2*y\n",
    "    \n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def hess_f3(X):\n",
    "    HESS=np.zeros((2,2))\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    HESS[0][0] = 12*x**2-2 # Derivata seconda rispetto a x\n",
    "    HESS[0][1] = 0 # Derivata mista\n",
    "    HESS[1][0] = HESS[0][1] # Per teorema di Sxhwarz, la derivata mista è uguale\n",
    "    HESS[1][1] = 2 # Derivata seconda rispetto a y\n",
    "    return HESS\n",
    "    \n",
    "\n",
    "x_range3=np.linspace(-1.5,1.5,200)\n",
    "y_range3=np.linspace(-1.5,1.5,200)\n",
    "\n",
    "maxit = 5000\n",
    "tolleranza_f = 1.e-6\n",
    "tolleranza_x = 1.e-6\n",
    "punto3_neg=np.array([-1.0,-1.0]) \n",
    "punto3_pos=np.array([1.0,1.0]) \n",
    "punto3_sad=np.array([0.0,5.0]) \n",
    "tmin3 = [0.0, 0.0]\n",
    "tmin3_alt = [-np.sqrt(1/2), 0.0]\n",
    "titolo3 = r\"$f(x,y) = x^4-x^2+y^2$\"\n",
    "\n",
    "path_gd_m, path_new_m = get_solutions(f3, df3, hess_f3, punto3_pos, punto3_neg, maxit, tolleranza_f, tolleranza_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a79f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradiente\n",
    "graph_generator(x_range3, y_range3, f3, path_gd_m, path_new_m, tmin3, titolo3)\n",
    "error_graph(f3, df3, path_gd_m, path_new_m, titolo3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6196fa8",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Minimi Quadrati Lineari: $f(x) = \\frac12||Ax-b||_2^2$ con $A \\in \\R^{n \\times n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19d1e799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [0.99625704 1.00355389 1.00625273 0.99168709 1.00979348 1.00182711\n",
      " 1.01014478 0.99395728 1.00008295 0.99502734 0.9858794  0.99914372\n",
      " 1.01017622 1.00076338 0.98224393 0.9993252  0.99016675 0.98258641\n",
      " 1.01481681 1.00137066 1.00096128 1.00664415 1.00600673 1.01206293\n",
      " 0.99271109 0.99909923 1.00198527 0.99908576 1.00704416 0.99822909\n",
      " 1.01263554 1.003518   1.00757365 1.00189956 0.99148729 1.01555899\n",
      " 0.99079325 0.99727536 1.01353544 1.00038796 0.98921114 1.01617054\n",
      " 0.99864073 0.9820591  0.98926317 0.98452137 1.00445715 0.99545007\n",
      " 1.00538258 0.99424881]\n",
      "Numero iterazioni: 2000\n",
      "Condizione di uscita: maxit\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1.]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo del Gradiente Stocastico (SGD) ---\n",
      "Soluzione: [0.9818633  1.0268749  1.01615164 0.97604503 1.04035015 0.99792692\n",
      " 0.95176016 0.99435469 0.9786316  0.98004009 0.93210156 0.94021268\n",
      " 1.00968727 1.06365436 0.93841558 0.96650018 0.96700585 0.98035323\n",
      " 0.93287558 1.06009377 1.03101323 0.99725955 1.02772704 1.02433966\n",
      " 1.00881395 0.94895533 1.05610191 0.95632054 1.05496768 0.99917013\n",
      " 1.04094971 1.07402074 0.98410995 1.102974   0.96549852 0.97913536\n",
      " 0.93918263 1.01091434 1.07193447 0.98966903 1.0067806  0.99590679\n",
      " 0.96735098 1.00326809 0.98569756 0.94143859 1.06131049 1.01106421\n",
      " 0.92612239 0.96627099]\n",
      "Numero iterazioni: 20000\n",
      "Condizione di uscita: maxit\n",
      "Epoche fatte: 2000\n"
     ]
    }
   ],
   "source": [
    "def f4(x, A, b):\n",
    "    e = A @ x -b\n",
    "    norm_sq = np.dot(e, e)\n",
    "    return 1/2 * norm_sq\n",
    "\n",
    "def df4(X, A, b):\n",
    "    return A.T @ (A @ X - b)\n",
    "\n",
    "def hess_f4(A):\n",
    "    return A.T @ A\n",
    "\n",
    "def sdf4(x, indices, A, b):\n",
    "    A_batch= A[indices, :]\n",
    "    b_batch= b[indices]\n",
    "    \n",
    "    return A_batch.T @ (A_batch @ x - b_batch)\n",
    "\n",
    "n = 50 \n",
    "A = np.random.rand(n,n) \n",
    "b = A @ np.ones(n) \n",
    "X_start_1 = np.zeros(n) \n",
    "X_start_2 = np.random.randn(n)\n",
    "\n",
    "title_f4 = r\"$f(x) = \\frac{1}{2}\\|Ax-b\\|_2^2 \\text{ con } A \\in \\mathbb{R}^{n \\times n}$\"\n",
    "maxit = 2000\n",
    "maxit_SGD = 20000\n",
    "tol_grad = 1e-3\n",
    "tol_step = 1e-6\n",
    "batch_k = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "f_min_sq = lambda x: f4(x, A, b) # Equivalente a scrivere def f_min_sq(x): return f4(x, A, b)\n",
    "df_min_sq = lambda x: df4(x, A, b) \n",
    "h_min_sq = lambda x: hess_f4(A)\n",
    "sdf_min_sq = lambda x, idx: sdf4(x, idx, A, b)\n",
    "\n",
    "path_gd_f4, path_newton_f4 = get_solutions(f_min_sq, df_min_sq, h_min_sq, X_start_1, X_start_2, maxit, tol_grad, tol_step)\n",
    "path_sgd_f4 = get_SGD_solutions(df_min_sq, sdf_min_sq, X_start_1, n, maxit_SGD, tol_grad, tol_step, batch_k, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5005e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_graph(f_min_sq, df_min_sq, path_gd_f4, path_newton_f4, title_f4, path_sgd_f4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b9f2c0",
   "metadata": {},
   "source": [
    "Notiamo che newton è velocissimo. Questo perchè in funzioni quadrate, salta direttamente in quel punto.\n",
    "Il grafico del gradiente è rumoroso, perchè nella iterazione k fa un passo fuori rotta, il grafico sale, per poi essere ritornato nella iterazione k+1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a619d",
   "metadata": {},
   "source": [
    "---\n",
    "# 7 Minimi quadrati con regolarizzazione: $f(x) = \\frac{1}{2}||Ax-b||^2_2 + \\lambda ||x||^2_2$, con $\\lambda \\in [0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f3cbb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [1.09421764 1.02032915 0.98984194 0.97438675 1.00971359 1.04545569\n",
      " 1.09582169 0.91199246 0.96950708 0.99887427 1.01150332 1.01955691\n",
      " 1.05798793 0.9596553  0.91455621 1.07600929 1.10620473 1.02498181\n",
      " 0.91155935 1.12399844 1.01124341 1.03585899 0.95155458 0.94081766\n",
      " 0.89462379 0.91183107 0.96745449 1.12894876 0.92117308 0.9785689\n",
      " 1.03305295 0.88022418 0.88126336 0.92863784 1.04620917 1.10802152\n",
      " 1.01808081 0.99435567 0.96735169 1.10158136 0.99500153 0.95350467\n",
      " 0.9991381  1.10595702 0.9735715  1.02307288 0.93558356 0.99619668\n",
      " 1.01273769 0.99838875]\n",
      "Numero iterazioni: 2000\n",
      "Condizione di uscita: maxit\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [1.0960548  1.01941702 0.98959346 0.97579331 1.00896934 1.04624707\n",
      " 1.09608428 0.91439047 0.96950788 0.99787836 1.01136811 1.01925969\n",
      " 1.06034389 0.95911814 0.91310555 1.07524609 1.10709169 1.0262\n",
      " 0.91162969 1.1233173  1.01330904 1.03546912 0.95132799 0.94116699\n",
      " 0.89504282 0.91072567 0.96742154 1.12855332 0.92016393 0.97920561\n",
      " 1.03220884 0.87980942 0.88134003 0.92770331 1.04692836 1.10985343\n",
      " 1.01726186 0.99330486 0.96674226 1.10297115 0.9945008  0.95440912\n",
      " 0.99800025 1.10640908 0.97194218 1.02343603 0.93616737 0.99595209\n",
      " 1.01195522 0.99716681]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo del Gradiente Stocastico (SGD) ---\n",
      "Soluzione: [1.00227412 0.98505184 0.99364492 0.9320732  0.99484761 0.98703484\n",
      " 1.03977524 0.9571496  0.95566391 1.00926797 0.95438017 1.02212348\n",
      " 0.99555763 1.00731557 1.03228785 0.98450751 1.04025564 0.97773496\n",
      " 1.02355277 1.09631559 0.90623642 0.99615473 1.01556825 0.98355357\n",
      " 0.95763613 0.97111812 0.95361514 1.05855703 1.00718767 0.95296768\n",
      " 1.06522526 0.98467542 0.91020565 0.97117188 1.02130176 0.97771911\n",
      " 1.05646006 0.9805699  0.98648492 1.02582478 0.93393939 0.99161913\n",
      " 1.03006844 0.98801112 1.04399201 1.02930365 0.93453732 1.0019236\n",
      " 1.00717712 1.04290665]\n",
      "Numero iterazioni: 20000\n",
      "Condizione di uscita: maxit\n",
      "Epoche fatte: 2000\n"
     ]
    }
   ],
   "source": [
    "# Definisco la funzione\n",
    "def f5(x, A, b, lambda_val):\n",
    "    e = A @ x -b\n",
    "    errore_ls = 1/2 * np.dot(e, e)\n",
    "    errore_reg = lambda_val*np.dot(x,x)\n",
    "    return errore_ls + errore_reg\n",
    "\n",
    "# Questa calcola il GRADIENTE COMPLETO (Batch)\n",
    "def df5(x, A, b, lambda_val):\n",
    "    grad_ls = A.T @ (A @ x - b)\n",
    "    grad_reg = 2*lambda_val*x\n",
    "    return grad_ls +  grad_reg\n",
    "\n",
    "# Hessiana\n",
    "def hess_f5(A, lambda_val):\n",
    "    return A.T @ A + 2 * lambda_val * np.identity(n)\n",
    "\n",
    "# Questa calcola il GRADIENTE STOCASTICO (Mini-Batch)\n",
    "def sdf5(x, indices, A, b, lambda_val):\n",
    "    A_batch= A[indices, :]\n",
    "    b_batch= b[indices]\n",
    "\n",
    "    grad_ls_stoc = A_batch.T @ (A_batch @ x - b_batch)\n",
    "    grad_reg = 2 * lambda_val * x\n",
    "    \n",
    "    return grad_ls_stoc + grad_reg\n",
    "\n",
    "n = 50 # siamo in R^50\n",
    "A = np.random.rand(n,n) # A è una matrice n x n con valori casuali\n",
    "X_true = np.ones(n)\n",
    "b = A @ X_true + 0.1 * np.random.randn(n) # rendiamo b in modo che b= A(vettori 1)+rumore\n",
    "lambda_val = 0.1 \n",
    "\n",
    "titolo_f5 = r\"$f(x) = \\frac{1}{2}\\|Ax-b\\|_2^2 + \\lambda \\|x\\|_2^2 \\text{ con } \\lambda \\in [0,1]$\"\n",
    "X_start1 = np.zeros(n)      # Punto di partenza\n",
    "X_start2 = np.ones(n)      # Punto di partenza\n",
    "maxit = 2000\n",
    "maxit_SGD = 20000         # L'SGD richiede molti più passi del GD\n",
    "batch_k = 5                # Dimensione del mini-batch\n",
    "learning_rate = 0.001      # Alpha (deve essere molto piccolo)\n",
    "# NOTA: Una epoca è fatta ad ogni 10 iterazioni, perchè n/batch_k = 50 / 5 = 10\n",
    "\n",
    "f_min_sqr = lambda x: f5(x, A, b, lambda_val)\n",
    "df_min_sqr = lambda x: df5(x, A, b, lambda_val)\n",
    "h_min_sqr = lambda x: hess_f5(A, lambda_val)\n",
    "sdf_min_sqr = lambda x, idx: sdf5(x, idx, A, b, lambda_val)\n",
    "\n",
    "path_gd_f5, path_newton_f5 = get_solutions(f_min_sqr, df_min_sqr, h_min_sqr, X_start1, X_start2, maxit, tol_grad, tol_step)\n",
    "path_sgd_f5 = get_SGD_solutions(df_min_sqr, sdf_min_sqr, X_start1, n, maxit_SGD, tol_grad, tol_step, batch_k, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5039254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_graph(f_min_sqr, df_min_sqr, path_gd_f5, path_newton_f5, titolo_f5, path_sgd_f5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379d5c3",
   "metadata": {},
   "source": [
    "---\n",
    "# 8 $f(x) = \\sum \\limits_{i=1}^n (x_i-i)^2-\\sum \\limits_{i=1}^n \\ln(x_i)$, con $x_i>0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "331a89f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [ 1.36585366  2.22474227  3.1583123   4.12132034  5.09807621  6.082207\n",
      "  7.07071421  8.0620192   9.05521679 10.04975247 11.04526825 12.04152299\n",
      " 13.03834842 14.03562364 15.03325959 16.0311892  17.02936105 18.02773504\n",
      " 19.02627944 20.02496883 21.02378259 22.02270384 23.02171862 24.02081528\n",
      " 25.01998403 26.01921657 27.01850583 28.01784577 29.01723114 30.01665742\n",
      " 31.01612065 32.01561738 33.01514456 34.01469953 35.01427989 36.01388353\n",
      " 37.01350858 38.01315334 39.0128163  40.0124961  41.0121915  42.01190139\n",
      " 43.01162476 44.0113607  45.01110837 46.010867   47.01063589 48.01041441\n",
      " 49.01020196 50.009998  ]\n",
      "Numero iterazioni: 6\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [ 1.3660254   2.22474487  3.15831235  4.12132032  5.0980762   6.082207\n",
      "  7.07071421  8.0620192   9.05521678 10.04975247 11.04526825 12.04152298\n",
      " 13.03834842 14.03562364 15.03325959 16.0311892  17.02936105 18.02773504\n",
      " 19.02627944 20.02496883 21.02378259 22.02270384 23.02171862 24.02081528\n",
      " 25.01998403 26.01921657 27.01850583 28.01784577 29.01723114 30.01665742\n",
      " 31.01612065 32.01561738 33.01514456 34.01469953 35.01427989 36.01388353\n",
      " 37.01350858 38.01315334 39.0128163  40.0124961  41.0121915  42.01190139\n",
      " 43.01162476 44.0113607  45.01110837 46.010867   47.01063589 48.01041441\n",
      " 49.01020196 50.009998  ]\n",
      "Numero iterazioni: 3\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo del Gradiente Stocastico (SGD) ---\n",
      "Soluzione: [ 1.52960244  1.96712917  2.46654654  3.08084647  3.70903408  4.36669024\n",
      "  5.05498869  5.60813734  6.06627692  6.83613042  7.5133715   7.91804866\n",
      "  8.77951535  9.37987171  9.98360593 10.49264723 11.13849852 11.6198008\n",
      " 12.27609744 12.80146491 13.64283903 14.07061223 14.91715596 15.33796381\n",
      " 15.88161511 16.59552514 17.20265545 17.88418841 18.40390466 18.98949273\n",
      " 19.9053026  20.24587446 21.0246308  21.69589537 22.03321295 22.95428211\n",
      " 23.48627102 24.01231562 24.54789758 25.20701933 25.79205503 26.48511328\n",
      " 27.13989629 27.81334717 28.26521835 28.90286701 29.73961096 30.12632828\n",
      " 30.65346338 31.3515617 ]\n",
      "Numero iterazioni: 10000\n",
      "Condizione di uscita: maxit\n",
      "Epoche fatte: 2000\n"
     ]
    }
   ],
   "source": [
    "def f6 (x, idx):\n",
    "    if np.any(x <= 0):\n",
    "        return np.inf # ritorna infinito se x non è nel dominio (ln(x) con x<=0)\n",
    "\n",
    "    sum1 = np.sum((x-idx)**2)\n",
    "    sum2 = np.sum(np.log(x))\n",
    "    return sum1 - sum2\n",
    "    \n",
    "def df6 (x, idx):\n",
    "    return 2 * (x - idx) - 1/x\n",
    "\n",
    "def hess_f6 (x):\n",
    "    HESS = 2+(1/(x**2))\n",
    "    return np.diag(HESS)\n",
    "\n",
    "def sdf6(x, mini_batch, n, idx):\n",
    "\n",
    "    x_batch = x[mini_batch]\n",
    "    idx_batch = idx[mini_batch]\n",
    "    grad_stoc = np.zeros(n)\n",
    "    \n",
    "    grad_batch_components = 2 * (x_batch - idx_batch) - (1 / x_batch)\n",
    "    \n",
    "    grad_stoc[mini_batch] = grad_batch_components\n",
    "    \n",
    "    return grad_stoc\n",
    "\n",
    "title_f6 = r\"$f(x) = \\sum_{i=1}^n (x_i-i)^2 - \\sum_{i=1}^n \\ln(x_i)$\"\n",
    "n = 50\n",
    "maxit_SGD = 10000\n",
    "batch_k = 10\n",
    "idx = np.arange(1, n+1)\n",
    "X_start1 = np.ones(n)\n",
    "X_start2 = np.random.rand(n) + 1\n",
    "\n",
    "f_lambda = lambda x: f6(x, idx)\n",
    "df_lambda = lambda x: df6(x, idx)\n",
    "h_lambda = lambda x: hess_f6(x)\n",
    "sdf_lambda = lambda x, k: sdf6(x, k, n, idx)\n",
    "\n",
    "path_gd_f6, path_newton_f6 = get_solutions(f_lambda, df_lambda, h_lambda, X_start1, X_start2, maxit, tol_grad, tol_step)\n",
    "path_sgd_f6 = get_SGD_solutions(df_lambda, sdf_lambda, X_start2, n, maxit_SGD, tol_grad, tol_step, batch_k, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ef8e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_graph(f_lambda, df_lambda, path_gd_f6, path_newton_f6, title_f6, path_sgd_f6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89828313",
   "metadata": {},
   "source": [
    "---\n",
    "# 9 $f(x) = \\sum \\limits_{i=1}^n (x_i-b_i)^2+c\\sum \\limits_{i=1}^n \\ln(x_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81f0fb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [ 1.61818182  2.41421393  3.30277564  4.23606798  5.1925824   6.16227766\n",
      "  7.14005494  8.12310563  9.10977223 10.09901951 11.09016994 12.08276253\n",
      " 13.07647322 14.07106781 15.06637298 16.06225775 17.05862138 18.05538514\n",
      " 19.05248659 20.04987562 21.04751155 22.04536102 23.04339638 24.04159458\n",
      " 25.0399362  26.03840481 27.03698637 28.03566885 29.03444185 30.03329638\n",
      " 31.03222457 32.03121954 33.03027525 34.02938637 35.02854814 36.02775638\n",
      " 37.02700731 38.02629759 39.02562419 40.02498439 41.02437575 42.02379604\n",
      " 43.02324325 44.02271555 45.02221126 46.02172887 47.02126697 48.0208243\n",
      " 49.02039967 50.01999201]\n",
      "Numero iterazioni: 9\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [ 1.61803399  2.41421356  3.30277564  4.23606798  5.1925824   6.16227766\n",
      "  7.14005494  8.12310406  9.10977223 10.09901951 11.09016994 12.08276253\n",
      " 13.07647322 14.07106781 15.06637298 16.06225775 17.05862138 18.05538514\n",
      " 19.05248659 20.04987562 21.04751155 22.04536102 23.04339638 24.04159458\n",
      " 25.0399362  26.03840481 27.03698637 28.03566885 29.03444185 30.03329638\n",
      " 31.03222457 32.03121954 33.03027525 34.02938637 35.02854814 36.02775638\n",
      " 37.02700731 38.02629759 39.02562419 40.02498439 41.02437575 42.02379604\n",
      " 43.02324325 44.02271555 45.02221126 46.02172887 47.02126697 48.0208243\n",
      " 49.02039967 50.01999201]\n",
      "Numero iterazioni: 5\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo del Gradiente Stocastico (SGD) ---\n",
      "Soluzione: [ 1.46922331  1.99503749  2.55125993  3.12503375  3.70984362  4.30175356\n",
      "  4.89874728  5.49932277  6.10274177  6.70810609  7.31525376  7.92325375\n",
      "  8.53277375  9.14311509  9.75392158 10.36544483 10.97805813 11.59044834\n",
      " 12.20337315 12.81659144 13.43026885 14.04388203 14.65821101 15.27211738\n",
      " 15.88700943 16.50142838 17.11636763 17.73105933 18.34611731 18.96142573\n",
      " 19.57680281 20.19189927 20.80734979 21.4229735  22.03845966 22.65424101\n",
      " 23.2697837  23.88545964 24.50150398 25.11671989 25.73324969 26.34921326\n",
      " 26.96509802 27.58103207 28.19710623 28.81320902 29.42989234 30.04635837\n",
      " 30.66192498 31.27747602]\n",
      "Numero iterazioni: 10000\n",
      "Condizione di uscita: maxit\n",
      "Epoche fatte: 2000\n"
     ]
    }
   ],
   "source": [
    "def f7(x,c,b):\n",
    "\n",
    "    sum1 = np.sum((x-b)**2)\n",
    "    sum2 = c*np.sum(np.log(x))\n",
    "\n",
    "    return sum1+sum2\n",
    "\n",
    "def df7(x,c,b):\n",
    "    return 2*(x-b)+c*(1/x)\n",
    "\n",
    "def hess_f7(x,c):\n",
    "    HESS =2-c*(1/(x**2))\n",
    "    return np.diag(HESS)\n",
    "\n",
    "def sdf7(x, c, b, n, mini_batch):\n",
    "    x_batch = x[mini_batch]\n",
    "    b_batch = b[mini_batch]\n",
    "    grad_stoc = np.zeros(n)\n",
    "    \n",
    "    grad_batch_components = 2 * (x_batch - b_batch) + c*(1 / x_batch)\n",
    "    grad_stoc[mini_batch] = grad_batch_components\n",
    "    \n",
    "    return grad_stoc\n",
    "\n",
    "title_f7 = r\"$f(x) = \\sum_{i=1}^n (x_i-b_i)^2+c\\sum_{i=1}^n \\ln(x_i)$\"\n",
    "b = np.arange(1, n+1)\n",
    "X_start1 = np.ones(n)\n",
    "X_start2 = np.random.rand(n)+0.1\n",
    "c= -2.0 # DEVE essere negativo\n",
    "\n",
    "f_c = lambda x: f7(x, c, b)\n",
    "df_c = lambda x: df7(x, c, b)\n",
    "h_c = lambda x: hess_f7(x, c)\n",
    "sdf_c = lambda x, k: sdf7(x, c, b, n, k)\n",
    "\n",
    "path_gd_f7, path_newton_f7 = get_solutions(f_c, df_c, h_c, X_start1, X_start2, maxit, tol_grad, tol_step)\n",
    "path_sgd_f7 = get_SGD_solutions(f_c, sdf_c, X_start1, n, maxit_SGD, tol_grad, tol_step, batch_k, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e480eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_graph(f_c, df_c, path_gd_f7, path_newton_f7, title_f7, path_sgd_f7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CN25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
