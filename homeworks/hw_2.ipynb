{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d465100e",
   "metadata": {},
   "source": [
    "# Analisi e Implementazione di Metodi di Ottimizzazione\n",
    "\n",
    "Creiamo i tre metodi di ottimizzazione per trovare i minimi di diverse funzioni:\n",
    "1.  **Metodo del Gradiente (Gradient Descent - GD)**\n",
    "2.  **Metodo di Newton**\n",
    "3.  **Metodo del Gradiente Stocastico (SGD)**\n",
    "\n",
    "Viene utilizzata una strategia di **Backtracking Line Search** per la determinazione del passo di apprendimento (alpha)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ebaf8",
   "metadata": {},
   "source": [
    "## 1 Import delle librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "30a3ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "from mpl_toolkits.mplot3d import Axes3D # Per grafici 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d34ad9f",
   "metadata": {},
   "source": [
    "## 2 Implementazione degli Algoritmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caaa8bd",
   "metadata": {},
   "source": [
    "## 2.1 Funzione Backtracking (Line Search)\n",
    "\n",
    "Questa funzione implementa la ricerca del passo $\\alpha$ tramite backtracking, basandosi sulla condizione di Armijo per garantire che il risultato converge alla soluzione.\n",
    "\n",
    "**Parametri**:\n",
    "- $f$: La funzione obiettivo\n",
    "- $df$: Il gradiente della funzione obiettivo\n",
    "- $X_k$: Il punto corrente (vettore)\n",
    "- $p_k$: La direzione di discesa (vettore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "83aebdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtracking per alpha\n",
    "def backtracking(f, df, X_k, p_k):\n",
    "    alpha = 1\n",
    "    rho = 1/2 # Fattore di riduzione per alpha\n",
    "    c1 = 0.25\n",
    "\n",
    "    # Calcola il gradiente del punto passato\n",
    "    grad_k = df(X_k)\n",
    "\n",
    "    # Calcola il prodotto scalare\n",
    "    grad_dot_p = np.dot(grad_k, p_k)\n",
    "    \n",
    "    # Condizione Di Armijo: Il loop \"while\" continua FINCHÉ la condizione è VIOLATA (segno >)\n",
    "    while f(X_k + alpha * p_k) > f(X_k) + c1 * alpha * grad_dot_p:\n",
    "        alpha = rho * alpha\n",
    "\n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad29c54",
   "metadata": {},
   "source": [
    "## 2.2 Metodo del gradiente\n",
    "\n",
    "La direzione di discesa $p_k = -\\nabla f(x)$, ovvero l'antigradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "38db714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo del gradiente\n",
    "def GD (f, df, X_old, maxit, tol_f, tol_x):\n",
    "    \n",
    "    count=0\n",
    "    dim = len(X_old) # Calcolo la dimensione di X_old\n",
    "\n",
    "    # Dichiare una matrice di 0 di dimensione maxit + 1 per dim\n",
    "    fun_history = np.zeros((maxit + 1, dim))\n",
    "    fun_history[0,] = X_old\n",
    "\n",
    "    exit_flag = 'maxit' # Flag di uscita di default\n",
    "\n",
    "    current_grad_norm=np.linalg.norm(df(X_old))\n",
    "\n",
    "    # Controllo se ho superato le iterazioni o se ho raggiunto la tolleranza desiderata\n",
    "    while count < maxit and current_grad_norm > tol_f:\n",
    "        # Direzione di discesa\n",
    "        p_k=-df(X_old)\n",
    "        \n",
    "        # Backtracking di alpha\n",
    "        alpha=backtracking(f, df, X_old, p_k)\n",
    "\n",
    "        # Calcolo di X_k+1\n",
    "        X_new = X_old + alpha * p_k\n",
    "\n",
    "        # Controllo se sto facendo progressi (tolleranza x)\n",
    "        if np.linalg.norm(X_new-X_old) < tol_x:\n",
    "            exit_flag = 'tol_x'\n",
    "            break\n",
    "\n",
    "        # Ricalcolo la norma del gradiente\n",
    "        current_grad_norm=np.linalg.norm(df(X_new))\n",
    "\n",
    "        # Aggiorno per l'iterazione successiva\n",
    "        X_old = X_new\n",
    "        count+=1\n",
    "\n",
    "        # Aggiungo il valore della funzione in x_k nella matrice dei risultati \n",
    "        fun_history[count] = X_new\n",
    "\n",
    "    # Controllo se l'uscita è dovuta a tol_f (norma del gradiente)\n",
    "    if exit_flag == 'maxit' and current_grad_norm <= tol_f:\n",
    "        exit_flag = 'tol_f'\n",
    "\n",
    "    # Rimuovo le righe non utilizzate dalla cronologia\n",
    "    if count<maxit:\n",
    "        fun_history = fun_history[:count+1]\n",
    "\n",
    "    return X_old, count, fun_history, exit_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af6510",
   "metadata": {},
   "source": [
    "## 2.2 Metodo Newton\n",
    "\n",
    "La direzione di discesa è $p_k = - \\frac{\\nabla f_k}{H_k} $\n",
    "\n",
    "Nota: questo metodo è molto lento perchè ad ogni iterazione si deve moltiplicare il gradiente e l'hessiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "75834970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo di Newton\n",
    "def Newton (f, df, hess_f, X_old, maxit, tol_f, tol_x):\n",
    "    \n",
    "    count=0\n",
    "    dim = len(X_old) \n",
    "    \n",
    "    # Cronologia\n",
    "    fun_history = np.zeros((maxit + 1, dim))\n",
    "    fun_history[0,]=X_old\n",
    "\n",
    "    exit_flag = 'maxit'\n",
    "\n",
    "    current_grad_norm=np.linalg.norm(df(X_old))\n",
    "\n",
    "    while count < maxit and current_grad_norm > tol_f:\n",
    "\n",
    "        # Calcolo l'Hessiana\n",
    "        H_k = hess_f(X_old)\n",
    "\n",
    "        # Calcolo il gradiente\n",
    "        grad_k = df(X_old)\n",
    "\n",
    "        # Direzione di discesa\n",
    "        p_k=np.linalg.solve(H_k, -grad_k)\n",
    "\n",
    "        # Backtracking di alpha\n",
    "        alpha=backtracking(f, df, X_old, p_k)\n",
    "\n",
    "        # Calcolo di X_k+1\n",
    "        X_new = X_old + alpha * p_k\n",
    "\n",
    "        # Controllo se sto facendo progressi (tolleranza x)\n",
    "        if np.linalg.norm(X_new-X_old) < tol_x:\n",
    "            exit_flag = 'tol_x'\n",
    "            break\n",
    "\n",
    "        # Ricalcolo la norma del gradiente\n",
    "        current_grad_norm=np.linalg.norm(df(X_new))\n",
    "\n",
    "        # Aggiorno per l'iterazione successiva\n",
    "        X_old = X_new\n",
    "        count+=1\n",
    "        fun_history[count] = X_new\n",
    "\n",
    "    \n",
    "    if exit_flag == 'maxit' and current_grad_norm <= tol_f:\n",
    "        exit_flag = 'tol_f'\n",
    "\n",
    "    if count<maxit:\n",
    "        fun_history = fun_history[:count+1]\n",
    "\n",
    "    return X_old, count, fun_history, exit_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e85ae8",
   "metadata": {},
   "source": [
    "## 2.4 Metodo gradiente stocastico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "764e6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo del gradiente stocastico\n",
    "def SGD (df_completo, df_stocastico, X_old, n_samples, max_steps, tol_f, tol_x, k, alpha):\n",
    "\n",
    "    # Calcolo la dimensione di X_old\n",
    "    dim = len(X_old)\n",
    "\n",
    "    # Inizializziamo il dataset S_k\n",
    "    S_k = np.arange(n_samples)\n",
    "\n",
    "    # Per la condizione di uscita\n",
    "    step_count=0\n",
    "    epoch_count=0\n",
    "\n",
    "    # Dichiare una matrice di 0 di dimensione maxit+1 per dim\n",
    "    fun_history = np.zeros((max_steps + 1, dim))\n",
    "    fun_history[0,]=X_old\n",
    "\n",
    "    # Caso che tutto va bene, esce per maxit\n",
    "    exit_flag = 'maxit'\n",
    "\n",
    "    current_grad_norm=np.linalg.norm(df_completo(X_old))\n",
    "\n",
    "    # Controllo se ho superato le iterazioni o se ho raggiunto la tolleranza desiderata\n",
    "    while step_count < max_steps and current_grad_norm > tol_f :\n",
    "\n",
    "        # Randomizziamo gli indici del mini-batch\n",
    "        np.random.shuffle(S_k)\n",
    "\n",
    "        epoch_count += 1\n",
    "\n",
    "        for i in range (0, n_samples, k):\n",
    "\n",
    "            indices = S_k[i:i+k]\n",
    "\n",
    "            # Selezioniamo il mini-batch\n",
    "            # A_batch = A[indices, :]\n",
    "            # b_batch = b[indices]\n",
    "            \n",
    "            # Calcola la direzione (gradiente stocastico)\n",
    "            # grad_stoc = A_batch.T @ (A_batch @ X_old - b_batch)\n",
    "            grad_stoc = df_stocastico(X_old, indices)\n",
    "            p_k = -grad_stoc\n",
    "\n",
    "            X_new = X_old + alpha * p_k\n",
    "\n",
    "            # Controllo se sto facendo progressi (tolleranza x)\n",
    "            if np.linalg.norm(X_new - X_old) < tol_x:\n",
    "                exit_flag = 'tol_x'\n",
    "                break\n",
    "\n",
    "            # Aggiorno per l'iterazione successiva\n",
    "            X_old = X_new\n",
    "            step_count += 1\n",
    "            fun_history[step_count] = X_new\n",
    "\n",
    "            # Controllo se ho superato il numero di passi\n",
    "            if step_count >= max_steps:\n",
    "                break # Interrompe il 'for' loop\n",
    "\n",
    "        # Ricalcolo la norma del gradiente \"completo\" per il check\n",
    "        current_grad_norm=np.linalg.norm(df_completo(X_new))\n",
    "\n",
    "        if exit_flag == 'tol_x':\n",
    "            break\n",
    "\n",
    "    # In caso siamo usciti per tolleranza\n",
    "    if exit_flag == 'maxit' and current_grad_norm <= tol_f:\n",
    "        exit_flag = 'tol_f'\n",
    "\n",
    "    # Tronciamo l'array inizializzato all'inizio\n",
    "    if step_count<max_steps: \n",
    "        fun_history = fun_history[:step_count+1]\n",
    "\n",
    "    return X_old, step_count, fun_history, exit_flag, epoch_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a46d2a",
   "metadata": {},
   "source": [
    "## 2.5 Funzione per la generazione dei grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "bc521666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_generator (x0, y0, f,path_history_gd, path_history_newton, tmin, titolo):\n",
    "    X_mesh, Y_mesh = np.meshgrid(x0, y0)\n",
    "    Z = f([X_mesh, Y_mesh])\n",
    "\n",
    "    # Memorizzo il percorso delle x e y\n",
    "    path_x_gd = path_history_gd[:, 0]\n",
    "    path_y_gd = path_history_gd[:, 1]\n",
    "    path_x_new = path_history_newton[:, 0]\n",
    "    path_y_new = path_history_newton[:, 1]\n",
    "\n",
    "    # Definisco la figura\n",
    "    fig = plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # <-- GENERAZIONE GRAFICO 3D -->\n",
    "\n",
    "    # Aggiungo subplot 3D\n",
    "    axs1 = fig.add_subplot(1,2,1,projection='3d')\n",
    "\n",
    "    # Disegno la superficie\n",
    "    axs1.plot_surface(X_mesh, Y_mesh, Z, cmap='viridis', rstride=1, cstride=1, linewidth=0, antialiased=True, alpha=0.7)\n",
    "\n",
    "    # Aggiunge il percorso anche nel grafico 3D gradiente\n",
    "    z_path_gd = f([path_x_gd, path_y_gd])\n",
    "    axs1.plot(path_x_gd, path_y_gd, z_path_gd, 'r-o', label='Percorso del Gradiente')\n",
    "\n",
    "    # Aggiunge il percorso anche nel grafico 3D newton\n",
    "    z_path_newton = f([path_x_new, path_y_new])\n",
    "    axs1.plot(path_x_new, path_y_new, z_path_newton, 'b-o', label='Percorso di Newton')\n",
    "\n",
    "    # Evidenzia il punto iniziale e finale gradiente\n",
    "    axs1.plot(path_x_gd[0], path_y_gd[0], 'go', label=f'Start: ({path_x_gd[0]}, {path_y_gd[0]})') # punto iniziale\n",
    "    axs1.plot(path_x_gd[-1], path_y_gd[-1], 'rx', label=f'End: ({path_x_gd[-1]:.2f}, {path_y_gd[-1]:.2f})', markersize=16) # punto finale\n",
    "\n",
    "    # Evidenzia il punto iniziale e finale newton\n",
    "    axs1.plot(path_x_new[0], path_y_new[0], 'go', label=f'Start: ({path_x_new[0]}, {path_y_new[0]})') # punto iniziale\n",
    "    axs1.plot(path_x_new[-1], path_y_new[-1], 'rx', label=f'End: ({path_x_new[-1]:.2f}, {path_y_new[-1]:.2f})', markersize=16) # punto finale\n",
    "\n",
    "    axs1.plot(tmin[0], tmin[1], 'b*', markersize=15, label=f\"Minimo Teorico ({tmin[0]}, {tmin[1]})\") # Minimo teorico\n",
    "\n",
    "    # Aggiungo label e titolo\n",
    "    axs1.set_title(f\"{titolo}\")\n",
    "    axs1.set_xlabel(\"x\")\n",
    "    axs1.set_ylabel(\"y\")\n",
    "    axs1.set_zlabel(\"z\")\n",
    "    axs1.legend()\n",
    "    axs1.view_init(elev=30, azim=135) # Imposta un angolo di visuale\n",
    "\n",
    "    # <--- GENERAZIONE GRAFICO 2D --->\n",
    "\n",
    "    # Definisco il grafico 2D\n",
    "    axs0 = fig.add_subplot(1,2,2)\n",
    "\n",
    "    # Disegna le curve di livello\n",
    "    c1 = axs0.contour(X_mesh, Y_mesh, Z, levels=50, cmap='viridis', alpha=0.7)\n",
    "    fig.colorbar(c1, label='Valore di $f(x,y)$')\n",
    "\n",
    "    # Disegna il percorso\n",
    "    axs0.plot(path_x_gd, path_y_gd, 'r-o', label='Percorso del Gradiente')\n",
    "    axs0.plot(path_x_new, path_y_new, 'b-o', label='Percorso di Newton')\n",
    "\n",
    "    # Evidenzia il punto iniziale e finale gradiente\n",
    "    axs0.plot(path_x_gd[0], path_y_gd[0], 'go', label=f'Start: ({path_x_gd[0]}, {path_y_gd[0]})') # punto iniziale\n",
    "    axs0.plot(path_x_gd[-1], path_y_gd[-1], 'rx', label=f'End: ({path_x_gd[-1]:.2f}, {path_y_gd[-1]:.2f})', markersize=16) # punto finale\n",
    "    \n",
    "    # Evidenzia il punto iniziale e finale newton\n",
    "    axs0.plot(path_x_new[0], path_y_new[0], 'go', label=f'Start: ({path_x_new[0]}, {path_y_new[0]})') # punto iniziale\n",
    "    axs0.plot(path_x_new[-1], path_y_new[-1], 'bx', label=f'End: ({path_x_new[-1]:.2f}, {path_y_new[-1]:.2f})', markersize=16) # punto finale\n",
    "\n",
    "    axs0.plot(tmin[0], tmin[1], 'y*', markersize=15, label=f\"Minimo Teorico ({tmin[0]}, {tmin[1]})\") # Minimo teorico\n",
    "    # Etichette e titoli\n",
    "    axs0.set_title(f\"{titolo}\")\n",
    "    axs0.set_xlabel(\"x\")\n",
    "    axs0.set_ylabel(\"y\")\n",
    "    axs0.legend()\n",
    "    axs0.set_aspect('equal', adjustable='box') # \"Grafo è \"quadrato\"\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07468013",
   "metadata": {},
   "source": [
    "### 2.6 Funzione per gli output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "770d5f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solutions(f, df, h_f, x1, x2, maxit, tol_grad, tol_step):\n",
    "    print(\"--- Metodo del Gradiente (GD) ---\")\n",
    "    sol_gd, iter_gd, path_gd, exit_gd = GD(f, df, x1, maxit, tol_grad, tol_step)\n",
    "    print(f\"Soluzione: {sol_gd}\")\n",
    "    print(f\"Numero iterazioni: {iter_gd}\")\n",
    "    print(f\"Condizione di uscita: {exit_gd}\")\n",
    "\n",
    "    print(\"--- Metodo di Newton ---\")\n",
    "    sol_newton, iter_newton, path_newton, exit_newton = Newton(f, df, h_f, x2, maxit, tol_grad, tol_step)\n",
    "    print(f\"Soluzione: {sol_newton}\")\n",
    "    print(f\"Numero iterazioni: {iter_newton}\")\n",
    "    print(f\"Condizione di uscita: {exit_newton}\")\n",
    "\n",
    "    return path_gd, path_newton\n",
    "\n",
    "def get_SGD_solutions(f, sdf, x, n, maxit, tol_grad, tol_step, batch_k, learning_rate):\n",
    "    print(\"--- Metodo del Gradiente Stocastico (SGD) ---\")\n",
    "    sol_sgd, iter_sgd, path_sgd, exit_sgd, epoch = SGD(f, sdf, x, n, maxit, tol_grad, tol_step, batch_k, learning_rate)\n",
    "    print(f\"Soluzione: {sol_sgd}\")\n",
    "    print(f\"Numero iterazioni: {iter_sgd}\")\n",
    "    print(f\"Condizione di uscita: {exit_sgd}\")\n",
    "    print(f\"Epoche fatte: {epoch}\")\n",
    "\n",
    "    return path_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff313e",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3 Funzione Quadratica: $f(x, y) = (x-5)^2+(y-2)^2$\n",
    "\n",
    "Cerchiamo i punti critici della funzione.\n",
    "\n",
    "Calcoliamo la derivata rispetto a $x$ e a $y$.\n",
    "\n",
    "- $\\frac{∂x}{∂f} = ​2(x-5)$\n",
    "- $\\frac{∂y}{∂f} =​ 2(y-2)$\n",
    "\n",
    "Ponendo $x=5$ e $y=2$, troviamo un punto critico.\n",
    "Possiamo notare gli elementi della funzione sono elevati al quadrato, quindi sappiamo che il punto critico è un minimo globale.\n",
    "\n",
    "**Nota:** Nei metodi utilizzati, converge in una sola iterazione."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde6573f",
   "metadata": {},
   "source": [
    "### 3.1 Implementazione Dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "75d54457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "    return (x-5)**2+(y-2)**2\n",
    "\n",
    "def df1(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    df_dx = 2*(x-5) \n",
    "    df_dy = 2*(y-2)\n",
    "    \n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def hess_f1(X):\n",
    "    HESS=np.zeros((2,2))\n",
    "\n",
    "    HESS[0][0] = 2\n",
    "    HESS[0][1] = 0\n",
    "    HESS[1][0] = 0\n",
    "    HESS[1][1] = 2\n",
    "    return HESS    \n",
    "\n",
    "x_range1=np.linspace(-2,12,200)\n",
    "y_range1=np.linspace(-2,8,200)\n",
    "\n",
    "maxit = 300\n",
    "tolleranza_f = 1.e-6\n",
    "tolleranza_x = 1.e-6\n",
    "punto1_gd=np.array([0.0,0.0])\n",
    "punto1_new=np.array([1.0,1.0])\n",
    "tmin1 = [5.0, 2.0] # Minimo Teorico\n",
    "titolo1 = r\"$f(x, y) = (x-5)^2+(y-2)^2$\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef17cefe",
   "metadata": {},
   "source": [
    "### 3.2 Soluzioni dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "ea128d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [5. 2.]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [5. 2.]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n"
     ]
    }
   ],
   "source": [
    "path_gd, path_new = get_solutions(f1, df1, hess_f1, punto1_gd, punto1_new, maxit, tolleranza_f, tolleranza_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2096d54",
   "metadata": {},
   "source": [
    "### 3.2 Generazione Grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "c2f8ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generazione grafico\n",
    "graph_generator(x_range1, y_range1, f1, path_gd, path_new, tmin1, titolo1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687edc4a",
   "metadata": {},
   "source": [
    "--- \n",
    "## 4. Funzione di Rosenbrock: $f(x, y) = (1-x)^2+100(y-x^2)^2$\n",
    "\n",
    "Cerchiamo i punti critici della funzione.\n",
    "\n",
    "Calcoliamo la derivata rispetto a $x$ e a $y$.\n",
    "\n",
    "$\\frac{df}{dx} = -2(1-x) + 200(y-x^2)(-2x) = 2(x-1) -400x(y-x^2) $\n",
    "\n",
    "$\\frac{df}{dy} =​ 200(y-x^2)$\n",
    "\n",
    "Poniamo $y=x^2$, troviamo che $ 2(x-1) -400x(x^2-x^2)=0 \\to 2x-2=0 \\to x=1 $  \n",
    "\n",
    "Quindi l'unico punto critico è su $(1,1)$\n",
    "\n",
    "$H= \\begin{bmatrix} \n",
    "\\frac{d^2f}{dx^2} & \\frac{d^2f}{dxdy} \\\\\n",
    "\\frac{d^2f}{dydx} & \\frac{d^2f}{dy^2}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix} \n",
    " 2-400y+1200x^2& -400x \\\\\n",
    "-400x & 200\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Studiamo $\\det(H)$.\n",
    "\n",
    "$P(1,1) = 802\\cdot 200 - 160000 = 160400 - 160000 = 400 > 0$, quindi è un punto di minimo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071591da",
   "metadata": {},
   "source": [
    "### 4.1 Implementazione dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "6d99e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    return (1-x)**2+100*(y-x**2)**2\n",
    "\n",
    "def df2(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    df_dx = -2*(1-x) - 400*(y-x**2)*x\n",
    "    df_dy = 200*(y-x**2)\n",
    "    \n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def hess_f2(X):\n",
    "    HESS=np.zeros((2,2)) # Crea una matrice 2x2\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    HESS[0][0] = 2 - 400*y + 1200*x**2 # Derivata seconda rispetto a x\n",
    "    HESS[0][1] = -400*x # Derivata mista\n",
    "    HESS[1][0] = HESS[0][1] # Per teorema di Sxhwarz\n",
    "    HESS[1][1] = 200 # Derivata seconda rispetto a y\n",
    "    return HESS\n",
    "    \n",
    "\n",
    "x_range2=np.linspace(-2,2,200)\n",
    "y_range2=np.linspace(-1,3,200)\n",
    "\n",
    "maxit = 5000\n",
    "tolleranza_f = 1.e-6\n",
    "tolleranza_x = 1.e-6\n",
    "punto2_gd=np.array([0.0,0.0])\n",
    "punto2_new=np.array([0.5,0.5])\n",
    "tmin2 = [1.0,1.0]\n",
    "titolo2 = r\"$f(x, y) = (1-x)^2+100(y-x^2)^2$\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3096080e",
   "metadata": {},
   "source": [
    "### 4.2 Soluzioni dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "2d2f42ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [0.9998038  0.99960895]\n",
      "Numero iterazioni: 2200\n",
      "Condizione di uscita: tol_x\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [0.99999971 0.99999935]\n",
      "Numero iterazioni: 10\n",
      "Condizione di uscita: tol_x\n"
     ]
    }
   ],
   "source": [
    "path_gd_r, path_newton_r = get_solutions(f2, df2, hess_f2, punto2_gd, punto2_new, maxit, tolleranza_f, tolleranza_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5478e08",
   "metadata": {},
   "source": [
    "### 4.3 Generazione Grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "c04dabfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico del metodo GD\n",
    "graph_generator(x_range2, y_range2, f2, path_gd_r, path_newton_r, tmin2, titolo2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbbfc40",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 Funzione multimodale: $f(x,y) = x^4-x^2+y^2$\n",
    "\n",
    "Cerchiamo i punti critici della funzione.\n",
    "\n",
    "Calcoliamo la derivata rispetto a $x$ e a $y$.\n",
    "\n",
    "- $\\frac{df}{dx} = ​4x^3-2x = 2x(2x^2-1) $\n",
    "- $\\frac{df}{dy} =​ 2y$\n",
    "\n",
    "Notiamo che ponendo $x=0$ e $y= 0$ troviamo il nostro primo punto critico.\n",
    "\n",
    "Ponendo $y=0$, troviamo i casi in cui $2x^2-1=0$, ovvero $x= \\pm \\sqrt{\\frac12}$, ovvero $\\simeq 0.70717$\n",
    "\n",
    "Calcoliamo se questi punti critici sono punti minimi, massimi o di sella.\n",
    "\n",
    "$H= \\begin{bmatrix} \n",
    "\\frac{d^2f}{dx^2} & \\frac{d^2f}{dxdy} \\\\\n",
    "\\frac{d^2f}{dydx} & \\frac{d^2f}{dy^2}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix} \n",
    "12x^2-2 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Studiamo $ \\det(H)$.\n",
    "\n",
    "$P(0,0) = -4 < 0$, quindi è un punto di sella\n",
    "\n",
    "$P(\\sqrt{\\frac12},0) = 2(6-2) = 8 > 0$, quindi è un punto di minimo\n",
    "\n",
    "$P(-\\sqrt{\\frac12},0) = 2(6-2) = 8 >0$, quindi è un punto di minimo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46a523a",
   "metadata": {},
   "source": [
    "### 5.1 Implementazione dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "37944ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    return x**4-x**2+y**2\n",
    "\n",
    "def df3(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    df_dx = 4*x**3-2*x\n",
    "    df_dy = 2*y\n",
    "    \n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def hess_f3(X):\n",
    "    HESS=np.zeros((2,2))\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    HESS[0][0] = 12*x**2-2 # Derivata seconda rispetto a x\n",
    "    HESS[0][1] = 0 # Derivata mista\n",
    "    HESS[1][0] = HESS[0][1] # Per teorema di Sxhwarz, la derivata mista è uguale\n",
    "    HESS[1][1] = 2 # Derivata seconda rispetto a y\n",
    "    return HESS\n",
    "    \n",
    "\n",
    "x_range3=np.linspace(-1.5,1.5,200)\n",
    "y_range3=np.linspace(-1.5,1.5,200)\n",
    "\n",
    "maxit = 5000\n",
    "tolleranza_f = 1.e-6\n",
    "tolleranza_x = 1.e-6\n",
    "punto3_neg=np.array([-1.0,-1.0]) \n",
    "punto3_pos=np.array([1.0,1.0]) \n",
    "punto3_sad=np.array([0.0,5.0]) \n",
    "tmin3 = [0.0, 0.0]\n",
    "tmin3_alt = [-np.sqrt(1/2), 0.0]\n",
    "titolo3 = r\"$f(x,y) = x^4-x^2+y^2$\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bccf36",
   "metadata": {},
   "source": [
    "### 5.2 Soluzioni dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "bc864a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [0. 0.]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [-0.70710712  0.        ]\n",
      "Numero iterazioni: 4\n",
      "Condizione di uscita: tol_x\n"
     ]
    }
   ],
   "source": [
    "path_gd_m, path_new_m = get_solutions(f3, df3, hess_f3, punto3_pos, punto3_neg, maxit, tolleranza_f, tolleranza_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438c2880",
   "metadata": {},
   "source": [
    "### 5.3 Generazione Grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "663a79f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dule/anaconda3/envs/CN25/lib/python3.13/tkinter/__init__.py:862: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  func(*args)\n"
     ]
    }
   ],
   "source": [
    "# Gradiente\n",
    "graph_generator(x_range3, y_range3, f3, path_gd_m, path_new_m, tmin3, titolo3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fed289",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Generatore grafici errore\n",
    "\n",
    "D'ora in poi lavoreremo in $\\R^n$, quindi non possiamo visualizzarli con dei grafici, ma possiamo avere dei grafici per gli errori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "e83c4066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_graph(f, df, gd, newton, sgd, titolo):\n",
    "    errori_gd = [f(x_k) for x_k in gd]\n",
    "    errori_newton = [f(x_k) for x_k in newton]\n",
    "    errori_sgd = [f(x_k) for x_k in sgd]\n",
    "\n",
    "    norma_grad_gd = [np.linalg.norm(df(x_k)) for x_k in gd]\n",
    "    norma_grad_newton = [np.linalg.norm(df(x_k)) for x_k in newton]\n",
    "    norma_grad_sgd = [np.linalg.norm(df(x_k)) for x_k in sgd]\n",
    "\n",
    "    # Crea il grafico di GD e Newton\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(16, 10)) # Dimensione più bilanciata per 2x2\n",
    "\n",
    "    # --- Grafico [0, 0]: Errore (f) per GD e Newton ---\n",
    "    ax = axs[0, 0]\n",
    "    ax.plot(errori_gd, label=\"Gradient Descent (GD)\", marker='.', linestyle='-')\n",
    "    ax.plot(errori_newton, label=\"Newton Smorzato\", marker='.', linestyle='--')\n",
    "    ax.set_title(\"Valore Funzione (GD vs Newton)\")\n",
    "    ax.set_ylabel(\"Valore Funzione $f(x_k)$ (log scale)\")\n",
    "    ax.set_xlabel(\"Iterazione (k)\")\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # --- Grafico [0, 1]: Errore (f) per SGD ---\n",
    "    ax = axs[0, 1]\n",
    "    ax.plot(errori_sgd, label=\"Stochastic GD\", color='green', alpha=0.8)\n",
    "    ax.set_title(\"Valore Funzione (SGD)\")\n",
    "    ax.set_ylabel(\"Valore Funzione $f(x_k)$ (log scale)\")\n",
    "    ax.set_xlabel(\"Iterazione (k)\")\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # --- Grafico [1, 0]: Norma Gradiente per GD e Newton ---\n",
    "    ax = axs[1, 0]\n",
    "    ax.plot(norma_grad_gd, label=\"Gradient Descent (GD)\", marker='.', linestyle='-')\n",
    "    ax.plot(norma_grad_newton, label=\"Newton Smorzato\", marker='.', linestyle='--')\n",
    "    ax.set_title(\"Norma Gradiente (GD vs Newton)\")\n",
    "    # Usiamo una stringa raw (r\"...\") per il LaTeX\n",
    "    ax.set_ylabel(r\"Norma Gradiente $\\|\\nabla f(x_k)\\|$ (log scale)\")\n",
    "    ax.set_xlabel(\"Iterazione (k)\")\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # --- Grafico [1, 1]: Norma Gradiente per SGD ---\n",
    "    ax = axs[1, 1]\n",
    "    ax.plot(norma_grad_sgd, label=\"Stochastic GD\", color='green', alpha=0.8)\n",
    "    ax.set_title(\"Norma Gradiente (SGD)\")\n",
    "    ax.set_ylabel(r\"Norma Gradiente $\\|\\nabla f(x_k)\\|$ (log scale)\")\n",
    "    ax.set_xlabel(\"Iterazione (k)\")\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # --- 3. Titolo finale e Layout ---\n",
    "    fig.suptitle(f\"Analisi Convergenza per {titolo}\", fontsize=18)\n",
    "    \n",
    "    # Applica tight_layout UNA SOLA VOLTA, con l'aggiustamento per il suptitle\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6196fa8",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Minimi Quadrati Lineari: $f(x) = \\frac12||Ax-b||_2^2$ con $A \\in \\R^{n \\times n}$\n",
    "\n",
    "Definiamo la $i$-esima funzione componente come: $f_i(x) = \\frac{1}{2} ( A_ix-b_i )^2$\n",
    "\n",
    "Per la linearità dell'operatore gradiente, il gradiente della somma è la somma dei gradienti:\n",
    "$\\nabla f(x) = \\nabla \\left( \\sum_{i=1}^m f_i(x) \\right) = \\sum_{i=1}^m \\nabla f_i(x)$\n",
    "\n",
    "Applichiamo la regola della catena:\n",
    "$\\nabla f_i(x) = \\frac{1}{2} \\cdot 2 \\cdot (A_i x - b_i) \\cdot \\nabla(A_i x - b_i)$\n",
    "\n",
    "Ora, calcoliamo $\\nabla(A_i x - b_i)$. La funzione $g(x) = A_i x - b_i$ è:\n",
    "$g(x) = (a_{i1}x_1 + a_{i2}x_2 + \\dots + a_{in}x_n) - b_i$\n",
    "\n",
    "Calcoliamo le derivate parziali rispetto a ogni $x_j$:\n",
    "$\\frac{\\partial g}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} (a_{i1}x_1 + \\dots + a_{ij}x_j + \\dots + a_{in}x_n - b_i) = a_{ij}$\n",
    "\n",
    "Assemblando queste derivate parziali in un vettore gradiente (colonna):\n",
    "$$ \\nabla(A_i x - b_i) = \n",
    "   \\begin{bmatrix} \n",
    "   \\frac{\\partial g}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial g}{\\partial x_n} \n",
    "   \\end{bmatrix} = \n",
    "   \\begin{bmatrix} \n",
    "   a_{i1} \\\\ \\vdots \\\\ a_{in} \n",
    "   \\end{bmatrix} = A_i^T $$\n",
    "(Questo è il trasposto della *riga* $A_i$).\n",
    "\n",
    "Sostituendo questo risultato, otteniamo il gradiente della $i$-esima componente:\n",
    "$\\nabla f_i(x) = (A_i x - b_i) A_i^T$\n",
    "Questo è un vettore $n \\times 1$ (uno scalare moltiplicato per un vettore colonna).\n",
    "\n",
    "**Ora sommiamo i gradienti di tutte le $m$ componenti:**\n",
    "\n",
    "$\\nabla f(x) = \\sum_{i=1}^m \\nabla f_i(x) = \\sum_{i=1}^m (A_i x - b_i) A_i^T$\n",
    "\n",
    "Definiamo il vettore \"errore\" $e = Ax - b$, dove $e_i = (A_i x - b_i)$ è la $i$-esima componente (scalare). La somma diventa:\n",
    "$\\nabla f(x) = \\sum_{i=1}^m e_i A_i^T$\n",
    "\n",
    "Analizziamo la matrice $A^T \\in \\R^{n \\times m}$. Le **colonne** di $A^T$ sono i vettori $A_i^T$:\n",
    "$$ A^T = \n",
    "   \\begin{bmatrix} \n",
    "   | & | & & | \\\\ \n",
    "   A_1^T & A_2^T & \\dots & A_m^T \\\\ \n",
    "   | & | & & | \n",
    "   \\end{bmatrix} $$\n",
    "\n",
    "La moltiplicazione matrice-vettore $A^T e$ è definita come la combinazione lineare delle colonne di $A^T$ con i pesi (scalari) $e_i$:\n",
    "$$ A^T e = \n",
    "   \\begin{bmatrix} \n",
    "   | & \\dots & | \\\\ \n",
    "   A_1^T & \\dots & A_m^T \\\\ \n",
    "   | & \\dots & | \n",
    "   \\end{bmatrix} \n",
    "   \\begin{bmatrix} \n",
    "   e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_m \n",
    "   \\end{bmatrix} \n",
    "   = e_1 A_1^T + e_2 A_2^T + \\dots + e_m A_m^T = \\sum_{i=1}^m e_i A_i^T $$\n",
    "\n",
    "Quindi $\\nabla f(x) = A^T (Ax - b)$\n",
    "\n",
    "I punti minimi sono $A^T(Ax-b)=0$\n",
    "\n",
    "Prendendo il gradiente ottenuto, notiamo che l'Hessiana è semplicemente $A^TA$, ovvero una costante.\n",
    "\n",
    "Questo significa che la curvatura del grafico è uguale ad ogni punto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da0bd1f",
   "metadata": {},
   "source": [
    "**Nota: questa è una loss function:**\n",
    "\n",
    "- $A$ sono i dati, ovvero il dataset in input\n",
    "\n",
    "- $x$ sono i parametri del modello, ovvero i $ \\theta$\n",
    "\n",
    "- $b$ sono i valori reali\n",
    "\n",
    "- $Ax-b$ sono le previsioni del modello - i valori reali: sono la distanza tra i valori predetti e quelli reali\n",
    "\n",
    "Lo eleviamo al quadrato per avere una distanza positiva e al quadrato per dare importanza ai valori distanti a quelli predetti.\n",
    "\n",
    "Quindi, questa funzione serve per quantificare quanto è \"sbagliato\" il nostro modello."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be580c84",
   "metadata": {},
   "source": [
    "## 7.1 Implementazione dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "19d1e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisco la funzione\n",
    "def f4(x, A, b):\n",
    "    e = A @ x -b\n",
    "    norm_sq = np.dot(e, e)\n",
    "    return 1/2 * norm_sq\n",
    "\n",
    "# Questa calcola il GRADIENTE COMPLETO (Batch)\n",
    "def df4(X, A, b):\n",
    "    return A.T @ (A @ X - b)\n",
    "\n",
    "# Hessiana\n",
    "def hess_f4(A):\n",
    "    return A.T @ A\n",
    "\n",
    "# Questa calcola il GRADIENTE STOCASTICO (Mini-Batch)\n",
    "def sdf4(x, indices, A, b):\n",
    "    A_batch= A[indices, :]\n",
    "    b_batch= b[indices]\n",
    "    \n",
    "    return A_batch.T @ (A_batch @ x - b_batch)\n",
    "\n",
    "n = 50 # siamo in R^50\n",
    "A = np.random.rand(n,n) # A è una matrice n x n con valori casuali\n",
    "b = A @ np.ones(n) # rendiamo b in modo che b= A(vettori 1)+rumore\n",
    "X_start_1 = np.zeros(n) # Punto di partenza\n",
    "X_start_2 = np.random.randn(n) # Punto di partenza\n",
    "\n",
    "title_f4 = r\"$f(x) = \\frac{1}{2}\\|Ax-b\\|_2^2 \\text{ con } A \\in \\mathbb{R}^{n \\times n}$\"\n",
    "maxit = 2000\n",
    "maxit_SGD = 20000          # L'SGD richiede molti più passi del GD\n",
    "tol_grad = 1e-3            # Tolleranza più alta per SGD\n",
    "tol_step = 1e-6\n",
    "batch_k = 5                # Dimensione del mini-batch\n",
    "learning_rate = 0.001      # Alpha (deve essere molto piccolo)\n",
    "\n",
    "f_min_sq = lambda x: f4(x, A, b) # Equivalente a scrivere def f_min_sq(x): return f4(x, A, b)\n",
    "df_min_sq = lambda x: df4(x, A, b) \n",
    "h_min_sq = lambda x: hess_f4(A)\n",
    "sdf_min_sq = lambda x, idx: sdf4(x, idx, A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97306e0",
   "metadata": {},
   "source": [
    "## 7.2 Soluzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "bc80fd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [0.97519061 1.01093011 0.98679271 0.99965344 0.99117447 0.9796362\n",
      " 1.02753856 0.99356038 0.9990253  1.01128273 0.95323749 0.992687\n",
      " 1.00370235 1.01182285 1.00595534 0.99676261 1.04531556 1.01145568\n",
      " 1.01441443 1.01868763 0.99424453 0.99961065 0.99819499 0.98806095\n",
      " 1.01616353 1.00208672 1.01518164 1.02263773 0.97722314 1.00353077\n",
      " 0.99175659 1.00717957 0.98288752 1.01896003 0.99333814 1.03581297\n",
      " 1.00988266 0.97743614 0.99124382 0.97057299 1.02172699 0.97846902\n",
      " 1.00803449 0.98048603 0.99762589 0.98999394 0.99781803 0.98703508\n",
      " 0.96462229 1.02449622]\n",
      "Numero iterazioni: 2000\n",
      "Condizione di uscita: maxit\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1.]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo del Gradiente Stocastico (SGD) ---\n",
      "Soluzione: [0.97859318 0.98787286 0.99209988 0.99116388 0.9850357  0.98025702\n",
      " 1.02974098 0.9984684  1.00330042 1.02765789 0.95180856 0.97449407\n",
      " 1.00290118 0.98323779 0.99751377 0.97620514 1.0636249  1.03958316\n",
      " 0.99905228 1.02780633 0.99014787 0.99655851 1.00226321 1.01140286\n",
      " 0.99212531 0.99339723 1.00578723 1.05098201 0.93281785 1.00602407\n",
      " 0.9739456  1.05053957 0.98512444 1.03989993 0.99790854 1.05550958\n",
      " 0.97719359 0.97540142 0.98215538 0.96334735 1.04451315 0.95475159\n",
      " 1.00347053 0.9903395  1.00458778 0.98833264 1.0285084  0.97101545\n",
      " 0.93433018 1.04198375]\n",
      "Numero iterazioni: 20000\n",
      "Condizione di uscita: maxit\n",
      "Epoche fatte: 2000\n"
     ]
    }
   ],
   "source": [
    "path_gd, path_newton = get_solutions(f_min_sq, df_min_sq, h_min_sq, X_start_1, X_start_2, maxit, tol_grad, tol_step)\n",
    "\n",
    "path_sgd = get_SGD_solutions(f_min_sq, sdf_min_sq, X_start_1, n, maxit_SGD, tol_grad, tol_step, batch_k, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e86802",
   "metadata": {},
   "source": [
    "## 7.3 Generazione Grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "5005e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_graph(f_min_sq, df_min_sq, path_gd, path_newton, path_sgd, title_f4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b9f2c0",
   "metadata": {},
   "source": [
    "Notiamo che newton è velocissimo. Questo perchè in funzioni quadrate, salta direttamente in quel punto.\n",
    "Il grafico del gradiente è rumoroso, perchè nella iterazione k fa un passo fuori rotta, il grafico sale, per poi essere ritornato nella iterazione k+1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a619d",
   "metadata": {},
   "source": [
    "---\n",
    "# 8 Minimi quadrati con regolarizzazione: $f(x) = \\frac{1}{2}||Ax-b||^2_2 + \\lambda ||x||^2_2$, con $\\lambda \\in [0,1]$\n",
    "\n",
    "Questa funzione viene chiamata anche come Regressione di Ridge.\n",
    "\n",
    "Ora dobbiamo trovare il termine noto $\\frac{1}{2}||Ax-b||^2_2$ e il termine di regolarizzazione $\\lambda ||x||^2_2$\n",
    "\n",
    "La medesima, vuole trovare una x con componenti (pesi) il più possibile vicini a zero. Penalizza soluzioni con valori molto grandi.\n",
    "\n",
    "Separiamo il problema come $\\nabla f(x) = \\nabla f_{LS}(x) + \\nabla f_{Reg}(x)$\n",
    "\n",
    "Troviamo il gradiente:\n",
    "\n",
    "- Come già fatto nella funzione precedente, $\\nabla f_{LS}(x) = A^T (Ax - b)$\n",
    "\n",
    "- $\\nabla f_{Reg} = 2 \\lambda x $\n",
    "\n",
    "Quindi la soluzione è $\\nabla f(x) = A^T (Ax - b) + 2\\lambda x$\n",
    "\n",
    "Per l'Hessiana, scomponiamo la funzione $H=f_A+f_\\lambda$. \n",
    "\n",
    "$f_A$ l'abbiamo già definita nel caso di funzione precedente $A^TA$\n",
    "\n",
    "$f_\\lambda$, invece, ha le derivate per $H_{ki}$ con $k != i$ sono uguali a $0$ perchè sono costanti.\n",
    "\n",
    "Mentre per $k=i$, ovvero la matrice diagonale, non vengono considerate costanti e la loro derivata è $2\\lambda$, \n",
    "\n",
    "Quindi $A^T A + 2\\lambda I$, dove I è la matrice identità."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d321c30",
   "metadata": {},
   "source": [
    "## 8.1 Definisco le funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "7f3cbb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisco la funzione\n",
    "def f5(x, A, b, lambda_val):\n",
    "    e = A @ x -b\n",
    "    errore_ls = 1/2 * np.dot(e, e)\n",
    "    errore_reg = lambda_val*np.dot(x,x)\n",
    "    return errore_ls + errore_reg\n",
    "\n",
    "# Questa calcola il GRADIENTE COMPLETO (Batch)\n",
    "def df5(x, A, b, lambda_val):\n",
    "    grad_ls = A.T @ (A @ x - b)\n",
    "    grad_reg = 2*lambda_val*x\n",
    "    return grad_ls +  grad_reg\n",
    "\n",
    "# Hessiana\n",
    "def hess_f5(A, lambda_val):\n",
    "    return A.T @ A + 2 * lambda_val * np.identity(n)\n",
    "\n",
    "# Questa calcola il GRADIENTE STOCASTICO (Mini-Batch)\n",
    "def sdf5(x, indices, A, b, lambda_val):\n",
    "    A_batch= A[indices, :]\n",
    "    b_batch= b[indices]\n",
    "\n",
    "    grad_ls_stoc = A_batch.T @ (A_batch @ x - b_batch)\n",
    "    grad_reg = 2 * lambda_val * x\n",
    "    \n",
    "    return grad_ls_stoc + grad_reg\n",
    "\n",
    "n = 50 # siamo in R^50\n",
    "A = np.random.rand(n,n) # A è una matrice n x n con valori casuali\n",
    "X_true = np.ones(n)\n",
    "b = A @ X_true + 0.1 * np.random.randn(n) # rendiamo b in modo che b= A(vettori 1)+rumore\n",
    "lambda_val = 0.1 \n",
    "\n",
    "titolo_f5 = r\"$f(x) = \\frac{1}{2}\\|Ax-b\\|_2^2 + \\lambda \\|x\\|_2^2 \\text{ con } \\lambda \\in [0,1]$\"\n",
    "X_start = np.zeros(n)      # Punto di partenza\n",
    "maxit = 2000\n",
    "maxit_SGD = 20000         # L'SGD richiede molti più passi del GD\n",
    "batch_k = 5                # Dimensione del mini-batch\n",
    "learning_rate = 0.001      # Alpha (deve essere molto piccolo)\n",
    "# NOTA: Una epoca è fatta ad ogni 10 iterazioni, perchè n/batch_k = 50 / 5 = 10\n",
    "\n",
    "f_min_sqr = lambda x: f5(x, A, b, lambda_val)\n",
    "df_min_sqr = lambda x: df5(x, A, b, lambda_val)\n",
    "h_min_sqr = lambda x: hess_f5(A, lambda_val)\n",
    "sdf_min_sqr = lambda x, idx: sdf5(x, idx, A, b, lambda_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f2b35",
   "metadata": {},
   "source": [
    "## 8.2 Soluzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "cd31a4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [1.06603086 0.97743522 1.07151049 0.91687968 1.00009479 1.00161952\n",
      " 0.96194733 1.07698235 1.06220052 0.96751674 0.98199613 0.98576459\n",
      " 0.9939642  0.97934077 0.99020506 1.03806357 0.95034622 1.02480236\n",
      " 1.04694916 1.08182595 1.00392421 0.91045892 1.07331167 1.03845134\n",
      " 1.15889071 0.97477527 1.08017719 1.01951742 1.06709794 0.8711588\n",
      " 1.00482695 0.97680848 0.97332964 0.99154753 0.8991189  1.08265919\n",
      " 1.16534341 0.89961414 1.02529299 0.77224158 1.03105206 0.98023906\n",
      " 0.91462646 0.96034272 1.04439518 0.9201893  0.94399081 1.02223214\n",
      " 0.92964854 0.99556451]\n",
      "Numero iterazioni: 2000\n",
      "Condizione di uscita: maxit\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [1.06321086 0.97630728 1.07449091 0.91534675 0.99929624 1.00108313\n",
      " 0.95993377 1.07649205 1.06307519 0.96776841 0.98040756 0.98538491\n",
      " 0.99294281 0.98212466 0.99428892 1.03731166 0.95059729 1.02434415\n",
      " 1.05021492 1.08517905 1.00486834 0.90934919 1.07763386 1.03972994\n",
      " 1.15919729 0.97320576 1.08041085 1.01844525 1.06689602 0.86743784\n",
      " 1.00804361 0.97784714 0.97394166 0.9893365  0.89662206 1.0837768\n",
      " 1.169413   0.8992     1.0258002  0.76826321 1.02981892 0.98092501\n",
      " 0.91242317 0.96106744 1.04125331 0.92073337 0.944956   1.02443811\n",
      " 0.92607168 0.99527929]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo del Gradiente Stocastico (SGD) ---\n",
      "Soluzione: [1.11838833 0.96539032 0.97350249 0.97810906 0.96704209 0.98420605\n",
      " 0.96112961 1.09160894 1.05266351 0.97822512 1.05369117 0.92191263\n",
      " 1.01887458 0.91688719 0.92242411 1.05625438 0.95990032 0.98172566\n",
      " 1.0130495  1.07592806 0.92746251 0.92858211 1.02229158 1.02687727\n",
      " 1.08361502 0.95649443 1.02559519 0.98186219 1.07453909 0.95916201\n",
      " 1.010062   0.96842295 0.97190446 1.03313024 0.95878146 1.00125095\n",
      " 1.09358228 0.87199472 1.04816479 0.89742003 1.00664351 0.96133996\n",
      " 0.96872356 1.01153885 1.02485243 0.96727812 0.90378505 1.00939184\n",
      " 0.9619319  1.03064689]\n",
      "Numero iterazioni: 20000\n",
      "Condizione di uscita: maxit\n",
      "Epoche fatte: 2000\n"
     ]
    }
   ],
   "source": [
    "path_gd, path_newton = get_solutions(f_min_sqr, df_min_sqr, h_min_sqr, X_start, maxit, tol_grad, tol_step)\n",
    "\n",
    "path_sgd = get_SGD_solutions(df_min_sqr, sdf_min_sqr, X_start, n, maxit_SGD, tol_grad, tol_step, batch_k, learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db42ed6",
   "metadata": {},
   "source": [
    "## 8.3 Generazione Grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "5039254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_graph(f_min_sqr, path_gd, path_newton, path_sgd, titolo_f5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379d5c3",
   "metadata": {},
   "source": [
    "---\n",
    "# 9 $f(x) = \\sum \\limits_{i=1}^n (x_i-i)^2-\\sum \\limits_{i=1}^n \\ln(x_i)$, con $x_i>0$\n",
    "\n",
    "$\\nabla f(x) = 2 (x-i) - \\frac{1}{x}$\n",
    "\n",
    "Notiamo che per $ i!= k$,$H_{ik}$ non ha elementi perchè la derivata rispetto a x_k ha solo costanti.\n",
    "\n",
    "I punti minimi sono: $2 (x-i) - \\frac{1}{x} = 0$ \n",
    "\n",
    "Ovvero $2x^2-2ix-1 = 0 \\to \\frac{2i\\pm\\sqrt{4i^2+8}}{4}$\n",
    "\n",
    "Invece, per $i = k$, $H_{ik} = 2+(\\frac{1}{x^2})$\n",
    "\n",
    "Quindi l'Hessiana ha solo elementi nella diagonale\n",
    "\n",
    "**Nota**: qua il metodo stocastico è inutile, dato che il gradiente non ha matrici e il metodo del gradiente ha già un costo $O(n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "331a89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f6 (x, idx):\n",
    "    if np.any(x <= 0):\n",
    "        return np.inf # ritorna infinito se x non è nel dominio (ln(x) con x<=0)\n",
    "\n",
    "    sum1 = np.sum((x-idx)**2)\n",
    "    sum2 = np.sum(np.log(x))\n",
    "    return sum1 - sum2\n",
    "    \n",
    "def df6 (x, idx):\n",
    "    return 2 * (x - idx) - 1/x\n",
    "\n",
    "def hess_f6 (x):\n",
    "    HESS = 2+(1/(x**2))\n",
    "    return np.diag(HESS)\n",
    "\n",
    "def sdf6(x, mini_batch, n, idx):\n",
    "\n",
    "    x_batch = x[mini_batch]\n",
    "    idx_batch = idx[mini_batch]\n",
    "    grad_stoc = np.zeros(n)\n",
    "    \n",
    "    # Calcola il gradiente solo per quei componenti\n",
    "    grad_batch_components = 2 * (x_batch - idx_batch) - (1 / x_batch)\n",
    "    \n",
    "    # Inserisci i componenti calcolati nel vettore zero\n",
    "    grad_stoc[mini_batch] = grad_batch_components\n",
    "    \n",
    "    return grad_stoc\n",
    "\n",
    "title_f6 = r\"$f(x) = \\sum_{i=1}^n (x_i-i)^2 - \\sum_{i=1}^n \\ln(x_i)$\"\n",
    "idx = np.arange(1, n+1)\n",
    "X_start = np.ones(n)\n",
    "b = A @ X_start + 0.1 * np.random.randn(n)\n",
    "n = 50\n",
    "\n",
    "f_lambda = lambda x: f6(x, idx)\n",
    "df_lambda = lambda x: df6(x, idx)\n",
    "h_lambda = lambda x: hess_f6(x)\n",
    "sdf_lambda = lambda x, k: sdf6(x, k, n, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "1e359e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [ 1.36585366  2.22474227  3.1583123   4.12132034  5.09807621  6.082207\n",
      "  7.07071421  8.0620192   9.05521679 10.04975247 11.04526825 12.04152299\n",
      " 13.03834842 14.03562364 15.03325959 16.0311892  17.02936105 18.02773504\n",
      " 19.02627944 20.02496883 21.02378259 22.02270384 23.02171862 24.02081528\n",
      " 25.01998403 26.01921657 27.01850583 28.01784577 29.01723114 30.01665742\n",
      " 31.01612065 32.01561738 33.01514456 34.01469953 35.01427989 36.01388353\n",
      " 37.01350858 38.01315334 39.0128163  40.0124961  41.0121915  42.01190139\n",
      " 43.01162476 44.0113607  45.01110837 46.010867   47.01063589 48.01041441\n",
      " 49.01020196 50.009998  ]\n",
      "Numero iterazioni: 6\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [ 1.3660254   2.22474461  3.15831201  4.12132005  5.09807602  6.08220688\n",
      "  7.07071414  8.06201915  9.05521676 10.04975245 11.04526824 12.04152298\n",
      " 13.03834841 14.03562363 15.03325958 16.0311892  17.02936105 18.02773504\n",
      " 19.02627944 20.02496883 21.02378259 22.02270384 23.02171862 24.02081528\n",
      " 25.01998403 26.01921657 27.01850583 28.01784577 29.01723114 30.01665742\n",
      " 31.01612065 32.01561738 33.01514456 34.01469953 35.01427989 36.01388353\n",
      " 37.01350858 38.01315334 39.0128163  40.0124961  41.0121915  42.01190139\n",
      " 43.01162476 44.0113607  45.01110837 46.010867   47.01063589 48.01041441\n",
      " 49.01020196 50.009998  ]\n",
      "Numero iterazioni: 3\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo del Gradiente Stocastico (SGD) ---\n",
      "Soluzione: [ 1.36388497  2.21084459  3.12775102  4.07265059  5.03089445  5.99641663\n",
      "  6.96631387  7.9390364   8.91368575  9.88970649 10.86673737 11.84453349\n",
      " 12.82292304 13.80178201 14.78101864 15.76056362 16.74036359 17.72037685\n",
      " 18.70057031 19.68091737 20.66139645 21.64198982 22.62268278 23.60346306\n",
      " 24.58432033 25.5652458  26.54623201 27.52727253 28.5083618  29.48949501\n",
      " 30.47066795 31.45187694 32.43311874 33.41439048 34.39568963 35.37701392\n",
      " 36.35836134 37.33973008 38.32111851 39.30252518 40.28394876 41.26538806\n",
      " 42.24684198 43.22830954 44.20978985 45.19128207 46.17278545 47.1542993\n",
      " 48.13582299 49.11735593]\n",
      "Numero iterazioni: 20000\n",
      "Condizione di uscita: maxit\n",
      "Epoche fatte: 2000\n"
     ]
    }
   ],
   "source": [
    "path_gd, path_newton = get_solutions(f_lambda, df_lambda, h_lambda, X_start, maxit, tol_grad, tol_step)\n",
    "\n",
    "path_sgd = get_SGD_solutions(df_lambda, sdf_lambda, X_start, n, maxit_SGD, tol_grad, tol_step, batch_k, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "1ef8e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_graph(f_lambda, path_gd, path_newton, path_sgd, title_f6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89828313",
   "metadata": {},
   "source": [
    "---\n",
    "# 10 $f(x) = \\sum \\limits_{i=1}^n (x_i-b_i)^2+c\\sum \\limits_{i=1}^n \\ln(x_i)$\n",
    "\n",
    "$\\nabla f(x) = 2(x_i-b_i) + c\\frac{1}{x_i}$\n",
    "\n",
    "I punti minimi sono $2(x_i-b_i) + c\\frac{1}{x_i} = 0 \\to 2x_i^2-2x_ib_i+c = 0 \\to \\frac{2b_i\\pm\\sqrt{4b_i^2-8c}}{4}$\n",
    "\n",
    "**Nota**: dato che abbiamo $\\ln(x_i)$, quando la funzione ha $x\\simeq 0$, si trova $-\\infty$. Quindi non esiste un minimo locale.\n",
    "\n",
    "Scomponiamo la funzione in $f(x) = f_{xb} + f_{ln}$\n",
    "\n",
    "Per l'Hessiana, se deriviamo rispetto a $x_k$:\n",
    "\n",
    "$f_{xb} = 2$, ovvero una $2\\cdot I$, la matrice identità\n",
    "\n",
    "$f_{ln} = -c\\frac{1}{x^2}$, sempre nella matrice diagonale.\n",
    "\n",
    "Quindi $H_{ik} = 2-c\\frac{1}{x^2}$\n",
    "\n",
    "**Note**: questa Hessiana non è sempre definita positiva, quindi è più difficile minimizzare.\n",
    "\n",
    "- Il metodo di Newton convergerà verso il risultato, il metodo del gradiente non funziona e quello del SGD converge molto lentamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "81f0fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f7(x,c,b):\n",
    "\n",
    "    sum1 = np.sum((x-b)**2)\n",
    "    sum2 = c*np.sum(np.log(x))\n",
    "\n",
    "    return sum1+sum2\n",
    "\n",
    "def df7(x,c,b):\n",
    "    return 2*(x-b)+c*(1/x)\n",
    "\n",
    "def hess_f7(x,c):\n",
    "    HESS =2-c*(1/(x**2))\n",
    "    return np.diag(HESS)\n",
    "\n",
    "def sdf7(x, c, b, n, mini_batch):\n",
    "    x_batch = x[mini_batch]\n",
    "    b_batch = b[mini_batch]\n",
    "    grad_stoc = np.zeros(n)\n",
    "    \n",
    "    # Calcola il gradiente solo per quei componenti\n",
    "    grad_batch_components = 2 * (x_batch - b_batch) + c*(1 / x_batch)\n",
    "    \n",
    "    # Inserisci i componenti calcolati nel vettore zero\n",
    "    grad_stoc[mini_batch] = grad_batch_components\n",
    "    \n",
    "    return grad_stoc\n",
    "\n",
    "title_f7 = r\"$f(x) = \\sum_{i=1}^n (x_i-b_i)^2+c\\sum_{i=1}^n \\ln(x_i)$\"\n",
    "b = np.arange(1, n+1)\n",
    "X_start = np.ones(n) # Partiamo da tutte x=1\n",
    "c= -2.0 # DEVE essere negativo\n",
    "\n",
    "f_c = lambda x: f7(x, c, b)\n",
    "df_c = lambda x: df7(x, c, b)\n",
    "h_c = lambda x: hess_f7(x, c)\n",
    "sdf_c = lambda x, k: sdf7(x, c, b, n, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "2bb0ac08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [ 1.61818182  2.41421393  3.30277564  4.23606798  5.1925824   6.16227766\n",
      "  7.14005494  8.12310563  9.10977223 10.09901951 11.09016994 12.08276253\n",
      " 13.07647322 14.07106781 15.06637298 16.06225775 17.05862138 18.05538514\n",
      " 19.05248659 20.04987562 21.04751155 22.04536102 23.04339638 24.04159458\n",
      " 25.0399362  26.03840481 27.03698637 28.03566885 29.03444185 30.03329638\n",
      " 31.03222457 32.03121954 33.03027525 34.02938637 35.02854814 36.02775638\n",
      " 37.02700731 38.02629759 39.02562419 40.02498439 41.02437575 42.02379604\n",
      " 43.02324325 44.02271555 45.02221126 46.02172887 47.02126697 48.0208243\n",
      " 49.02039967 50.01999201]\n",
      "Numero iterazioni: 9\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [ 1.61803279  2.41420118  3.30275694  4.2360515   5.19257028  6.16226925\n",
      "  7.14004919  8.12310166  9.10976946 10.09901755 11.09016853 12.08276149\n",
      " 13.07647245 14.07106723 15.06637253 16.0622574  17.05862111 18.05538492\n",
      " 19.05248641 20.04987548 21.04751144 22.04536092 23.0433963  24.04159451\n",
      " 25.03993615 26.03840477 27.03698633 28.03566881 29.03444183 30.03329635\n",
      " 31.03222455 32.03121952 33.03027524 34.02938635 35.02854813 36.02775637\n",
      " 37.0270073  38.02629758 39.02562418 40.02498439 41.02437575 42.02379604\n",
      " 43.02324325 44.02271554 45.02221126 46.02172886 47.02126697 48.0208243\n",
      " 49.02039967 50.019992  ]\n",
      "Numero iterazioni: 3\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo del Gradiente Stocastico (SGD) ---\n",
      "Soluzione: [ 1.61587975  2.40269507  3.27626714  4.19210342  5.13023449  6.08122228\n",
      "  7.04020036  8.00445019  8.972348    9.94286927 10.91533768 11.88928971\n",
      " 12.86439745 13.84042262 14.81718802 15.79455921 16.77243235 17.75072602\n",
      " 18.72937547 19.70832857 20.6875429  21.66698357 22.64662165 23.62643291\n",
      " 24.60639696 25.58649646 26.56671661 27.5470447  28.52746972 29.50798213\n",
      " 30.48857359 31.4692368  32.44996532 33.43075347 34.4115962  35.39248902\n",
      " 36.37342792 37.3544093  38.33542994 39.31648692 40.29757762 41.27869965\n",
      " 42.25985086 43.24102927 44.2222331  45.20346069 46.18471056 47.16598131\n",
      " 48.14727169 49.12858054]\n",
      "Numero iterazioni: 20000\n",
      "Condizione di uscita: maxit\n",
      "Epoche fatte: 2000\n"
     ]
    }
   ],
   "source": [
    "path_gd, path_newton = get_solutions(f_c, df_c, h_c, X_start, maxit, tol_grad, tol_step)\n",
    "\n",
    "path_sgd = get_SGD_solutions(df_c, sdf_c, X_start, n, maxit_SGD, tol_grad, tol_step, batch_k, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "e480eb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid command name \"127261938136000process_stream_events\"\n",
      "    while executing\n",
      "\"127261938136000process_stream_events\"\n",
      "    (\"after\" script)\n"
     ]
    }
   ],
   "source": [
    "error_graph(f_c, path_gd, path_newton, path_sgd, title_f7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CN25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
