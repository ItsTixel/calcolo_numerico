{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d465100e",
   "metadata": {},
   "source": [
    "# Analisi e Implementazione di Metodi di Ottimizzazione\n",
    "\n",
    "Creiamo i tre metodi di ottimizzazione per trovare i minimi di diverse funzioni:\n",
    "1.  **Metodo del Gradiente (Gradient Descent - GD)**\n",
    "2.  **Metodo di Newton**\n",
    "3.  **Metodo del Gradiente Stocastico (SGD)**\n",
    "\n",
    "Viene utilizzata una strategia di **Backtracking Line Search** per la determinazione del passo di apprendimento (alpha)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ebaf8",
   "metadata": {},
   "source": [
    "## 1 Import delle librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a3ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "from mpl_toolkits.mplot3d import Axes3D # Per grafici 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d34ad9f",
   "metadata": {},
   "source": [
    "## 2 Implementazione degli Algoritmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caaa8bd",
   "metadata": {},
   "source": [
    "## 2.1 Funzione Backtracking (Line Search)\n",
    "\n",
    "Questa funzione implementa la ricerca del passo $\\alpha$ tramite backtracking, basandosi sulla condizione di Armijo per garantire che il risultato converge alla soluzione.\n",
    "\n",
    "**Parametri**:\n",
    "- $f$: La funzione obiettivo\n",
    "- $df$: Il gradiente della funzione obiettivo\n",
    "- $X_k$: Il punto corrente (vettore)\n",
    "- $p_k$: La direzione di discesa (vettore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83aebdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtracking per alpha\n",
    "def backtracking(f, df, X_k, p_k):\n",
    "    alpha = 1\n",
    "    rho = 1/2 # Fattore di riduzione per alpha\n",
    "    c1 = 0.25\n",
    "\n",
    "    # Calcola il gradiente del punto passato\n",
    "    grad_k = df(X_k)\n",
    "\n",
    "    # Calcola il prodotto scalare\n",
    "    grad_dot_p = np.dot(grad_k, p_k)\n",
    "    \n",
    "    # Condizione Di Armijo: Il loop \"while\" continua FINCHÉ la condizione è VIOLATA (segno >)\n",
    "    while f(X_k + alpha * p_k) > f(X_k) + c1 * alpha * grad_dot_p:\n",
    "        alpha = rho * alpha\n",
    "\n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad29c54",
   "metadata": {},
   "source": [
    "## 2.2 Metodo del gradiente\n",
    "\n",
    "La direzione di discesa $p_k = -\\nabla f(x)$, ovvero l'antigradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38db714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo del gradiente\n",
    "def GD (f, df, X_old, maxit, tol_f, tol_x):\n",
    "    \n",
    "    count=0\n",
    "    dim = len(X_old) # Calcolo la dimensione di X_old\n",
    "\n",
    "    # Dichiare una matrice di 0 di dimensione maxit + 1 per dim\n",
    "    fun_history = np.zeros((maxit + 1, dim))\n",
    "    fun_history[0,] = X_old\n",
    "\n",
    "    exit_flag = 'maxit' # Flag di uscita di default\n",
    "\n",
    "    current_grad_norm=np.linalg.norm(df(X_old))\n",
    "\n",
    "    # Controllo se ho superato le iterazioni o se ho raggiunto la tolleranza desiderata\n",
    "    while count < maxit and current_grad_norm > tol_f:\n",
    "        # Direzione di discesa\n",
    "        p_k=-df(X_old)\n",
    "        \n",
    "        # Backtracking di alpha\n",
    "        alpha=backtracking(f, df, X_old, p_k)\n",
    "\n",
    "        # Calcolo di X_k+1\n",
    "        X_new = X_old + alpha * p_k\n",
    "\n",
    "        # Controllo se sto facendo progressi (tolleranza x)\n",
    "        if np.linalg.norm(X_new-X_old) < tol_x:\n",
    "            exit_flag = 'tol_x'\n",
    "            break\n",
    "\n",
    "        # Ricalcolo la norma del gradiente\n",
    "        current_grad_norm=np.linalg.norm(df(X_new))\n",
    "\n",
    "        # Aggiorno per l'iterazione successiva\n",
    "        X_old = X_new\n",
    "        count+=1\n",
    "\n",
    "        # Aggiungo il valore della funzione in x_k nella matrice dei risultati \n",
    "        fun_history[count] = X_new\n",
    "\n",
    "    # Controllo se l'uscita è dovuta a tol_f (norma del gradiente)\n",
    "    if exit_flag == 'maxit' and current_grad_norm <= tol_f:\n",
    "        exit_flag = 'tol_f'\n",
    "\n",
    "    # Rimuovo le righe non utilizzate dalla cronologia\n",
    "    if count<maxit:\n",
    "        fun_history = fun_history[:count+1]\n",
    "\n",
    "    return X_old, count, fun_history, exit_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af6510",
   "metadata": {},
   "source": [
    "## 2.2 Metodo Newton\n",
    "\n",
    "La direzione di discesa è $p_k = - \\frac{\\nabla f_k}{H_k} $\n",
    "\n",
    "Nota: questo metodo è molto lento perchè ad ogni iterazione si deve moltiplicare il gradiente e l'hessiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75834970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo di Newton\n",
    "def Newton (f, df, hess_f, X_old, maxit, tol_f, tol_x):\n",
    "    \n",
    "    count=0\n",
    "    dim = len(X_old) \n",
    "    \n",
    "    # Cronologia\n",
    "    fun_history = np.zeros((maxit + 1, dim))\n",
    "    fun_history[0,]=X_old\n",
    "\n",
    "    exit_flag = 'maxit'\n",
    "\n",
    "    current_grad_norm=np.linalg.norm(df(X_old))\n",
    "\n",
    "    while count < maxit and current_grad_norm > tol_f:\n",
    "\n",
    "        # Calcolo l'Hessiana\n",
    "        H_k = hess_f(X_old)\n",
    "\n",
    "        # Calcolo il gradiente\n",
    "        grad_k = df(X_old)\n",
    "\n",
    "        # Direzione di discesa\n",
    "        p_k=np.linalg.solve(H_k, -grad_k)\n",
    "\n",
    "        # Backtracking di alpha\n",
    "        alpha=backtracking(f, df, X_old, p_k)\n",
    "\n",
    "        # Calcolo di X_k+1\n",
    "        X_new = X_old + alpha * p_k\n",
    "\n",
    "        # Controllo se sto facendo progressi (tolleranza x)\n",
    "        if np.linalg.norm(X_new-X_old) < tol_x:\n",
    "            exit_flag = 'tol_x'\n",
    "            break\n",
    "\n",
    "        # Ricalcolo la norma del gradiente\n",
    "        current_grad_norm=np.linalg.norm(df(X_new))\n",
    "\n",
    "        # Aggiorno per l'iterazione successiva\n",
    "        X_old = X_new\n",
    "        count+=1\n",
    "        fun_history[count] = X_new\n",
    "\n",
    "    \n",
    "    if exit_flag == 'maxit' and current_grad_norm <= tol_f:\n",
    "        exit_flag = 'tol_f'\n",
    "\n",
    "    if count<maxit:\n",
    "        fun_history = fun_history[:count+1]\n",
    "\n",
    "    return X_old, count, fun_history, exit_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e85ae8",
   "metadata": {},
   "source": [
    "## 2.4 Metodo gradiente stocastico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "764e6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD (df_completo, df_stocastico, X_old, n_samples, max_steps, tol_f, tol_x, k, alpha):\n",
    "\n",
    "    dim = len(X_old)\n",
    "\n",
    "    S_k = np.arange(n_samples)\n",
    "\n",
    "    step_count=0\n",
    "    epoch_count=0\n",
    "\n",
    "    fun_history = np.zeros((max_steps + 1, dim))\n",
    "    fun_history[0,]=X_old\n",
    "\n",
    "    exit_flag = 'maxit'\n",
    "\n",
    "    current_grad_norm=np.linalg.norm(df_completo(X_old))\n",
    "\n",
    "    while step_count < max_steps and current_grad_norm > tol_f :\n",
    "\n",
    "        np.random.shuffle(S_k)\n",
    "\n",
    "        epoch_count += 1\n",
    "        for i in range (0, n_samples, k):\n",
    "\n",
    "            indices = S_k[i:i+k]\n",
    "            grad_stoc = df_stocastico(X_old, indices)\n",
    "            p_k = -grad_stoc\n",
    "\n",
    "            X_new = X_old + alpha * p_k\n",
    "\n",
    "            if np.linalg.norm(X_new - X_old) < tol_x:\n",
    "                exit_flag = 'tol_x'\n",
    "                break\n",
    "\n",
    "            X_old = X_new\n",
    "            step_count += 1\n",
    "            fun_history[step_count] = X_new\n",
    "\n",
    "            if step_count >= max_steps:\n",
    "                break\n",
    "\n",
    "        current_grad_norm=np.linalg.norm(df_completo(X_new))\n",
    "\n",
    "        if exit_flag == 'tol_x':\n",
    "            break\n",
    "\n",
    "    if exit_flag == 'maxit' and current_grad_norm <= tol_f:\n",
    "        exit_flag = 'tol_f'\n",
    "\n",
    "    if step_count<max_steps: # troncamento\n",
    "        fun_history = fun_history[:step_count+1]\n",
    "\n",
    "    return X_old, step_count, fun_history, exit_flag, epoch_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a46d2a",
   "metadata": {},
   "source": [
    "## 2.5 Funzione per la generazione dei grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc521666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_generator (x0, y0, f, path_history_gd, path_history_newton, tmin, titolo):\n",
    "    X_mesh, Y_mesh = np.meshgrid(x0, y0)\n",
    "    Z = f([X_mesh, Y_mesh])\n",
    "\n",
    "    # Memorizzo il percorso delle x e y\n",
    "    path_x_gd = path_history_gd[:, 0]\n",
    "    path_y_gd = path_history_gd[:, 1]\n",
    "    path_x_new = path_history_newton[:, 0]\n",
    "    path_y_new = path_history_newton[:, 1]\n",
    "\n",
    "    # Definisco la figura\n",
    "    fig = plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # <-- GENERAZIONE GRAFICO 3D -->\n",
    "\n",
    "    # Aggiungo subplot 3D\n",
    "    axs1 = fig.add_subplot(1,2,1,projection='3d')\n",
    "\n",
    "    # Disegno la superficie\n",
    "    axs1.plot_surface(X_mesh, Y_mesh, Z, cmap='viridis', rstride=1, cstride=1, linewidth=0, antialiased=True, alpha=0.7)\n",
    "\n",
    "    # Aggiunge il percorso anche nel grafico 3D gradiente\n",
    "    z_path_gd = f([path_x_gd, path_y_gd])\n",
    "    axs1.plot(path_x_gd, path_y_gd, z_path_gd, 'r-o', label='Percorso del Gradiente')\n",
    "\n",
    "    # Aggiunge il percorso anche nel grafico 3D newton\n",
    "    z_path_newton = f([path_x_new, path_y_new])\n",
    "    axs1.plot(path_x_new, path_y_new, z_path_newton, 'b-o', label='Percorso di Newton')\n",
    "\n",
    "    # Evidenzia il punto iniziale e finale gradiente\n",
    "    axs1.plot(path_x_gd[0], path_y_gd[0], 'go', label=f'Start: ({path_x_gd[0]}, {path_y_gd[0]})') # punto iniziale\n",
    "    axs1.plot(path_x_gd[-1], path_y_gd[-1], 'rx', label=f'End: ({path_x_gd[-1]:.2f}, {path_y_gd[-1]:.2f})', markersize=16) # punto finale\n",
    "\n",
    "    # Evidenzia il punto iniziale e finale newton\n",
    "    axs1.plot(path_x_new[0], path_y_new[0], 'go', label=f'Start: ({path_x_new[0]}, {path_y_new[0]})') # punto iniziale\n",
    "    axs1.plot(path_x_new[-1], path_y_new[-1], 'rx', label=f'End: ({path_x_new[-1]:.2f}, {path_y_new[-1]:.2f})', markersize=16) # punto finale\n",
    "\n",
    "    axs1.plot(tmin[0], tmin[1], 'b*', markersize=15, label=f\"Minimo Teorico ({tmin[0]}, {tmin[1]})\") # Minimo teorico\n",
    "\n",
    "    # Aggiungo label e titolo\n",
    "    axs1.set_title(f\"{titolo}\")\n",
    "    axs1.set_xlabel(\"x\")\n",
    "    axs1.set_ylabel(\"y\")\n",
    "    axs1.set_zlabel(\"z\")\n",
    "    axs1.legend()\n",
    "    axs1.view_init(elev=30, azim=135) # Imposta un angolo di visuale\n",
    "\n",
    "    # <--- GENERAZIONE GRAFICO 2D --->\n",
    "\n",
    "    # Definisco il grafico 2D\n",
    "    axs0 = fig.add_subplot(1,2,2)\n",
    "\n",
    "    # Disegna le curve di livello\n",
    "    c1 = axs0.contour(X_mesh, Y_mesh, Z, levels=50, cmap='viridis', alpha=0.7)\n",
    "    fig.colorbar(c1, label='Valore di $f(x,y)$')\n",
    "\n",
    "    # Disegna il percorso\n",
    "    axs0.plot(path_x_gd, path_y_gd, 'r-o', label='Percorso del Gradiente')\n",
    "    axs0.plot(path_x_new, path_y_new, 'b-o', label='Percorso di Newton')\n",
    "\n",
    "    # Evidenzia il punto iniziale e finale gradiente\n",
    "    axs0.plot(path_x_gd[0], path_y_gd[0], 'go', label=f'Start: ({path_x_gd[0]}, {path_y_gd[0]})') # punto iniziale\n",
    "    axs0.plot(path_x_gd[-1], path_y_gd[-1], 'rx', label=f'End: ({path_x_gd[-1]:.2f}, {path_y_gd[-1]:.2f})', markersize=16) # punto finale\n",
    "    \n",
    "    # Evidenzia il punto iniziale e finale newton\n",
    "    axs0.plot(path_x_new[0], path_y_new[0], 'go', label=f'Start: ({path_x_new[0]}, {path_y_new[0]})') # punto iniziale\n",
    "    axs0.plot(path_x_new[-1], path_y_new[-1], 'bx', label=f'End: ({path_x_new[-1]:.2f}, {path_y_new[-1]:.2f})', markersize=16) # punto finale\n",
    "\n",
    "    axs0.plot(tmin[0], tmin[1], 'y*', markersize=15, label=f\"Minimo Teorico ({tmin[0]}, {tmin[1]})\") # Minimo teorico\n",
    "    # Etichette e titoli\n",
    "    axs0.set_title(f\"{titolo}\")\n",
    "    axs0.set_xlabel(\"x\")\n",
    "    axs0.set_ylabel(\"y\")\n",
    "    axs0.legend()\n",
    "    axs0.set_aspect('equal', adjustable='box') # \"Grafo è \"quadrato\"\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07468013",
   "metadata": {},
   "source": [
    "### 2.6 Funzione per gli output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "770d5f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solutions(f, df, h_f, x1, x2, maxit, tol_grad, tol_step):\n",
    "    print(\"--- Metodo del Gradiente (GD) ---\")\n",
    "    sol_gd, iter_gd, path_gd, exit_gd = GD(f, df, x1, maxit, tol_grad, tol_step)\n",
    "    print(f\"Soluzione: {sol_gd}\")\n",
    "    print(f\"Numero iterazioni: {iter_gd}\")\n",
    "    print(f\"Condizione di uscita: {exit_gd}\")\n",
    "\n",
    "    print(\"--- Metodo di Newton ---\")\n",
    "    sol_newton, iter_newton, path_newton, exit_newton = Newton(f, df, h_f, x2, maxit, tol_grad, tol_step)\n",
    "    print(f\"Soluzione: {sol_newton}\")\n",
    "    print(f\"Numero iterazioni: {iter_newton}\")\n",
    "    print(f\"Condizione di uscita: {exit_newton}\")\n",
    "\n",
    "    return path_gd, path_newton\n",
    "\n",
    "def get_SGD_solutions(f, sdf, x, n, maxit, tol_grad, tol_step, batch_k, learning_rate):\n",
    "    print(\"--- Metodo del Gradiente Stocastico (SGD) ---\")\n",
    "    sol_sgd, iter_sgd, path_sgd, exit_sgd, epoch = SGD(f, sdf, x, n, maxit, tol_grad, tol_step, batch_k, learning_rate)\n",
    "    print(f\"Soluzione: {sol_sgd}\")\n",
    "    print(f\"Numero iterazioni: {iter_sgd}\")\n",
    "    print(f\"Condizione di uscita: {exit_sgd}\")\n",
    "    print(f\"Epoche fatte: {epoch}\")\n",
    "\n",
    "    return path_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e83c4066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_graph(f, df, gd, newton, titolo, sgd=None): \n",
    "    \n",
    "    errori_gd = [f(x_k) for x_k in gd]\n",
    "    errori_newton = [f(x_k) for x_k in newton]\n",
    "\n",
    "    norma_grad_gd = [np.linalg.norm(df(x_k)) for x_k in gd]\n",
    "    norma_grad_newton = [np.linalg.norm(df(x_k)) for x_k in newton]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(16, 10)) \n",
    "\n",
    "    ax = axs[0, 0]\n",
    "    ax.plot(errori_gd, label=\"Gradient Descent (GD)\", marker='.', linestyle='-')\n",
    "    ax.plot(errori_newton, label=\"Newton Smorzato\", marker='.', linestyle='--')\n",
    "    ax.set_title(\"Valore Funzione (GD vs Newton)\")\n",
    "    ax.set_ylabel(\"Valore Funzione $f(x_k)$ (log scale)\")\n",
    "    ax.set_xlabel(\"Iterazione (k)\")\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(which=\"both\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax = axs[1, 0]\n",
    "    ax.plot(norma_grad_gd, label=\"Gradient Descent (GD)\", marker='.', linestyle='-')\n",
    "    ax.plot(norma_grad_newton, label=\"Newton Smorzato\", marker='.', linestyle='--')\n",
    "    ax.set_title(\"Norma Gradiente (GD vs Newton)\")\n",
    "    ax.set_ylabel(r\"Norma Gradiente $\\|\\nabla f(x_k)\\|$ (log scale)\")\n",
    "    ax.set_xlabel(\"Iterazione (k)\")\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "\n",
    "    if sgd is not None:\n",
    "        errori_sgd = [f(x_k) for x_k in sgd]\n",
    "        norma_grad_sgd = [np.linalg.norm(df(x_k)) for x_k in sgd]\n",
    "\n",
    "        ax = axs[0, 1]\n",
    "        ax.plot(errori_sgd, label=\"Stochastic GD\", color='green', alpha=0.8)\n",
    "        ax.set_title(\"Valore Funzione (SGD)\")\n",
    "        ax.set_ylabel(\"Valore Funzione $f(x_k)$ (log scale)\")\n",
    "        ax.set_xlabel(\"Iterazione (k)\")\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend()\n",
    "        ax.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "        ax = axs[1, 1]\n",
    "        ax.plot(norma_grad_sgd, label=\"Stochastic GD\", color='green', alpha=0.8)\n",
    "        ax.set_title(\"Norma Gradiente (SGD)\")\n",
    "        ax.set_ylabel(r\"Norma Gradiente $\\|\\nabla f(x_k)\\|$ (log scale)\")\n",
    "        ax.set_xlabel(\"Iterazione (k)\")\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend()\n",
    "        ax.grid(which=\"both\", linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    else:\n",
    "        axs[0, 1].axis('off')\n",
    "        axs[1, 1].axis('off')\n",
    "\n",
    "    fig.suptitle(f\"Analisi Convergenza per {titolo}\", fontsize=18)\n",
    "    \n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff313e",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3 Funzione Quadratica: $f(x, y) = (x-5)^2+(y-2)^2$\n",
    "\n",
    "Cerchiamo i punti critici della funzione.\n",
    "\n",
    "Calcoliamo la derivata rispetto a $x$ e a $y$.\n",
    "\n",
    "- $\\frac{∂x}{∂f} = ​2(x-5)$\n",
    "- $\\frac{∂y}{∂f} =​ 2(y-2)$\n",
    "\n",
    "Ponendo $x=5$ e $y=2$, troviamo un punto critico.\n",
    "Possiamo notare gli elementi della funzione sono elevati al quadrato, quindi sappiamo che il punto critico è un minimo globale.\n",
    "\n",
    "**Nota:** Nei metodi utilizzati, converge in una sola iterazione."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde6573f",
   "metadata": {},
   "source": [
    "### 3.1 Implementazione Dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d54457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "    return (x-5)**2+(y-2)**2\n",
    "\n",
    "def df1(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    df_dx = 2*(x-5) \n",
    "    df_dy = 2*(y-2)\n",
    "    \n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def hess_f1(X):\n",
    "    HESS=np.zeros((2,2))\n",
    "\n",
    "    HESS[0][0] = 2\n",
    "    HESS[0][1] = 0\n",
    "    HESS[1][0] = 0\n",
    "    HESS[1][1] = 2\n",
    "    return HESS    \n",
    "\n",
    "x_range1=np.linspace(-2,12,200)\n",
    "y_range1=np.linspace(-2,8,200)\n",
    "\n",
    "maxit = 300\n",
    "tolleranza_f = 1.e-6\n",
    "tolleranza_x = 1.e-6\n",
    "punto1_gd=np.array([0.0,0.0])\n",
    "punto1_new=np.array([1.0,1.0])\n",
    "tmin1 = [5.0, 2.0]\n",
    "titolo1 = r\"$f(x, y) = (x-5)^2+(y-2)^2$\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef17cefe",
   "metadata": {},
   "source": [
    "### 3.2 Soluzioni dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea128d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [5. 2.]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [5. 2.]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n"
     ]
    }
   ],
   "source": [
    "path_gd_f1, path_new_f1 = get_solutions(f1, df1, hess_f1, punto1_gd, punto1_new, maxit, tolleranza_f, tolleranza_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2096d54",
   "metadata": {},
   "source": [
    "### 3.2 Generazione Grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2f8ce90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dule/anaconda3/envs/CN25/lib/python3.13/tkinter/__init__.py:862: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  func(*args)\n"
     ]
    }
   ],
   "source": [
    "# Generazione grafico\n",
    "graph_generator(x_range1, y_range1, f1, path_gd_f1, path_new_f1, tmin1, titolo1)\n",
    "error_graph(f1, df1, path_gd_f1, path_new_f1, titolo1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687edc4a",
   "metadata": {},
   "source": [
    "--- \n",
    "## 4. Funzione di Rosenbrock: $f(x, y) = (1-x)^2+100(y-x^2)^2$\n",
    "\n",
    "Cerchiamo i punti critici della funzione.\n",
    "\n",
    "Calcoliamo la derivata rispetto a $x$ e a $y$.\n",
    "\n",
    "$\\frac{df}{dx} = -2(1-x) + 200(y-x^2)(-2x) = 2(x-1) -400x(y-x^2) $\n",
    "\n",
    "$\\frac{df}{dy} =​ 200(y-x^2)$\n",
    "\n",
    "Poniamo $y=x^2$, troviamo che $ 2(x-1) -400x(x^2-x^2)=0 \\to 2x-2=0 \\to x=1 $  \n",
    "\n",
    "Quindi l'unico punto critico è su $(1,1)$\n",
    "\n",
    "$H= \\begin{bmatrix} \n",
    "\\frac{d^2f}{dx^2} & \\frac{d^2f}{dxdy} \\\\\n",
    "\\frac{d^2f}{dydx} & \\frac{d^2f}{dy^2}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix} \n",
    " 2-400y+1200x^2& -400x \\\\\n",
    "-400x & 200\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Studiamo $\\det(H)$.\n",
    "\n",
    "$P(1,1) = 802\\cdot 200 - 160000 = 160400 - 160000 = 400 > 0$, quindi è un punto di minimo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071591da",
   "metadata": {},
   "source": [
    "### 4.1 Implementazione dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d99e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    return (1-x)**2+100*(y-x**2)**2\n",
    "\n",
    "def df2(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    df_dx = -2*(1-x) - 400*(y-x**2)*x\n",
    "    df_dy = 200*(y-x**2)\n",
    "    \n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def hess_f2(X):\n",
    "    HESS=np.zeros((2,2)) # Crea una matrice 2x2\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    HESS[0][0] = 2 - 400*y + 1200*x**2 \n",
    "    HESS[0][1] = -400*x \n",
    "    HESS[1][0] = HESS[0][1] # Per teorema di Sxhwarz\n",
    "    HESS[1][1] = 200 \n",
    "    return HESS\n",
    "    \n",
    "\n",
    "x_range2=np.linspace(-2,2,200)\n",
    "y_range2=np.linspace(-1,3,200)\n",
    "\n",
    "maxit = 5000\n",
    "tolleranza_f = 1.e-6\n",
    "tolleranza_x = 1.e-6\n",
    "punto2_gd=np.array([0.0,0.0])\n",
    "punto2_new=np.array([0.5,0.5])\n",
    "tmin2 = [1.0,1.0]\n",
    "titolo2 = r\"$f(x, y) = (1-x)^2+100(y-x^2)^2$\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3096080e",
   "metadata": {},
   "source": [
    "### 4.2 Soluzioni dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d2f42ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [0.9998038  0.99960895]\n",
      "Numero iterazioni: 2200\n",
      "Condizione di uscita: tol_x\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [0.99999971 0.99999935]\n",
      "Numero iterazioni: 10\n",
      "Condizione di uscita: tol_x\n"
     ]
    }
   ],
   "source": [
    "path_gd_r, path_newton_r = get_solutions(f2, df2, hess_f2, punto2_gd, punto2_new, maxit, tolleranza_f, tolleranza_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5478e08",
   "metadata": {},
   "source": [
    "### 4.3 Generazione Grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c04dabfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid command name \"131780283445760process_stream_events\"\n",
      "    while executing\n",
      "\"131780283445760process_stream_events\"\n",
      "    (\"after\" script)\n"
     ]
    }
   ],
   "source": [
    "# Grafico del metodo GD\n",
    "graph_generator(x_range2, y_range2, f2, path_gd_r, path_newton_r, tmin2, titolo2)\n",
    "error_graph(f2, df2, path_gd_r, path_newton_r, titolo2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbbfc40",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 Funzione multimodale: $f(x,y) = x^4-x^2+y^2$\n",
    "\n",
    "Cerchiamo i punti critici della funzione.\n",
    "\n",
    "Calcoliamo la derivata rispetto a $x$ e a $y$.\n",
    "\n",
    "- $\\frac{df}{dx} = ​4x^3-2x = 2x(2x^2-1) $\n",
    "- $\\frac{df}{dy} =​ 2y$\n",
    "\n",
    "Notiamo che ponendo $x=0$ e $y= 0$ troviamo il nostro primo punto critico.\n",
    "\n",
    "Ponendo $y=0$, troviamo i casi in cui $2x^2-1=0$, ovvero $x= \\pm \\sqrt{\\frac12}$, ovvero $\\simeq 0.70717$\n",
    "\n",
    "Calcoliamo se questi punti critici sono punti minimi, massimi o di sella.\n",
    "\n",
    "$H= \\begin{bmatrix} \n",
    "\\frac{d^2f}{dx^2} & \\frac{d^2f}{dxdy} \\\\\n",
    "\\frac{d^2f}{dydx} & \\frac{d^2f}{dy^2}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix} \n",
    "12x^2-2 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Studiamo $ \\det(H)$.\n",
    "\n",
    "$P(0,0) = -4 < 0$, quindi è un punto di sella\n",
    "\n",
    "$P(\\sqrt{\\frac12},0) = 2(6-2) = 8 > 0$, quindi è un punto di minimo\n",
    "\n",
    "$P(-\\sqrt{\\frac12},0) = 2(6-2) = 8 >0$, quindi è un punto di minimo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46a523a",
   "metadata": {},
   "source": [
    "### 5.1 Implementazione dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37944ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    return x**4-x**2+y**2\n",
    "\n",
    "def df3(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    df_dx = 4*x**3-2*x\n",
    "    df_dy = 2*y\n",
    "    \n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def hess_f3(X):\n",
    "    HESS=np.zeros((2,2))\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "\n",
    "    HESS[0][0] = 12*x**2-2 # Derivata seconda rispetto a x\n",
    "    HESS[0][1] = 0 # Derivata mista\n",
    "    HESS[1][0] = HESS[0][1] # Per teorema di Sxhwarz, la derivata mista è uguale\n",
    "    HESS[1][1] = 2 # Derivata seconda rispetto a y\n",
    "    return HESS\n",
    "    \n",
    "\n",
    "x_range3=np.linspace(-1.5,1.5,200)\n",
    "y_range3=np.linspace(-1.5,1.5,200)\n",
    "\n",
    "maxit = 5000\n",
    "tolleranza_f = 1.e-6\n",
    "tolleranza_x = 1.e-6\n",
    "punto3_neg=np.array([-1.0,-1.0]) \n",
    "punto3_pos=np.array([1.0,1.0]) \n",
    "punto3_sad=np.array([0.0,5.0]) \n",
    "tmin3 = [0.0, 0.0]\n",
    "tmin3_alt = [-np.sqrt(1/2), 0.0]\n",
    "titolo3 = r\"$f(x,y) = x^4-x^2+y^2$\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bccf36",
   "metadata": {},
   "source": [
    "### 5.2 Soluzioni dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc864a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [0. 0.]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [-0.70710712  0.        ]\n",
      "Numero iterazioni: 4\n",
      "Condizione di uscita: tol_x\n"
     ]
    }
   ],
   "source": [
    "path_gd_m, path_new_m = get_solutions(f3, df3, hess_f3, punto3_pos, punto3_neg, maxit, tolleranza_f, tolleranza_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438c2880",
   "metadata": {},
   "source": [
    "### 5.3 Generazione Grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "663a79f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid command name \"131780337958528process_stream_events\"\n",
      "    while executing\n",
      "\"131780337958528process_stream_events\"\n",
      "    (\"after\" script)\n"
     ]
    }
   ],
   "source": [
    "# Gradiente\n",
    "graph_generator(x_range3, y_range3, f3, path_gd_m, path_new_m, tmin3, titolo3)\n",
    "error_graph(f3, df3, path_gd_m, path_new_m, titolo3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6196fa8",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Minimi Quadrati Lineari: $f(x) = \\frac12||Ax-b||_2^2$ con $A \\in \\R^{n \\times n}$\n",
    "\n",
    "Definiamo la $i$-esima funzione componente come: $f_i(x) = \\frac{1}{2} ( A_ix-b_i )^2$\n",
    "\n",
    "Per la linearità dell'operatore gradiente, il gradiente della somma è la somma dei gradienti:\n",
    "$\\nabla f(x) = \\nabla \\left( \\sum_{i=1}^m f_i(x) \\right) = \\sum_{i=1}^m \\nabla f_i(x)$\n",
    "\n",
    "Applichiamo la regola della catena:\n",
    "$\\nabla f_i(x) = \\frac{1}{2} \\cdot 2 \\cdot (A_i x - b_i) \\cdot \\nabla(A_i x - b_i)$\n",
    "\n",
    "Ora, calcoliamo $\\nabla(A_i x - b_i)$. La funzione $g(x) = A_i x - b_i$ è:\n",
    "$g(x) = (a_{i1}x_1 + a_{i2}x_2 + \\dots + a_{in}x_n) - b_i$\n",
    "\n",
    "Calcoliamo le derivate parziali rispetto a ogni $x_j$:\n",
    "$\\frac{\\partial g}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} (a_{i1}x_1 + \\dots + a_{ij}x_j + \\dots + a_{in}x_n - b_i) = a_{ij}$\n",
    "\n",
    "Assemblando queste derivate parziali in un vettore gradiente (colonna):\n",
    "$$ \\nabla(A_i x - b_i) = \n",
    "   \\begin{bmatrix} \n",
    "   \\frac{\\partial g}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial g}{\\partial x_n} \n",
    "   \\end{bmatrix} = \n",
    "   \\begin{bmatrix} \n",
    "   a_{i1} \\\\ \\vdots \\\\ a_{in} \n",
    "   \\end{bmatrix} = A_i^T $$\n",
    "(Questo è il trasposto della *riga* $A_i$).\n",
    "\n",
    "Sostituendo questo risultato, otteniamo il gradiente della $i$-esima componente:\n",
    "$\\nabla f_i(x) = (A_i x - b_i) A_i^T$\n",
    "Questo è un vettore $n \\times 1$ (uno scalare moltiplicato per un vettore colonna).\n",
    "\n",
    "**Ora sommiamo i gradienti di tutte le $m$ componenti:**\n",
    "\n",
    "$\\nabla f(x) = \\sum_{i=1}^m \\nabla f_i(x) = \\sum_{i=1}^m (A_i x - b_i) A_i^T$\n",
    "\n",
    "Definiamo il vettore \"errore\" $e = Ax - b$, dove $e_i = (A_i x - b_i)$ è la $i$-esima componente (scalare). La somma diventa:\n",
    "$\\nabla f(x) = \\sum_{i=1}^m e_i A_i^T$\n",
    "\n",
    "Analizziamo la matrice $A^T \\in \\R^{n \\times m}$. Le **colonne** di $A^T$ sono i vettori $A_i^T$:\n",
    "$$ A^T = \n",
    "   \\begin{bmatrix} \n",
    "   | & | & & | \\\\ \n",
    "   A_1^T & A_2^T & \\dots & A_m^T \\\\ \n",
    "   | & | & & | \n",
    "   \\end{bmatrix} $$\n",
    "\n",
    "La moltiplicazione matrice-vettore $A^T e$ è definita come la combinazione lineare delle colonne di $A^T$ con i pesi (scalari) $e_i$:\n",
    "$$ A^T e = \n",
    "   \\begin{bmatrix} \n",
    "   | & \\dots & | \\\\ \n",
    "   A_1^T & \\dots & A_m^T \\\\ \n",
    "   | & \\dots & | \n",
    "   \\end{bmatrix} \n",
    "   \\begin{bmatrix} \n",
    "   e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_m \n",
    "   \\end{bmatrix} \n",
    "   = e_1 A_1^T + e_2 A_2^T + \\dots + e_m A_m^T = \\sum_{i=1}^m e_i A_i^T $$\n",
    "\n",
    "Quindi $\\nabla f(x) = A^T (Ax - b)$\n",
    "\n",
    "I punti minimi sono $A^T(Ax-b)=0$\n",
    "\n",
    "Prendendo il gradiente ottenuto, notiamo che l'Hessiana è semplicemente $A^TA$, ovvero una costante.\n",
    "\n",
    "Questo significa che la curvatura del grafico è uguale ad ogni punto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da0bd1f",
   "metadata": {},
   "source": [
    "**Nota: questa è una loss function:**\n",
    "\n",
    "- $A$ sono i dati, ovvero il dataset in input\n",
    "\n",
    "- $x$ sono i parametri del modello, ovvero i $ \\theta$\n",
    "\n",
    "- $b$ sono i valori reali\n",
    "\n",
    "- $Ax-b$ sono le previsioni del modello - i valori reali: sono la distanza tra i valori predetti e quelli reali\n",
    "\n",
    "Lo eleviamo al quadrato per avere una distanza positiva e al quadrato per dare importanza ai valori distanti a quelli predetti.\n",
    "\n",
    "Quindi, questa funzione serve per quantificare quanto è \"sbagliato\" il nostro modello."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be580c84",
   "metadata": {},
   "source": [
    "## 6.1 Implementazione dei Metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19d1e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f4(x, A, b):\n",
    "    e = A @ x -b\n",
    "    norm_sq = np.dot(e, e)\n",
    "    return 1/2 * norm_sq\n",
    "\n",
    "def df4(X, A, b):\n",
    "    return A.T @ (A @ X - b)\n",
    "\n",
    "def hess_f4(A):\n",
    "    return A.T @ A\n",
    "\n",
    "def sdf4(x, indices, A, b):\n",
    "    A_batch= A[indices, :]\n",
    "    b_batch= b[indices]\n",
    "    \n",
    "    return A_batch.T @ (A_batch @ x - b_batch)\n",
    "\n",
    "n = 50 \n",
    "A = np.random.rand(n,n) \n",
    "b = A @ np.ones(n) \n",
    "X_start_1 = np.zeros(n) \n",
    "X_start_2 = np.random.randn(n)\n",
    "\n",
    "title_f4 = r\"$f(x) = \\frac{1}{2}\\|Ax-b\\|_2^2 \\text{ con } A \\in \\mathbb{R}^{n \\times n}$\"\n",
    "maxit = 2000\n",
    "maxit_SGD = 20000\n",
    "tol_grad = 1e-3\n",
    "tol_step = 1e-6\n",
    "batch_k = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "f_min_sq = lambda x: f4(x, A, b) # Equivalente a scrivere def f_min_sq(x): return f4(x, A, b)\n",
    "df_min_sq = lambda x: df4(x, A, b) \n",
    "h_min_sq = lambda x: hess_f4(A)\n",
    "sdf_min_sq = lambda x, idx: sdf4(x, idx, A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97306e0",
   "metadata": {},
   "source": [
    "## 6.2 Soluzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc80fd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [1.01152232 1.00052267 1.01217291 0.99579023 0.98049756 1.0199512\n",
      " 0.99144506 0.98398371 1.00695575 1.00235895 1.01631959 0.98742921\n",
      " 1.00608326 0.9972468  1.01570454 1.01516282 1.01386927 1.00230167\n",
      " 1.0118751  1.00246302 1.0170393  0.97611337 0.99367676 1.01017065\n",
      " 1.00157371 1.00078452 1.00091149 1.005342   0.98919008 0.98904217\n",
      " 1.01019523 0.98608156 1.01285332 1.00654458 0.99217518 0.96979779\n",
      " 1.01317068 1.00133523 0.99418201 1.00550677 0.99470613 0.97610048\n",
      " 0.99052814 1.00160309 1.00457812 1.00723355 0.98507842 0.99563384\n",
      " 0.98640579 0.99270217]\n",
      "Numero iterazioni: 2000\n",
      "Condizione di uscita: maxit\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1.]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo del Gradiente Stocastico (SGD) ---\n",
      "Soluzione: [1.02308454 1.00641879 1.00086112 1.00488687 0.96164928 1.00598435\n",
      " 0.97364256 0.9916934  1.00701938 0.96499592 1.00585474 0.97025622\n",
      " 1.03334551 1.01760674 1.01116175 1.01269051 1.02740869 0.9872186\n",
      " 1.05158269 1.0080631  1.02315575 0.97304114 0.97496878 1.06513219\n",
      " 1.03552911 0.99773846 0.99512588 0.9713546  0.94406487 0.98407817\n",
      " 1.04598416 0.95107493 1.01308592 0.98732962 0.99927363 0.93090719\n",
      " 1.04681789 0.97080541 1.00618532 1.0094497  1.02855645 0.92698151\n",
      " 0.9852701  1.00205901 1.03685019 1.03578153 0.99860115 1.00222064\n",
      " 0.94213147 0.96125206]\n",
      "Numero iterazioni: 20000\n",
      "Condizione di uscita: maxit\n",
      "Epoche fatte: 2000\n"
     ]
    }
   ],
   "source": [
    "path_gd_f4, path_newton_f4 = get_solutions(f_min_sq, df_min_sq, h_min_sq, X_start_1, X_start_2, maxit, tol_grad, tol_step)\n",
    "\n",
    "path_sgd_f4 = get_SGD_solutions(df_min_sq, sdf_min_sq, X_start_1, n, maxit_SGD, tol_grad, tol_step, batch_k, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e86802",
   "metadata": {},
   "source": [
    "## 6.3 Generazione Grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5005e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_graph(f_min_sq, df_min_sq, path_gd_f4, path_newton_f4, title_f4, path_sgd_f4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b9f2c0",
   "metadata": {},
   "source": [
    "Notiamo che newton è velocissimo. Questo perchè in funzioni quadrate, salta direttamente in quel punto.\n",
    "Il grafico del gradiente è rumoroso, perchè nella iterazione k fa un passo fuori rotta, il grafico sale, per poi essere ritornato nella iterazione k+1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a619d",
   "metadata": {},
   "source": [
    "---\n",
    "# 7 Minimi quadrati con regolarizzazione: $f(x) = \\frac{1}{2}||Ax-b||^2_2 + \\lambda ||x||^2_2$, con $\\lambda \\in [0,1]$\n",
    "\n",
    "Questa funzione viene chiamata anche come Regressione di Ridge.\n",
    "\n",
    "Ora dobbiamo trovare il termine noto $\\frac{1}{2}||Ax-b||^2_2$ e il termine di regolarizzazione $\\lambda ||x||^2_2$\n",
    "\n",
    "La medesima, vuole trovare una x con componenti (pesi) il più possibile vicini a zero. Penalizza soluzioni con valori molto grandi.\n",
    "\n",
    "Separiamo il problema come $\\nabla f(x) = \\nabla f_{LS}(x) + \\nabla f_{Reg}(x)$\n",
    "\n",
    "Troviamo il gradiente:\n",
    "\n",
    "- Come già fatto nella funzione precedente, $\\nabla f_{LS}(x) = A^T (Ax - b)$\n",
    "\n",
    "- $\\nabla f_{Reg} = 2 \\lambda x $\n",
    "\n",
    "Quindi la soluzione è $\\nabla f(x) = A^T (Ax - b) + 2\\lambda x$\n",
    "\n",
    "Per l'Hessiana, scomponiamo la funzione $H=f_A+f_\\lambda$. \n",
    "\n",
    "$f_A$ l'abbiamo già definita nel caso di funzione precedente $A^TA$\n",
    "\n",
    "$f_\\lambda$, invece, ha le derivate per $H_{ki}$ con $k != i$ sono uguali a $0$ perchè sono costanti.\n",
    "\n",
    "Mentre per $k=i$, ovvero la matrice diagonale, non vengono considerate costanti e la loro derivata è $2\\lambda$, \n",
    "\n",
    "Quindi $A^T A + 2\\lambda I$, dove I è la matrice identità."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d321c30",
   "metadata": {},
   "source": [
    "## 7.1 Definisco le funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f3cbb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisco la funzione\n",
    "def f5(x, A, b, lambda_val):\n",
    "    e = A @ x -b\n",
    "    errore_ls = 1/2 * np.dot(e, e)\n",
    "    errore_reg = lambda_val*np.dot(x,x)\n",
    "    return errore_ls + errore_reg\n",
    "\n",
    "# Questa calcola il GRADIENTE COMPLETO (Batch)\n",
    "def df5(x, A, b, lambda_val):\n",
    "    grad_ls = A.T @ (A @ x - b)\n",
    "    grad_reg = 2*lambda_val*x\n",
    "    return grad_ls +  grad_reg\n",
    "\n",
    "# Hessiana\n",
    "def hess_f5(A, lambda_val):\n",
    "    return A.T @ A + 2 * lambda_val * np.identity(n)\n",
    "\n",
    "# Questa calcola il GRADIENTE STOCASTICO (Mini-Batch)\n",
    "def sdf5(x, indices, A, b, lambda_val):\n",
    "    A_batch= A[indices, :]\n",
    "    b_batch= b[indices]\n",
    "\n",
    "    grad_ls_stoc = A_batch.T @ (A_batch @ x - b_batch)\n",
    "    grad_reg = 2 * lambda_val * x\n",
    "    \n",
    "    return grad_ls_stoc + grad_reg\n",
    "\n",
    "n = 50 # siamo in R^50\n",
    "A = np.random.rand(n,n) # A è una matrice n x n con valori casuali\n",
    "X_true = np.ones(n)\n",
    "b = A @ X_true + 0.1 * np.random.randn(n) # rendiamo b in modo che b= A(vettori 1)+rumore\n",
    "lambda_val = 0.1 \n",
    "\n",
    "titolo_f5 = r\"$f(x) = \\frac{1}{2}\\|Ax-b\\|_2^2 + \\lambda \\|x\\|_2^2 \\text{ con } \\lambda \\in [0,1]$\"\n",
    "X_start1 = np.zeros(n)      # Punto di partenza\n",
    "X_start2 = np.ones(n)      # Punto di partenza\n",
    "maxit = 2000\n",
    "maxit_SGD = 20000         # L'SGD richiede molti più passi del GD\n",
    "batch_k = 5                # Dimensione del mini-batch\n",
    "learning_rate = 0.001      # Alpha (deve essere molto piccolo)\n",
    "# NOTA: Una epoca è fatta ad ogni 10 iterazioni, perchè n/batch_k = 50 / 5 = 10\n",
    "\n",
    "f_min_sqr = lambda x: f5(x, A, b, lambda_val)\n",
    "df_min_sqr = lambda x: df5(x, A, b, lambda_val)\n",
    "h_min_sqr = lambda x: hess_f5(A, lambda_val)\n",
    "sdf_min_sqr = lambda x, idx: sdf5(x, idx, A, b, lambda_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f2b35",
   "metadata": {},
   "source": [
    "## 7.2 Soluzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd31a4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [0.90851979 1.06626785 1.05103482 1.07641106 1.02862166 0.95714053\n",
      " 0.90873702 1.00043408 0.98201779 0.8936849  0.98151688 0.90575344\n",
      " 1.09602631 1.02056512 1.0192956  1.05226951 0.86739431 0.94959825\n",
      " 0.95848545 1.08539647 1.05298013 1.04179387 0.90640231 1.01247384\n",
      " 0.93532497 1.0401471  0.9487455  0.96228804 0.99131204 0.91623973\n",
      " 0.95229556 0.93549642 0.90480037 0.97744093 1.0558069  0.96811907\n",
      " 0.93566282 1.0781445  0.90285515 1.14738763 0.89095749 1.20702273\n",
      " 1.13843741 1.05794841 1.03700812 1.10140275 1.0631378  0.91927001\n",
      " 1.04444058 1.0117241 ]\n",
      "Numero iterazioni: 2000\n",
      "Condizione di uscita: maxit\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [0.90740807 1.06663965 1.04949208 1.08018149 1.02919425 0.95908122\n",
      " 0.91029718 1.00127583 0.98215715 0.89388185 0.98201905 0.9038801\n",
      " 1.09624463 1.01999562 1.01748005 1.05236295 0.86482586 0.94850672\n",
      " 0.958292   1.08580567 1.05450761 1.04217094 0.90506324 1.01370078\n",
      " 0.93700071 1.04059328 0.94888206 0.96157546 0.99102855 0.91577455\n",
      " 0.95146302 0.93670162 0.90338057 0.97948968 1.05391595 0.96861566\n",
      " 0.93602699 1.07931177 0.90275784 1.14643469 0.88945949 1.20853708\n",
      " 1.1398357  1.05790479 1.03686157 1.10084629 1.06200149 0.91879806\n",
      " 1.04509588 1.01083755]\n",
      "Numero iterazioni: 1\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo del Gradiente Stocastico (SGD) ---\n",
      "Soluzione: [0.98168484 0.96706839 1.03860769 0.96074961 1.00317873 0.92349638\n",
      " 0.96979946 0.93407138 0.947996   0.94971682 0.99907921 0.98172465\n",
      " 1.00744958 0.97242853 1.09308666 1.00410317 0.95672804 1.03912225\n",
      " 0.97959121 0.97615019 1.09296734 0.97909957 0.99089084 0.94356636\n",
      " 0.88970622 0.98993287 0.91776108 1.07888924 0.91259209 0.87364697\n",
      " 1.01791861 0.96176526 1.05358752 0.97278465 1.0159983  0.96112651\n",
      " 0.93825797 1.03914574 0.93947588 1.15780609 1.00103399 1.06579263\n",
      " 1.06960083 0.99375223 1.00379327 1.06569494 1.04724893 1.00450744\n",
      " 0.96595094 1.01133325]\n",
      "Numero iterazioni: 20000\n",
      "Condizione di uscita: maxit\n",
      "Epoche fatte: 2000\n"
     ]
    }
   ],
   "source": [
    "path_gd_f5, path_newton_f5 = get_solutions(f_min_sqr, df_min_sqr, h_min_sqr, X_start1, X_start2, maxit, tol_grad, tol_step)\n",
    "\n",
    "path_sgd_f5 = get_SGD_solutions(df_min_sqr, sdf_min_sqr, X_start1, n, maxit_SGD, tol_grad, tol_step, batch_k, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db42ed6",
   "metadata": {},
   "source": [
    "## 7.3 Generazione Grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5039254f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid command name \"131780326405440process_stream_events\"\n",
      "    while executing\n",
      "\"131780326405440process_stream_events\"\n",
      "    (\"after\" script)\n"
     ]
    }
   ],
   "source": [
    "error_graph(f_min_sqr, df_min_sqr, path_gd_f5, path_newton_f5, titolo_f5, path_sgd_f5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379d5c3",
   "metadata": {},
   "source": [
    "---\n",
    "# 8 $f(x) = \\sum \\limits_{i=1}^n (x_i-i)^2-\\sum \\limits_{i=1}^n \\ln(x_i)$, con $x_i>0$\n",
    "\n",
    "$\\nabla f(x) = 2 (x-i) - \\frac{1}{x}$\n",
    "\n",
    "Notiamo che per $ i!= k$,$H_{ik}$ non ha elementi perchè la derivata rispetto a x_k ha solo costanti.\n",
    "\n",
    "I punti minimi sono: $2 (x-i) - \\frac{1}{x} = 0$ \n",
    "\n",
    "Ovvero $2x^2-2ix-1 = 0 \\to \\frac{2i\\pm\\sqrt{4i^2+8}}{4}$\n",
    "\n",
    "Invece, per $i = k$, $H_{ik} = 2+(\\frac{1}{x^2})$\n",
    "\n",
    "Quindi l'Hessiana ha solo elementi nella diagonale\n",
    "\n",
    "**Nota**: qua il metodo stocastico è inutile, dato che il gradiente non ha matrici e il metodo del gradiente ha già un costo $O(n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "331a89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f6 (x, idx):\n",
    "    if np.any(x <= 0):\n",
    "        return np.inf # ritorna infinito se x non è nel dominio (ln(x) con x<=0)\n",
    "\n",
    "    sum1 = np.sum((x-idx)**2)\n",
    "    sum2 = np.sum(np.log(x))\n",
    "    return sum1 - sum2\n",
    "    \n",
    "def df6 (x, idx):\n",
    "    return 2 * (x - idx) - 1/x\n",
    "\n",
    "def hess_f6 (x):\n",
    "    HESS = 2+(1/(x**2))\n",
    "    return np.diag(HESS)\n",
    "\n",
    "def sdf6(x, mini_batch, n, idx):\n",
    "\n",
    "    x_batch = x[mini_batch]\n",
    "    idx_batch = idx[mini_batch]\n",
    "    grad_stoc = np.zeros(n)\n",
    "    \n",
    "    # Calcola il gradiente solo per quei componenti\n",
    "    grad_batch_components = 2 * (x_batch - idx_batch) - (1 / x_batch)\n",
    "    \n",
    "    # Inserisci i componenti calcolati nel vettore zero\n",
    "    grad_stoc[mini_batch] = grad_batch_components\n",
    "    \n",
    "    return grad_stoc\n",
    "\n",
    "\n",
    "title_f6 = r\"$f(x) = \\sum_{i=1}^n (x_i-i)^2 - \\sum_{i=1}^n \\ln(x_i)$\"\n",
    "n = 50\n",
    "maxit_SGD = 10000\n",
    "batch_k = 10\n",
    "idx = np.arange(1, n+1)\n",
    "X_start1 = np.ones(n)\n",
    "X_start2 = np.random.rand(n) + 0.1\n",
    "\n",
    "f_lambda = lambda x: f6(x, idx)\n",
    "df_lambda = lambda x: df6(x, idx)\n",
    "h_lambda = lambda x: hess_f6(x)\n",
    "sdf_lambda = lambda x, k: sdf6(x, k, n, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e359e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [ 1.36585366  2.22474227  3.1583123   4.12132034  5.09807621  6.082207\n",
      "  7.07071421  8.0620192   9.05521679 10.04975247 11.04526825 12.04152299\n",
      " 13.03834842 14.03562364 15.03325959 16.0311892  17.02936105 18.02773504\n",
      " 19.02627944 20.02496883 21.02378259 22.02270384 23.02171862 24.02081528\n",
      " 25.01998403 26.01921657 27.01850583 28.01784577 29.01723114 30.01665742\n",
      " 31.01612065 32.01561738 33.01514456 34.01469953 35.01427989 36.01388353\n",
      " 37.01350858 38.01315334 39.0128163  40.0124961  41.0121915  42.01190139\n",
      " 43.01162476 44.0113607  45.01110837 46.010867   47.01063589 48.01041441\n",
      " 49.01020196 50.009998  ]\n",
      "Numero iterazioni: 6\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [ 1.3660254   2.22474487  3.1583124   4.12132034  5.09807616  6.08220023\n",
      "  7.07071421  8.0620192   9.05521679 10.04975247 11.04526825 12.04152299\n",
      " 13.03834842 14.03562364 15.03325958 16.0311892  17.02936105 18.02773504\n",
      " 19.02627944 20.02496883 21.02378259 22.02270384 23.02171862 24.02081528\n",
      " 25.01998403 26.01921657 27.01850583 28.01784577 29.01723114 30.01665742\n",
      " 31.01612065 32.01561738 33.01514456 34.01469953 35.01427989 36.01388353\n",
      " 37.01350858 38.01315334 39.0128163  40.0124961  41.0121915  42.01190139\n",
      " 43.01162476 44.0113607  45.01110837 46.010867   47.01063589 48.01041441\n",
      " 49.01020196 50.009998  ]\n",
      "Numero iterazioni: 4\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo del Gradiente Stocastico (SGD) ---\n",
      "Soluzione: [ 1.36418813  2.20721855  3.12854658  4.06878034  5.02067329  5.98432252\n",
      "  6.96392994  7.93294487  8.90283201  9.87959953 10.86207261 11.83676838\n",
      " 12.82238848 13.80131729 14.76703546 15.74674231 16.73135866 17.71621821\n",
      " 18.69821807 19.67323301 20.6608926  21.63295377 22.61812076 23.59765063\n",
      " 24.57112772 25.564892   26.53638517 27.52139296 28.50612899 29.47423276\n",
      " 30.45666754 31.44527009 32.43012238 33.40503022 34.38215334 35.37016568\n",
      " 36.35477416 37.33609631 38.32057587 39.29945032 40.27692661 41.26639729\n",
      " 42.2449767  43.22274558 44.20796967 45.18838636 46.16202377 47.14444729\n",
      " 48.1244516  49.1188442 ]\n",
      "Numero iterazioni: 10000\n",
      "Condizione di uscita: maxit\n",
      "Epoche fatte: 2000\n"
     ]
    }
   ],
   "source": [
    "path_gd_f6, path_newton_f6 = get_solutions(f_lambda, df_lambda, h_lambda, X_start1, X_start2, maxit, tol_grad, tol_step)\n",
    "\n",
    "path_sgd_f6 = get_SGD_solutions(df_lambda, sdf_lambda, X_start2, n, maxit_SGD, tol_grad, tol_step, batch_k, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ef8e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_graph(f_lambda, df_lambda, path_gd_f6, path_newton_f6, title_f6, path_sgd_f6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89828313",
   "metadata": {},
   "source": [
    "---\n",
    "# 9 $f(x) = \\sum \\limits_{i=1}^n (x_i-b_i)^2+c\\sum \\limits_{i=1}^n \\ln(x_i)$\n",
    "\n",
    "$\\nabla f(x) = 2(x_i-b_i) + c\\frac{1}{x_i}$\n",
    "\n",
    "I punti minimi sono $2(x_i-b_i) + c\\frac{1}{x_i} = 0 \\to 2x_i^2-2x_ib_i+c = 0 \\to \\frac{2b_i\\pm\\sqrt{4b_i^2-8c}}{4}$\n",
    "\n",
    "**Nota**: dato che abbiamo $\\ln(x_i)$, quando la funzione ha $x\\simeq 0$, si trova $-\\infty$. Quindi non esiste un minimo locale.\n",
    "\n",
    "Scomponiamo la funzione in $f(x) = f_{xb} + f_{ln}$\n",
    "\n",
    "Per l'Hessiana, se deriviamo rispetto a $x_k$:\n",
    "\n",
    "$f_{xb} = 2$, ovvero una $2\\cdot I$, la matrice identità\n",
    "\n",
    "$f_{ln} = -c\\frac{1}{x^2}$, sempre nella matrice diagonale.\n",
    "\n",
    "Quindi $H_{ik} = 2-c\\frac{1}{x^2}$\n",
    "\n",
    "**Note**: questa Hessiana non è sempre definita positiva, quindi è più difficile minimizzare.\n",
    "\n",
    "- Il metodo di Newton convergerà verso il risultato, il metodo del gradiente non funziona e quello del SGD converge molto lentamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81f0fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f7(x,c,b):\n",
    "\n",
    "    sum1 = np.sum((x-b)**2)\n",
    "    sum2 = c*np.sum(np.log(x))\n",
    "\n",
    "    return sum1+sum2\n",
    "\n",
    "def df7(x,c,b):\n",
    "    return 2*(x-b)+c*(1/x)\n",
    "\n",
    "def hess_f7(x,c):\n",
    "    HESS =2-c*(1/(x**2))\n",
    "    return np.diag(HESS)\n",
    "\n",
    "def sdf7(x, c, b, n, mini_batch):\n",
    "    x_batch = x[mini_batch]\n",
    "    b_batch = b[mini_batch]\n",
    "    grad_stoc = np.zeros(n)\n",
    "    \n",
    "    grad_batch_components = 2 * (x_batch - b_batch) + c*(1 / x_batch)\n",
    "    grad_stoc[mini_batch] = grad_batch_components\n",
    "    \n",
    "    return grad_stoc\n",
    "\n",
    "title_f7 = r\"$f(x) = \\sum_{i=1}^n (x_i-b_i)^2+c\\sum_{i=1}^n \\ln(x_i)$\"\n",
    "b = np.arange(1, n+1)\n",
    "X_start1 = np.ones(n)\n",
    "X_start2 = np.random.rand(n)+0.1\n",
    "c= -2.0 # DEVE essere negativo\n",
    "\n",
    "f_c = lambda x: f7(x, c, b)\n",
    "df_c = lambda x: df7(x, c, b)\n",
    "h_c = lambda x: hess_f7(x, c)\n",
    "sdf_c = lambda x, k: sdf7(x, c, b, n, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bb0ac08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Metodo del Gradiente (GD) ---\n",
      "Soluzione: [ 1.61818182  2.41421393  3.30277564  4.23606798  5.1925824   6.16227766\n",
      "  7.14005494  8.12310563  9.10977223 10.09901951 11.09016994 12.08276253\n",
      " 13.07647322 14.07106781 15.06637298 16.06225775 17.05862138 18.05538514\n",
      " 19.05248659 20.04987562 21.04751155 22.04536102 23.04339638 24.04159458\n",
      " 25.0399362  26.03840481 27.03698637 28.03566885 29.03444185 30.03329638\n",
      " 31.03222457 32.03121954 33.03027525 34.02938637 35.02854814 36.02775638\n",
      " 37.02700731 38.02629759 39.02562419 40.02498439 41.02437575 42.02379604\n",
      " 43.02324325 44.02271555 45.02221126 46.02172887 47.02126697 48.0208243\n",
      " 49.02039967 50.01999201]\n",
      "Numero iterazioni: 9\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo di Newton ---\n",
      "Soluzione: [ 1.61802747  2.41421356  3.30277564  4.23606798  5.1925824   6.16227766\n",
      "  7.14005494  8.12310563  9.10977223 10.09901951 11.09016994 12.08276253\n",
      " 13.07647322 14.07106781 15.06637298 16.06225775 17.05862138 18.05538514\n",
      " 19.05248659 20.04987562 21.04751155 22.04536102 23.04339638 24.04159458\n",
      " 25.0399362  26.03840481 27.03698637 28.03566885 29.03444185 30.03329638\n",
      " 31.03222457 32.03121954 33.03027525 34.02938637 35.02854814 36.02775638\n",
      " 37.02700731 38.02629759 39.02562419 40.02498439 41.02437575 42.02379604\n",
      " 43.02324325 44.02271555 45.02221126 46.02172887 47.02126697 48.0208243\n",
      " 49.02039967 50.01999201]\n",
      "Numero iterazioni: 6\n",
      "Condizione di uscita: tol_f\n",
      "--- Metodo del Gradiente Stocastico (SGD) ---\n",
      "Soluzione: [ 1.61587975  2.40269507  3.27626714  4.19210342  5.13023449  6.08122228\n",
      "  7.04020036  8.00445019  8.972348    9.94286927 10.91533768 11.88928971\n",
      " 12.86439745 13.84042262 14.81718802 15.79455921 16.77243235 17.75072602\n",
      " 18.72937547 19.70832857 20.6875429  21.66698357 22.64662165 23.62643291\n",
      " 24.60639696 25.58649646 26.56671661 27.5470447  28.52746972 29.50798213\n",
      " 30.48857359 31.4692368  32.44996532 33.43075347 34.4115962  35.39248902\n",
      " 36.37342792 37.3544093  38.33542994 39.31648692 40.29757762 41.27869965\n",
      " 42.25985086 43.24102927 44.2222331  45.20346069 46.18471056 47.16598131\n",
      " 48.14727169 49.12858054]\n",
      "Numero iterazioni: 10000\n",
      "Condizione di uscita: maxit\n",
      "Epoche fatte: 2000\n",
      "Questo è il path di SGD: [[ 1.          1.          1.         ...  1.          1.\n",
      "   1.        ]\n",
      " [ 1.          1.          1.006      ...  1.          1.\n",
      "   1.        ]\n",
      " [ 1.002       1.          1.006      ...  1.          1.098\n",
      "   1.        ]\n",
      " ...\n",
      " [ 1.61587378  2.40269507  3.27626714 ... 47.16598131 48.1455212\n",
      "  49.12679341]\n",
      " [ 1.61587975  2.40269507  3.27626714 ... 47.16598131 48.14727169\n",
      "  49.12858054]\n",
      " [ 1.61587975  2.40269507  3.27626714 ... 47.16598131 48.14727169\n",
      "  49.12858054]]\n"
     ]
    }
   ],
   "source": [
    "path_gd_f7, path_newton_f7 = get_solutions(f_c, df_c, h_c, X_start1, X_start2, maxit, tol_grad, tol_step)\n",
    "\n",
    "path_sgd_f7 = get_SGD_solutions(f_c, sdf_c, X_start1, n, maxit_SGD, tol_grad, tol_step, batch_k, learning_rate)\n",
    "print(f\"Questo è il path di SGD: {path_sgd_f7}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e480eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_graph(f_c, df_c, path_gd_f7, path_newton_f7, title_f7, path_sgd_f7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4662eab8",
   "metadata": {},
   "source": [
    "TODO: nel error_graph guarda se devi mettere sdf (e risolvi indici).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CN25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
