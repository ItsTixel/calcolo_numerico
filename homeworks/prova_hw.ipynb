{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_cell",
   "metadata": {},
   "source": [
    "# Analisi e Implementazione di Metodi di Ottimizzazione\n",
    "\n",
    "Questo notebook implementa e confronta tre metodi di ottimizzazione non vincolata per trovare i minimi di diverse funzioni test:\n",
    "1.  **Metodo del Gradiente (Gradient Descent - GD)**\n",
    "2.  **Metodo di Newton**\n",
    "3.  **Metodo del Gradiente Stocastico (SGD)** - *Implementazione semplificata*\n",
    "\n",
    "Viene utilizzata una strategia di **Backtracking Line Search** per la determinazione del passo di apprendimento (alpha) in tutti i metodi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ebaf8",
   "metadata": {},
   "source": [
    "## 1. Import delle librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a3ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import numpy.linalg as LA # Per algebra lineare (es. solve)\n",
    "\n",
    "# matplotlib.use(\"TkAgg\") # Decommentare se si usa un ambiente script, solitamente non necessario in notebook\n",
    "from mpl_toolkits.mplot3d import Axes3D # Per grafici 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "algos_header",
   "metadata": {},
   "source": [
    "## 2. Implementazione degli Algoritmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d34ad9f",
   "metadata": {},
   "source": [
    "### 2.1 Funzione Backtracking (Line Search)\n",
    "\n",
    "Questa funzione implementa la ricerca del passo $\\alpha$ tramite backtracking, basandosi sulla condizione di Armijo per garantire una discesa sufficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aebdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtracking per alpha (Condizione di Armijo)\n",
    "def backtracking(f, df, X_k, p_k):\n",
    "    \"\"\"\n",
    "    Implementa il backtracking line search per trovare un passo alpha idoneo.\n",
    "    \n",
    "    Parametri:\n",
    "    f: La funzione obiettivo\n",
    "    df: Il gradiente della funzione obiettivo\n",
    "    X_k: Il punto corrente (vettore)\n",
    "    p_k: La direzione di discesa (vettore)\n",
    "    \"\"\"\n",
    "    alpha = 1.0\n",
    "    rho = 0.5  # Fattore di riduzione per alpha\n",
    "    c = 0.25   # Costante per la condizione di Armijo\n",
    "\n",
    "    # Calcola il gradiente nel punto corrente\n",
    "    grad_k = df(X_k)\n",
    "\n",
    "    # Calcola il prodotto scalare (grad_k^T * p_k)\n",
    "    grad_dot_p = np.dot(grad_k, p_k)\n",
    "    \n",
    "    # Condizione Di Armijo: Il loop \"while\" continua FINCHÉ la condizione è VIOLATA\n",
    "    while f(X_k + alpha * p_k) > f(X_k) + c * alpha * grad_dot_p:\n",
    "        alpha = rho * alpha\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad29c54",
   "metadata": {},
   "source": [
    "### 2.2 Metodo del Gradiente (GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo del gradiente\n",
    "def GD (f, df, X_old, maxit, tol_f, tol_x):\n",
    "    \n",
    "    count = 0\n",
    "    dim = len(X_old)\n",
    "\n",
    "    # Array per memorizzare la cronologia dei punti visitati\n",
    "    fun_history = np.zeros((maxit + 1, dim))\n",
    "    fun_history[0, :] = X_old\n",
    "\n",
    "    exit_flag = 'maxit' # Flag di uscita di default\n",
    "\n",
    "    current_grad_norm = np.linalg.norm(df(X_old))\n",
    "\n",
    "    # Ciclo principale: itera fino a maxit o fino a convergenza\n",
    "    while count < maxit and current_grad_norm > tol_f:\n",
    "        # Direzione di discesa = anti-gradiente\n",
    "        p_k = -df(X_old)\n",
    "\n",
    "        # Calcolo del passo alpha con backtracking\n",
    "        alpha = backtracking(f, df, X_old, p_k)\n",
    "\n",
    "        # Calcolo del nuovo punto X_k+1\n",
    "        X_new = X_old + alpha * p_k\n",
    "\n",
    "        # Controllo tolleranza sulla distanza tra iterati (tol_x)\n",
    "        if np.linalg.norm(X_new - X_old) < tol_x:\n",
    "            exit_flag = 'tol_x'\n",
    "            break\n",
    "\n",
    "        # Ricalcolo la norma del gradiente per il prossimo ciclo\n",
    "        current_grad_norm = np.linalg.norm(df(X_new))\n",
    "\n",
    "        # Aggiorno per l'iterazione successiva\n",
    "        X_old = X_new\n",
    "        count += 1\n",
    "\n",
    "        # Salvo il punto corrente nella cronologia\n",
    "        fun_history[count, :] = X_new\n",
    "\n",
    "    # Controllo se l'uscita è dovuta a tol_f (norma del gradiente)\n",
    "    if exit_flag == 'maxit' and current_grad_norm <= tol_f:\n",
    "        exit_flag = 'tol_f'\n",
    "\n",
    "    # Rimuovo le righe non utilizzate dalla cronologia\n",
    "    fun_history = fun_history[:count+1]\n",
    "\n",
    "    return X_old, count, fun_history, exit_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af6510",
   "metadata": {},
   "source": [
    "### 2.3 Metodo di Newton\n",
    "\n",
    "Questo metodo utilizza l'informazione del secondo ordine (matrice Hessiana) per calcolare la direzione di discesa, risolvendo il sistema lineare $H_k p_k = -\\nabla f_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75834970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo di Newton Puro\n",
    "def Newton (f, df, hess_f, X_old, maxit, tol_f, tol_x):\n",
    "    \n",
    "    count = 0\n",
    "    dim = len(X_old)\n",
    "\n",
    "    # Array per la cronologia\n",
    "    fun_history = np.zeros((maxit + 1, dim))\n",
    "    fun_history[0, :] = X_old\n",
    "\n",
    "    exit_flag = 'maxit'\n",
    "\n",
    "    current_grad_norm = np.linalg.norm(df(X_old))\n",
    "\n",
    "    while count < maxit and current_grad_norm > tol_f:\n",
    "\n",
    "        # Calcolo Gradiente e Hessiana\n",
    "        grad_k = df(X_old)\n",
    "        H_k = hess_f(X_old)\n",
    "\n",
    "        # Direzione di discesa: risolvo il sistema H_k * p_k = -grad_k\n",
    "        try:\n",
    "            p_k = LA.solve(H_k, -grad_k)\n",
    "        except LA.LinAlgError: \n",
    "            # Se l'Hessiana è singolare, il metodo fallisce\n",
    "            print(\"Errore: Matrice Hessiana Singolare.\")\n",
    "            exit_flag = 'Hessian Error'\n",
    "            break\n",
    "\n",
    "        # Backtracking di alpha\n",
    "        alpha = backtracking(f, df, X_old, p_k)\n",
    "\n",
    "        # Calcolo di X_k+1\n",
    "        X_new = X_old + alpha * p_k\n",
    "\n",
    "        # Controllo tolleranza x\n",
    "        if np.linalg.norm(X_new - X_old) < tol_x:\n",
    "            exit_flag = 'tol_x'\n",
    "            break\n",
    "\n",
    "        # Ricalcolo la norma del gradiente\n",
    "        current_grad_norm = np.linalg.norm(df(X_new))\n",
    "\n",
    "        # Aggiorno per l'iterazione successiva\n",
    "        X_old = X_new\n",
    "        count += 1\n",
    "        fun_history[count, :] = X_new\n",
    "\n",
    "    \n",
    "    if exit_flag == 'maxit' and current_grad_norm <= tol_f:\n",
    "        exit_flag = 'tol_f'\n",
    "\n",
    "    # Rimuovo le righe non utilizzate\n",
    "    fun_history = fun_history[:count+1]\n",
    "\n",
    "    return X_old, count, fun_history, exit_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e85ae8",
   "metadata": {},
   "source": [
    "### 2.4 Metodo del Gradiente Stocastico (SGD)\n",
    "\n",
    "**Nota:** L'implementazione seguente è solo una *struttura* per SGD. Per come sono definite le funzioni obiettivo (es. Rosenbrock), non c'è una dipendenza da un dataset $S_k$. Per questo motivo, il calcolo del gradiente `df(X_old)` è in realtà un gradiente *completo* (Full Batch). L'algoritmo si comporta quindi in modo identico al Metodo del Gradiente standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo del gradiente stocastico (Simulato)\n",
    "def SGD (f, df, X_old, max_steps, tol_f, tol_x):\n",
    "    \n",
    "    # --- Inizializzazione Dati (per un VERO SGD) ---\n",
    "    # Dimensione del mini-batch (fittizia in questo caso)\n",
    "    k = 3 \n",
    "    # Inizializziamo un dataset fittizio di indici\n",
    "    S_k = np.arange(0, 20, 1)\n",
    "    # ------------------------------------------------\n",
    "\n",
    "    # Calcolo la dimensione di X_old\n",
    "    dim = len(X_old)\n",
    "\n",
    "    step_count = 0\n",
    "\n",
    "    # Dichiarare una matrice per la cronologia\n",
    "    fun_history = np.zeros((max_steps + 1, dim))\n",
    "    fun_history[0, :] = X_old\n",
    "\n",
    "    exit_flag = 'maxit'\n",
    "\n",
    "    current_grad_norm = np.linalg.norm(df(X_old))\n",
    "\n",
    "    # Il 'while' loop esterno rappresenta le EPOCHE\n",
    "    while step_count < max_steps and current_grad_norm > tol_f:\n",
    "\n",
    "        # Randomizziamo gli indici del dataset (shuffle)\n",
    "        np.random.shuffle(S_k)\n",
    "\n",
    "        # Il 'for' loop interno itera sui MINI-BATCH\n",
    "        for i in range (0, len(S_k), k):\n",
    "\n",
    "            # Estrae gli indici per il mini-batch corrente\n",
    "            indices = S_k[i:i+k]\n",
    "            \n",
    "            # --- CALCOLO GRADIENTE ---\n",
    "            # NOTA BENE: Questa implementazione NON È STOCASTICA.\n",
    "            # La variabile 'indices' non viene usata.\n",
    "            # 'df(X_old)' calcola il gradiente COMPLETO (Full Batch).\n",
    "            # Per un VERO SGD, si dovrebbe calcolare df(X_old, indices)\n",
    "            grad_stoc = df(X_old) \n",
    "            p_k = -grad_stoc\n",
    "            # -------------------------\n",
    "\n",
    "            # Backtracking per alpha\n",
    "            alpha = backtracking(f, df, X_old, p_k)\n",
    "\n",
    "            # Calcolo di X_k+1\n",
    "            X_new = X_old + alpha * p_k\n",
    "\n",
    "            # Controllo tolleranza x (progresso minimo)\n",
    "            if np.linalg.norm(X_new - X_old) < tol_x:\n",
    "                exit_flag = 'tol_x'\n",
    "                break\n",
    "\n",
    "            # Aggiorno per l'iterazione successiva\n",
    "            X_old = X_new\n",
    "            step_count += 1\n",
    "            \n",
    "            # Aggiungo alla cronologia\n",
    "            if step_count <= max_steps:\n",
    "                fun_history[step_count, :] = X_new\n",
    "\n",
    "            # Controllo se ho superato il numero di passi totali (non epoche)\n",
    "            if step_count >= max_steps:\n",
    "                break # Interrompe il 'for' loop (mini-batch)\n",
    "\n",
    "        # Ricalcolo la norma del gradiente completo (per condizione d'arresto)\n",
    "        current_grad_norm = np.linalg.norm(df(X_new))\n",
    "\n",
    "        # Se uno dei break interni è stato attivato, esci anche dal 'while'\n",
    "        if exit_flag == 'tol_x' or step_count >= max_steps:\n",
    "            break\n",
    "\n",
    "    if exit_flag == 'maxit' and current_grad_norm <= tol_f:\n",
    "        exit_flag = 'tol_f'\n",
    "\n",
    "    # Tronchiamo l'array della cronologia\n",
    "    fun_history = fun_history[:step_count+1]\n",
    "\n",
    "    return X_old, step_count, fun_history, exit_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot_header",
   "metadata": {},
   "source": [
    "## 3. Funzione per la Generazione dei Grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc521666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_generator (x_range, y_range, f, path_history, tmin, title_2d=\"Percorso Ottimizzazione\", title_3d=\"Superficie 3D\"):\n",
    "    \"\"\"\n",
    "    Genera due grafici affiancati: un contour plot 2D e un surface plot 3D.\n",
    "    \n",
    "    Parametri:\n",
    "    x_range: Range di valori per l'asse x (linspace)\n",
    "    y_range: Range di valori per l'asse y (linspace)\n",
    "    f: La funzione obiettivo\n",
    "    path_history: Matrice (N_iter, 2) con la cronologia dei punti X_k\n",
    "    tmin: Coordinate (x, y) del minimo teorico\n",
    "    title_2d: Titolo per il grafico 2D\n",
    "    title_3d: Titolo per il grafico 3D\n",
    "    \"\"\"\n",
    "    \n",
    "    X_mesh, Y_mesh = np.meshgrid(x_range, y_range)\n",
    "    Z = f([X_mesh, Y_mesh])\n",
    "\n",
    "    # Estrae il percorso delle coordinate x e y\n",
    "    path_x = path_history[:, 0]\n",
    "    path_y = path_history[:, 1]\n",
    "\n",
    "    # Definisco la figura\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    # --- GENERAZIONE GRAFICO 3D (a sinistra) ---\n",
    "    axs1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    \n",
    "    # Disegno la superficie\n",
    "    axs1.plot_surface(X_mesh, Y_mesh, Z, cmap='viridis', rstride=1, cstride=1, linewidth=0, antialiased=True, alpha=0.7)\n",
    "\n",
    "    # Aggiunge il percorso anche nel grafico 3D\n",
    "    z_path = f([path_x, path_y])\n",
    "    axs1.plot(path_x, path_y, z_path, 'r-o', markersize=3, label='Percorso Ottimizzazione')\n",
    "\n",
    "    # Evidenzia il punto iniziale e finale\n",
    "    axs1.plot([path_x[0]], [path_y[0]], [z_path[0]], 'go', markersize=10, label=f'Start: ({path_x[0]:.1f}, {path_y[0]:.1f})')\n",
    "    axs1.plot([path_x[-1]], [path_y[-1]], [z_path[-1]], 'rx', markersize=12, label=f'End: ({path_x[-1]:.2f}, {path_y[-1]:.2f})')\n",
    "    axs1.plot([tmin[0]], [tmin[1]], [f(tmin)], 'b*', markersize=15, label=f'Minimo Teorico ({tmin[0]}, {tmin[1]})')\n",
    "\n",
    "    # Aggiungo label e titolo\n",
    "    axs1.set_title(title_3d)\n",
    "    axs1.set_xlabel(\"x\")\n",
    "    axs1.set_ylabel(\"y\")\n",
    "    axs1.set_zlabel(\"z\")\n",
    "    axs1.legend()\n",
    "    axs1.view_init(elev=30, azim=135) # Imposta un angolo di visuale\n",
    "\n",
    "    # --- GENERAZIONE GRAFICO 2D (a destra) ---\n",
    "    axs0 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "    # Disegna le curve di livello\n",
    "    c1 = axs0.contour(X_mesh, Y_mesh, Z, levels=50, cmap='viridis', alpha=0.7)\n",
    "    fig.colorbar(c1, ax=axs0, label='Valore di $f(x,y)$')\n",
    "\n",
    "    # Disegna il percorso\n",
    "    axs0.plot(path_x, path_y, 'r-o', markersize=3, label='Percorso Ottimizzazione')\n",
    "\n",
    "    # Evidenzia il punto iniziale e finale\n",
    "    axs0.plot(path_x[0], path_y[0], 'go', markersize=10, label=f'Start: ({path_x[0]:.1f}, {path_y[0]:.1f})')\n",
    "    axs0.plot(path_x[-1], path_y[-1], 'rx', markersize=12, label=f'End: ({path_x[-1]:.2f}, {path_y[-1]:.2f})')\n",
    "    axs0.plot(tmin[0], tmin[1], 'b*', markersize=15, label=f'Minimo Teorico ({tmin[0]}, {tmin[1]})')\n",
    "\n",
    "    # Etichette e titoli\n",
    "    axs0.set_title(title_2d)\n",
    "    axs0.set_xlabel(\"x\")\n",
    "    axs0.set_ylabel(\"y\")\n",
    "    axs0.legend()\n",
    "    axs0.set_aspect('equal', adjustable='box')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff313e",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Caso di Studio 1: Funzione Quadratica\n",
    "$$f(x, y) = (x-5)^2 + (y-2)^2$$\n",
    "\n",
    "Cerchiamo i punti critici della funzione (punti a gradiente nullo).\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x} = 2(x-5)$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial y} = 2(y-2)$\n",
    "\n",
    "Ponendo entrambe le derivate a zero, troviamo un unico punto critico in $\\mathbf{x}^* = (5, 2)$.\n",
    "Dato che la funzione è una somma di quadrati, questo è l'unico minimo globale ($f(5,2) = 0$).\n",
    "\n",
    "**Nota:** Per una funzione quadratica, il metodo del Gradiente con line search esatta converge in una sola iterazione. Il backtracking approssima questo comportamento, portando a una convergenza molto rapida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d54457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Definizione Funzione 1 ----\n",
    "def f1(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "    return (x - 5)**2 + (y - 2)**2\n",
    "\n",
    "def df1(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "    df_dx = 2 * (x - 5)\n",
    "    df_dy = 2 * (y - 2)\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def hess_f1(X):\n",
    "    # Hessiana costante per una funzione quadratica\n",
    "    return np.array([ [2.0, 0.0],\n",
    "                      [0.0, 2.0] ])\n",
    "    \n",
    "# ---- Parametri Comuni ----\n",
    "x_range1 = np.linspace(-2, 12, 100)\n",
    "y_range1 = np.linspace(-2, 8, 100)\n",
    "tmin1 = [5.0, 2.0]\n",
    "\n",
    "maxit = 300\n",
    "tolleranza_f = 1.e-6\n",
    "tolleranza_x = 1.e-6\n",
    "punto_iniziale1 = np.array([0.0, 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7428493",
   "metadata": {},
   "source": [
    "### 4.1 Esecuzione e Confronto Metodi (Caso 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea128d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Metodo del Gradiente (GD) ---\")\n",
    "sol_gd, iter_gd, path_gd, exit_gd = GD(f1, df1, punto_iniziale1, maxit, tolleranza_f, tolleranza_x)\n",
    "print(f\"Soluzione: {sol_gd}\")\n",
    "print(f\"Numero iterazioni: {iter_gd}\")\n",
    "print(f\"Condizione di uscita: {exit_gd}\")\n",
    "\n",
    "print(\"--- Metodo di Newton ---\")\n",
    "sol_newton, iter_newton, path_newton, exit_newton = Newton(f1, df1, hess_f1, punto_iniziale1, maxit, tolleranza_f, tolleranza_x)\n",
    "print(f\"Soluzione: {sol_newton}\")\n",
    "print(f\"Numero iterazioni: {iter_newton}\")\n",
    "print(f\"Condizione di uscita: {exit_newton}\")\n",
    "\n",
    "print(\"--- Metodo del Gradiente Stocastico (Simulato) ---\")\n",
    "sol_sgd, iter_sgd, path_sgd, exit_sgd = SGD(f1, df1, punto_iniziale1, maxit, tolleranza_f, tolleranza_x)\n",
    "print(f\"Soluzione: {sol_sgd}\")\n",
    "print(f\"Numero iterazioni: {iter_sgd}\")\n",
    "print(f\"Condizione di uscita: {exit_sgd}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eef308",
   "metadata": {},
   "source": [
    "### 4.2 Grafici (Caso 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_plot_cell_1_gd",
   "metadata": {},
   "outputs": [],
   "source": [
    "titolo = f\"Caso 1 (Quadratica) - Metodo del Gradiente ({iter_gd} iterazioni)\"\n",
    "graph_generator(x_range1, y_range1, f1, path_gd, tmin1, \n",
    "                title_2d=titolo,\n",
    "                title_3d=titolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_plot_cell_1_newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "titolo = f\"Caso 1 (Quadratica) - Metodo di Newton ({iter_newton} iterazioni)\"\n",
    "graph_generator(x_range1, y_range1, f1, path_newton, tmin1, \n",
    "                title_2d=titolo,\n",
    "                title_3d=titolo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687edc4a",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Caso di Studio 2: Funzione di Rosenbrock\n",
    "$$f(x, y) = (1-x)^2 + 100(y-x^2)^2$$\n",
    "\n",
    "Questa è una funzione non-convessa classica, nota per avere una valle parabolica stretta e lunga, che rende difficile la convergenza per i metodi basati solo sul gradiente.\n",
    "\n",
    "Calcoliamo le derivate parziali:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x} = -2(1-x) + 100 \\cdot 2(y-x^2)(-2x) = 2(x-1) - 400x(y-x^2)$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial y} = 100 \\cdot 2(y-x^2) = 200(y-x^2)$\n",
    "\n",
    "Ponendo $\\frac{\\partial f}{\\partial y} = 0$, otteniamo $y=x^2$. Sostituendo nella prima equazione:\n",
    "$2(x-1) - 400x(x^2-x^2) = 0 \\implies 2(x-1) = 0 \\implies x=1$.\n",
    "\n",
    "L'unico punto critico è $(1, 1)$.\n",
    "\n",
    "L'Hessiana è:\n",
    "$$H(x,y) = \\begin{bmatrix} \n",
    "\\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix} \n",
    " 2 - 400y + 1200x^2 & -400x \\\\\n",
    "-400x & 200\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Valutando nel punto critico $(1, 1)$:\n",
    "$H(1,1) = \\begin{bmatrix} 2 - 400 + 1200 & -400 \\\\ -400 & 200 \\end{bmatrix} = \\begin{bmatrix} 802 & -400 \\\\ -400 & 200 \\end{bmatrix}$\n",
    "\n",
    "Il determinante è $\\det(H) = 802 \\cdot 200 - (-400)^2 = 160400 - 160000 = 400 > 0$.\n",
    "Poiché $H_{11} = 802 > 0$ e $\\det(H) > 0$, la matrice è definita positiva e il punto $(1, 1)$ è un minimo locale (e globale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d99e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Definizione Funzione 2 (Rosenbrock) ----\n",
    "def f2(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "def df2(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "    df_dx = 2 * (x - 1) - 400 * x * (y - x**2)\n",
    "    df_dy = 200 * (y - x**2)\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def hess_f2(X):\n",
    "    HESS = np.zeros((2, 2))\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "    HESS[0, 0] = 2 - 400 * y + 1200 * x**2 # d2f/dx2\n",
    "    HESS[0, 1] = -400 * x                 # d2f/dxdy\n",
    "    HESS[1, 0] = -400 * x                 # d2f/dydx\n",
    "    HESS[1, 1] = 200                      # d2f/dy2\n",
    "    return HESS\n",
    "    \n",
    "# ---- Parametri Comuni ----\n",
    "x_range2 = np.linspace(-2, 2, 200)\n",
    "y_range2 = np.linspace(-1, 3, 200)\n",
    "tmin2 = [1.0, 1.0]\n",
    "\n",
    "maxit_rosen = 10000 # Aumentiamo le iterazioni massime\n",
    "tolleranza_f = 1.e-6\n",
    "tolleranza_x = 1.e-6\n",
    "punto_iniziale2 = np.array([0.0, 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rosen_exec_md",
   "metadata": {},
   "source": [
    "### 5.1 Esecuzione e Confronto Metodi (Caso 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2f42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Metodo del Gradiente (GD) ---\")\n",
    "sol_gd_r, iter_gd_r, path_gd_r, exit_gd_r = GD(f2, df2, punto_iniziale2, maxit_rosen, tolleranza_f, tolleranza_x)\n",
    "print(f\"Soluzione: {sol_gd_r}\")\n",
    "print(f\"Numero iterazioni: {iter_gd_r}\")\n",
    "print(f\"Condizione di uscita: {exit_gd_r}\")\n",
    "\n",
    "print(\"--- Metodo di Newton ---\")\n",
    "sol_newton_r, iter_newton_r, path_newton_r, exit_newton_r = Newton(f2, df2, hess_f2, punto_iniziale2, maxit_rosen, tolleranza_f, tolleranza_x)\n",
    "print(f\"Soluzione: {sol_newton_r}\")\n",
    "print(f\"Numero iterazioni: {iter_newton_r}\")\n",
    "print(f\"Condizione di uscita: {exit_newton_r}\")\n",
    "\n",
    "print(\"--- Metodo del Gradiente Stocastico (Simulato) ---\")\n",
    "sol_sgd_r, iter_sgd_r, path_sgd_r, exit_sgd_r = SGD(f2, df2, punto_iniziale2, maxit_rosen, tolleranza_f, tolleranza_x)\n",
    "print(f\"Soluzione: {sol_sgd_r}\")\n",
    "print(f\"Numero iterazioni: {iter_sgd_r}\")\n",
    "print(f\"Condizione di uscita: {exit_sgd_r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rosen_plot_md",
   "metadata": {},
   "source": [
    "### 5.2 Grafici (Caso 2)\n",
    "\n",
    "Notare la netta differenza nel numero di iterazioni. Il metodo del Gradiente impiega migliaia di passi, zig-zagando lentamente lungo la valle, mentre il metodo di Newton, utilizzando la curvatura (Hessiana), punta direttamente al minimo in poche iterazioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_plot_cell_2_gd",
   "metadata": {},
   "outputs": [],
   "source": [
    "titolo_r_gd = f\"Caso 2 (Rosenbrock) - Metodo del Gradiente ({iter_gd_r} iterazioni)\"\n",
    "graph_generator(x_range2, y_range2, f2, path_gd_r, tmin2, \n",
    "                title_2d=titolo_r_gd,\n",
    "                title_3d=titolo_r_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_plot_cell_2_newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "titolo_r_n = f\"Caso 2 (Rosenbrock) - Metodo di Newton ({iter_newton_r} iterazioni)\"\n",
    "graph_generator(x_range2, y_range2, f2, path_newton_r, tmin2, \n",
    "                title_2d=titolo_r_n,\n",
    "                title_3d=titolo_r_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbbfc40",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Caso di Studio 3: Funzione Multimodale\n",
    "$$f(x,y) = x^4 - x^2 + y^2$$\n",
    "\n",
    "Questa funzione ha più punti critici: due minimi locali (e globali) e un punto di sella.\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x} = 4x^3 - 2x = 2x(2x^2 - 1)$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial y} = 2y$\n",
    "\n",
    "I punti critici si ottengono ponendo il gradiente a zero:\n",
    "1.  $y=0$\n",
    "2.  $2x(2x^2 - 1) = 0 \\implies x=0$ oppure $x = \\pm \\sqrt{\\frac{1}{2}} \\approx \\pm 0.707$\n",
    "\n",
    "Abbiamo tre punti critici: $P_1 = (0, 0)$, $P_2 = (\\sqrt{1/2}, 0)$, $P_3 = (-\\sqrt{1/2}, 0)$.\n",
    "\n",
    "L'Hessiana è:\n",
    "$$H(x,y) = \\begin{bmatrix} \n",
    "12x^2 - 2 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Analizziamo i punti:\n",
    "\n",
    "**Punto $P_1(0,0)$:**\n",
    "$H(0,0) = \\begin{bmatrix} -2 & 0 \\\\ 0 & 2 \\end{bmatrix}$. Gli autovalori sono $\\lambda_1 = -2, \\lambda_2 = 2$. Essendo di segno opposto, $\\mathbf{(0,0)}$ **è un punto di sella**.\n",
    "\n",
    "**Punti $P_2$ e $P_3$ $(\\pm\\sqrt{1/2}, 0)$:**\n",
    "$H(\\pm\\sqrt{1/2}, 0) = \\begin{bmatrix} 12(1/2) - 2 & 0 \\\\ 0 & 2 \\end{bmatrix} = \\begin{bmatrix} 4 & 0 \\\\ 0 & 2 \\end{bmatrix}$. Gli autovalori sono $\\lambda_1 = 4, \\lambda_2 = 2$. Essendo entrambi positivi, ${(\\pm\\sqrt{1/2}, 0)}$ **sono due punti di minimo**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37944ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Definizione Funzione 3 (Multimodale) ----\n",
    "def f3(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "    return x**4 - x**2 + y**2\n",
    "\n",
    "def df3(X):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "    df_dx = 4 * x**3 - 2 * x\n",
    "    df_dy = 2 * y\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def hess_f3(X):\n",
    "    HESS = np.zeros((2, 2))\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "    HESS[0, 0] = 12 * x**2 - 2 # d2f/dx2\n",
    "    HESS[0, 1] = 0               # d2f/dxdy\n",
    "    HESS[1, 0] = 0               # d2f/dydx\n",
    "    HESS[1, 1] = 2               # d2f/dy2\n",
    "    return HESS\n",
    "    \n",
    "# ---- Parametri Comuni ----\n",
    "x_range3 = np.linspace(-1.5, 1.5, 200)\n",
    "y_range3 = np.linspace(-1.5, 1.5, 200)\n",
    "# Minimo teorico (il più vicino al punto di partenza)\n",
    "tmin3 = [np.sqrt(1/2), 0.0]\n",
    "tmin3_alt = [-np.sqrt(1/2), 0.0]\n",
    "tsaddle = [0.0, 0.0]\n",
    "\n",
    "maxit_multi = 500\n",
    "tolleranza_f = 1.e-6\n",
    "tolleranza_x = 1.e-6\n",
    "punto_iniziale3 = np.array([2.0, 1.0]) # Punto di partenza\n",
    "# Proviamo anche un punto di partenza vicino al punto di sella\n",
    "punto_iniziale3_sella = np.array([0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi_exec_md",
   "metadata": {},
   "source": [
    "### 6.1 Esecuzione e Confronto Metodi (Caso 3)\n",
    "\n",
    "Partendo da $(2.0, 1.0)$, ci aspettiamo che l'algoritmo converga al minimo più vicino, $P_2 = (\\sqrt{1/2}, 0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc864a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Partenza (2.0, 1.0) --- MINIMO P2 ---\")\n",
    "print(\"--- Metodo del Gradiente (GD) ---\")\n",
    "sol_gd_m, iter_gd_m, path_gd_m, exit_gd_m = GD(f3, df3, punto_iniziale3, maxit_multi, tolleranza_f, tolleranza_x)\n",
    "print(f\"Soluzione: {sol_gd_m}\")\n",
    "print(f\"Numero iterazioni: {iter_gd_m}\")\n",
    "print(f\"Condizione di uscita: {exit_gd_m}\")\n",
    "\n",
    "print(\"--- Metodo di Newton ---\")\n",
    "sol_newton_m, iter_newton_m, path_newton_m, exit_newton_m = Newton(f3, df3, hess_f3, punto_iniziale3, maxit_multi, tolleranza_f, tolleranza_x)\n",
    "print(f\"Soluzione: {sol_newton_m}\")\n",
    "print(f\"Numero iterazioni: {iter_newton_m}\")\n",
    "print(f\"Condizione di uscita: {exit_newton_m}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi_plot_md",
   "metadata": {},
   "source": [
    "### 6.2 Grafici (Caso 3 - Partenza 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_plot_cell_3_gd",
   "metadata": {},
   "outputs": [],
   "source": [
    "titolo_m_gd = f\"Caso 3 (Multimodale) - Metodo del Gradiente ({iter_gd_m} iterazioni)\"\n",
    "graph_generator(x_range3, y_range3, f3, path_gd_m, tmin3, \n",
    "                title_2d=titolo_m_gd,\n",
    "                title_3d=titolo_m_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_plot_cell_3_newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "titolo_m_n = f\"Caso 3 (Multimodale) - Metodo di Newton ({iter_newton_m} iterazioni)\"\n",
    "graph_generator(x_range3, y_range3, f3, path_newton_m, tmin3, \n",
    "                title_2d=titolo_m_n,\n",
    "                title_3d=titolo_m_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi_exec_2_md",
   "metadata": {},
   "source": [
    "### 6.3 Esecuzione (Caso 3 - Partenza vicino alla sella)\n",
    "\n",
    "Cosa succede se partiamo molto vicini al punto di sella $(0,0)$? L'algoritmo (specialmente Newton) potrebbe avere difficoltà o convergere molto lentamente all'inizio, prima di \"cadere\" in uno dei due minimi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi_exec_2_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Partenza (0.1, 0.1) --- VICINO A SELLA ---\")\n",
    "print(\"--- Metodo del Gradiente (GD) ---\")\n",
    "sol_gd_m2, iter_gd_m2, path_gd_m2, exit_gd_m2 = GD(f3, df3, punto_iniziale3_sella, maxit_multi, tolleranza_f, tolleranza_x)\n",
    "print(f\"Soluzione: {sol_gd_m2}\")\n",
    "print(f\"Numero iterazioni: {iter_gd_m2}\")\n",
    "print(f\"Condizione di uscita: {exit_gd_m2}\")\n",
    "\n",
    "print(\"--- Metodo di Newton ---\")\n",
    "sol_newton_m2, iter_newton_m2, path_newton_m2, exit_newton_m2 = Newton(f3, df3, hess_f3, punto_iniziale3_sella, maxit_multi, tolleranza_f, tolleranza_x)\n",
    "print(f\"Soluzione: {sol_newton_m2}\")\n",
    "print(f\"Numero iterazioni: {iter_newton_m2}\")\n",
    "print(f\"Condizione di uscita: {exit_newton_m2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_plot_cell_3_newton_saddle",
   "metadata": {},
   "outputs": [],
   "source": [
    "titolo_m_n2 = f\"Caso 3 (Multimodale) - Newton da (0.1, 0.1) ({iter_newton_m2} iterazioni)\"\n",
    "graph_generator(x_range3, y_range3, f3, path_newton_m2, tmin3, \n",
    "                title_2d=titolo_m_n2,\n",
    "                title_3d=titolo_m_n2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6196fa8",
   "metadata": {},
   "source": [
    "---\n",
    "Fine dell'analisi."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (o kernel esistente)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
